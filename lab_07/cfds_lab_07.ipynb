{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"images/cfds_logo.png\">\n",
    "\n",
    "###  Lab 07 - \"Supervised Deep Learning - LSTMs\"\n",
    "\n",
    "Chartered Financial Data Scientist (CFDS), Spring Term 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the sixth lab you learned about how to utilize an **unsupervised** (deep) learning technique namely **Convolutional Neural Networks (CNNs)** to classify tiny images of objects contained in the CIFAR-10 dataset.\n",
    "\n",
    "In this seventh lab we will learn how to apply another type of deep learning technique referred to as **Long-Short-Term-Memory (LSTM)** neural networks. Unlike standard feedforward neural networks, LSTMs encompass feedback connections that make it a \"general purpose computer\". LSTMs are designed to not only process single data points (such as images), but also entire sequences of data, e.g., such as speech, video, or financial time series.\n",
    "\n",
    "\n",
    "We will again use the functionality of the **'PyTorch'** library to implement and train an LSTM based neural network. The network will be trained on the historic daily (in-sample) returns of an exemplary financial stock. Once the network is trained, we will use the learned model to predict future (out-of-sample) returns. Finally, we will convert the predictions into tradable signals and the backtest the signals accordingly. \n",
    "\n",
    "The figure below illustrates a high-level eye view on the machine learning process we aim to establish in this lab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px\" src=\"images/process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pls. don't hesitate to contact me (via marco.schreyer@unisg.ch) in case of any difficulties with the lab content or any questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After today's lab you should be able to:\n",
    "\n",
    "> 1. Understand the basic concepts, intuitions and major building blocks of **Long-Short Term Memory (LSTM) Neural Networks**.\n",
    "> 2. Know how to **implement and to train a LSTM** to learn a model of financial time-series data.\n",
    "> 3. Understand how to apply such a learned model to **predict future data points of a time-series**.\n",
    "> 4. Know how to **interpret the model's prediction results** and backtest the predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. We will mostly use the PyTorch, Numpy, Sklearn, Matplotlib, Seaborn, BT and a few utility libraries throughout the course of this lab:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import python data science libraries\n",
    "import os\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import financial data science libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quandl as ql # library to retreive financial data\n",
    "import bt as bt # library to backtest trading signals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python machine / deep learning libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils import data\n",
    "from torch.utils.data import dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the sklearn pre-processing functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sklearn libraries\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python plotting libraries and set general plotting parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [10, 5]\n",
    "plt.rcParams['figure.dpi']= 150"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable notebook matplotlib inline plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress potential warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create notebook folder structure to store the data as well as the trained neural network models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data'): os.makedirs('./data')  # create data directory\n",
    "if not os.path.exists('./models'): os.makedirs('./models')  # create trained models directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set random seed value to obtain reproducable results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111bb5bd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init deterministic seed\n",
    "seed_value = 1234\n",
    "np.random.seed(seed_value) # set numpy seed\n",
    "torch.manual_seed(seed_value) # set pytorch seed CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.0: Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the lab notebook we will download and access historic daily stock market data ranging from **01/01/2000** to **31/12/2017** of the **\"International Business Machines\" (IBM)** corporation (ticker symbol: \"IBM\"). In order to start the data download let's initialize the \"quandl\" financial data download API and set an API key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ql.ApiConfig.api_key = '<enter you own quandl api code here>'\n",
    "ql.ApiConfig.api_key = 'xn6g-K_ebmMgSJRTCSUk'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's specify the start and end date of the stock market data download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = dt.datetime(2000, 1, 1)\n",
    "end_date = dt.datetime(2017, 12, 31)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the daily \"International Business Machines\" (IBM) stock market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = ql.get('WIKI/IBM', start_date=start_date, end_date=end_date, collapse='daily')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect top 10 records of the retreived IBM stock market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ex-Dividend</th>\n",
       "      <th>Split Ratio</th>\n",
       "      <th>Adj. Open</th>\n",
       "      <th>Adj. High</th>\n",
       "      <th>Adj. Low</th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>Adj. Volume</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2000-01-03</th>\n",
       "      <td>112.44</td>\n",
       "      <td>116.00</td>\n",
       "      <td>111.87</td>\n",
       "      <td>116.00</td>\n",
       "      <td>10347700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.542051</td>\n",
       "      <td>85.155442</td>\n",
       "      <td>82.123614</td>\n",
       "      <td>85.155442</td>\n",
       "      <td>10347700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-04</th>\n",
       "      <td>114.00</td>\n",
       "      <td>114.50</td>\n",
       "      <td>110.87</td>\n",
       "      <td>112.06</td>\n",
       "      <td>8227800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>83.687245</td>\n",
       "      <td>84.054294</td>\n",
       "      <td>81.389516</td>\n",
       "      <td>82.263093</td>\n",
       "      <td>8227800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-05</th>\n",
       "      <td>112.94</td>\n",
       "      <td>119.75</td>\n",
       "      <td>112.12</td>\n",
       "      <td>116.00</td>\n",
       "      <td>12733200.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>82.909100</td>\n",
       "      <td>87.908312</td>\n",
       "      <td>82.307139</td>\n",
       "      <td>85.155442</td>\n",
       "      <td>12733200.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-06</th>\n",
       "      <td>118.00</td>\n",
       "      <td>118.94</td>\n",
       "      <td>113.50</td>\n",
       "      <td>114.00</td>\n",
       "      <td>7971900.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.623639</td>\n",
       "      <td>87.313692</td>\n",
       "      <td>83.320195</td>\n",
       "      <td>83.687245</td>\n",
       "      <td>7971900.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-07</th>\n",
       "      <td>117.25</td>\n",
       "      <td>117.94</td>\n",
       "      <td>110.62</td>\n",
       "      <td>113.50</td>\n",
       "      <td>11856700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.073065</td>\n",
       "      <td>86.579593</td>\n",
       "      <td>81.205991</td>\n",
       "      <td>83.320195</td>\n",
       "      <td>11856700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-10</th>\n",
       "      <td>117.25</td>\n",
       "      <td>119.37</td>\n",
       "      <td>115.37</td>\n",
       "      <td>118.00</td>\n",
       "      <td>8540500.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.073065</td>\n",
       "      <td>87.629354</td>\n",
       "      <td>84.692960</td>\n",
       "      <td>86.623639</td>\n",
       "      <td>8540500.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-11</th>\n",
       "      <td>117.87</td>\n",
       "      <td>121.12</td>\n",
       "      <td>116.62</td>\n",
       "      <td>119.00</td>\n",
       "      <td>7873300.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.528206</td>\n",
       "      <td>88.914027</td>\n",
       "      <td>85.610583</td>\n",
       "      <td>87.357738</td>\n",
       "      <td>7873300.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-12</th>\n",
       "      <td>119.62</td>\n",
       "      <td>122.00</td>\n",
       "      <td>118.25</td>\n",
       "      <td>119.50</td>\n",
       "      <td>6803800.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>87.812879</td>\n",
       "      <td>89.560034</td>\n",
       "      <td>86.807164</td>\n",
       "      <td>87.724787</td>\n",
       "      <td>6803800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-13</th>\n",
       "      <td>119.94</td>\n",
       "      <td>121.00</td>\n",
       "      <td>115.75</td>\n",
       "      <td>118.25</td>\n",
       "      <td>8489700.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.047790</td>\n",
       "      <td>88.825935</td>\n",
       "      <td>84.971917</td>\n",
       "      <td>86.807164</td>\n",
       "      <td>8489700.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000-01-14</th>\n",
       "      <td>120.94</td>\n",
       "      <td>123.31</td>\n",
       "      <td>117.50</td>\n",
       "      <td>119.62</td>\n",
       "      <td>10956600.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>88.781889</td>\n",
       "      <td>90.521703</td>\n",
       "      <td>86.256590</td>\n",
       "      <td>87.812879</td>\n",
       "      <td>10956600.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open    High     Low   Close      Volume  Ex-Dividend  \\\n",
       "Date                                                                  \n",
       "2000-01-03  112.44  116.00  111.87  116.00  10347700.0          0.0   \n",
       "2000-01-04  114.00  114.50  110.87  112.06   8227800.0          0.0   \n",
       "2000-01-05  112.94  119.75  112.12  116.00  12733200.0          0.0   \n",
       "2000-01-06  118.00  118.94  113.50  114.00   7971900.0          0.0   \n",
       "2000-01-07  117.25  117.94  110.62  113.50  11856700.0          0.0   \n",
       "2000-01-10  117.25  119.37  115.37  118.00   8540500.0          0.0   \n",
       "2000-01-11  117.87  121.12  116.62  119.00   7873300.0          0.0   \n",
       "2000-01-12  119.62  122.00  118.25  119.50   6803800.0          0.0   \n",
       "2000-01-13  119.94  121.00  115.75  118.25   8489700.0          0.0   \n",
       "2000-01-14  120.94  123.31  117.50  119.62  10956600.0          0.0   \n",
       "\n",
       "            Split Ratio  Adj. Open  Adj. High   Adj. Low  Adj. Close  \\\n",
       "Date                                                                   \n",
       "2000-01-03          1.0  82.542051  85.155442  82.123614   85.155442   \n",
       "2000-01-04          1.0  83.687245  84.054294  81.389516   82.263093   \n",
       "2000-01-05          1.0  82.909100  87.908312  82.307139   85.155442   \n",
       "2000-01-06          1.0  86.623639  87.313692  83.320195   83.687245   \n",
       "2000-01-07          1.0  86.073065  86.579593  81.205991   83.320195   \n",
       "2000-01-10          1.0  86.073065  87.629354  84.692960   86.623639   \n",
       "2000-01-11          1.0  86.528206  88.914027  85.610583   87.357738   \n",
       "2000-01-12          1.0  87.812879  89.560034  86.807164   87.724787   \n",
       "2000-01-13          1.0  88.047790  88.825935  84.971917   86.807164   \n",
       "2000-01-14          1.0  88.781889  90.521703  86.256590   87.812879   \n",
       "\n",
       "            Adj. Volume  \n",
       "Date                     \n",
       "2000-01-03   10347700.0  \n",
       "2000-01-04    8227800.0  \n",
       "2000-01-05   12733200.0  \n",
       "2000-01-06    7971900.0  \n",
       "2000-01-07   11856700.0  \n",
       "2000-01-10    8540500.0  \n",
       "2000-01-11    7873300.0  \n",
       "2000-01-12    6803800.0  \n",
       "2000-01-13    8489700.0  \n",
       "2000-01-14   10956600.0  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also evaluate the data quality of the download by creating a set of summary statistics of the retrieved data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Ex-Dividend</th>\n",
       "      <th>Split Ratio</th>\n",
       "      <th>Adj. Open</th>\n",
       "      <th>Adj. High</th>\n",
       "      <th>Adj. Low</th>\n",
       "      <th>Adj. Close</th>\n",
       "      <th>Adj. Volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4.527000e+03</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.0</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4527.000000</td>\n",
       "      <td>4.527000e+03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>128.222709</td>\n",
       "      <td>129.396728</td>\n",
       "      <td>127.159447</td>\n",
       "      <td>128.298614</td>\n",
       "      <td>6.278760e+06</td>\n",
       "      <td>0.009600</td>\n",
       "      <td>1.0</td>\n",
       "      <td>107.207865</td>\n",
       "      <td>108.166503</td>\n",
       "      <td>106.338592</td>\n",
       "      <td>107.269724</td>\n",
       "      <td>6.278760e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>40.122937</td>\n",
       "      <td>40.198267</td>\n",
       "      <td>40.057692</td>\n",
       "      <td>40.137144</td>\n",
       "      <td>3.314386e+06</td>\n",
       "      <td>0.094086</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.589963</td>\n",
       "      <td>40.694568</td>\n",
       "      <td>40.490652</td>\n",
       "      <td>40.598655</td>\n",
       "      <td>3.314386e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>54.650000</td>\n",
       "      <td>56.700000</td>\n",
       "      <td>54.010000</td>\n",
       "      <td>55.070000</td>\n",
       "      <td>1.027500e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40.714758</td>\n",
       "      <td>42.242027</td>\n",
       "      <td>40.237952</td>\n",
       "      <td>41.027662</td>\n",
       "      <td>1.027500e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>92.270000</td>\n",
       "      <td>93.180000</td>\n",
       "      <td>91.500000</td>\n",
       "      <td>92.385000</td>\n",
       "      <td>4.023725e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>70.101463</td>\n",
       "      <td>70.880441</td>\n",
       "      <td>69.417812</td>\n",
       "      <td>70.119535</td>\n",
       "      <td>4.023725e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>119.310000</td>\n",
       "      <td>120.550000</td>\n",
       "      <td>117.850000</td>\n",
       "      <td>119.370000</td>\n",
       "      <td>5.502900e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>92.441676</td>\n",
       "      <td>93.664367</td>\n",
       "      <td>91.575772</td>\n",
       "      <td>92.473450</td>\n",
       "      <td>5.502900e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>161.935000</td>\n",
       "      <td>162.985000</td>\n",
       "      <td>160.845000</td>\n",
       "      <td>161.950000</td>\n",
       "      <td>7.644000e+06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>147.825960</td>\n",
       "      <td>148.985000</td>\n",
       "      <td>146.896456</td>\n",
       "      <td>147.799857</td>\n",
       "      <td>7.644000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>215.380000</td>\n",
       "      <td>215.900000</td>\n",
       "      <td>214.300000</td>\n",
       "      <td>215.800000</td>\n",
       "      <td>4.120730e+07</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>186.042617</td>\n",
       "      <td>186.491787</td>\n",
       "      <td>185.109726</td>\n",
       "      <td>186.405408</td>\n",
       "      <td>4.120730e+07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Open         High          Low        Close        Volume  \\\n",
       "count  4527.000000  4527.000000  4527.000000  4527.000000  4.527000e+03   \n",
       "mean    128.222709   129.396728   127.159447   128.298614  6.278760e+06   \n",
       "std      40.122937    40.198267    40.057692    40.137144  3.314386e+06   \n",
       "min      54.650000    56.700000    54.010000    55.070000  1.027500e+06   \n",
       "25%      92.270000    93.180000    91.500000    92.385000  4.023725e+06   \n",
       "50%     119.310000   120.550000   117.850000   119.370000  5.502900e+06   \n",
       "75%     161.935000   162.985000   160.845000   161.950000  7.644000e+06   \n",
       "max     215.380000   215.900000   214.300000   215.800000  4.120730e+07   \n",
       "\n",
       "       Ex-Dividend  Split Ratio    Adj. Open    Adj. High     Adj. Low  \\\n",
       "count  4527.000000       4527.0  4527.000000  4527.000000  4527.000000   \n",
       "mean      0.009600          1.0   107.207865   108.166503   106.338592   \n",
       "std       0.094086          0.0    40.589963    40.694568    40.490652   \n",
       "min       0.000000          1.0    40.714758    42.242027    40.237952   \n",
       "25%       0.000000          1.0    70.101463    70.880441    69.417812   \n",
       "50%       0.000000          1.0    92.441676    93.664367    91.575772   \n",
       "75%       0.000000          1.0   147.825960   148.985000   146.896456   \n",
       "max       1.500000          1.0   186.042617   186.491787   185.109726   \n",
       "\n",
       "        Adj. Close   Adj. Volume  \n",
       "count  4527.000000  4.527000e+03  \n",
       "mean    107.269724  6.278760e+06  \n",
       "std      40.598655  3.314386e+06  \n",
       "min      41.027662  1.027500e+06  \n",
       "25%      70.119535  4.023725e+06  \n",
       "50%      92.473450  5.502900e+06  \n",
       "75%     147.799857  7.644000e+06  \n",
       "max     186.405408  4.120730e+07  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the daily adjusted closing prices of the \"International Business Machines\" (IBM) stock market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'International Business Machines (IBM) - Daily Adjusted Historical Stock Closing Prices')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.plot(stock_data.index, stock_data['Adj. Close'], color='#9b59b6')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([pd.to_datetime('01-01-2000'), pd.to_datetime('31-12-2017')])\n",
    "ax.set_ylabel('[stock adj. closing price]', fontsize=10)\n",
    "ax.set_ylim(50, 220)\n",
    "\n",
    "# set plot title\n",
    "plt.title('International Business Machines (IBM) - Daily Adjusted Historical Stock Closing Prices', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pls. note that we plotted the \"adjusted\" daily closing prices of the IBM stock. The stock prices are adjusted by the quandl team by several types of regular corporate actions, e.g., stock dividends, stock splits. For further details on the applied adjustments pls. refer to the following reference: https://blog.quandl.com/guide-to-stock-price-calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the obtained and validated stock market data to the local data directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save retrieved data to local data directory\n",
    "stock_data.to_csv('data/ibm_data_2010_2017_daily.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2.0: Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will obtain daily returns of the retrieved daily adjusted closing prices. In addition we will convert the time-series of daily returns into a set of sequences $s$ of $n$ time steps respectively. The created sequences will then be used to learn a model using an LSTM based neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Daily Returns Calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the daily returns of the \"International Business Machines\" (IBM) daily adjusted closing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data['RETURN'] = stock_data['Adj. Close'].pct_change()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visually inspect the obtained daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'International Business Machines (IBM) - Daily Historical Stock Closing Prices')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# plot reconstruction error scatter plot\n",
    "ax.plot(stock_data.index, stock_data['RETURN'], color='#9b59b6')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels and limits\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([pd.to_datetime('01-01-2000'), pd.to_datetime('31-12-2017')])\n",
    "ax.set_ylabel('[daily stock returns]', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('International Business Machines (IBM) - Daily Historical Stock Closing Prices', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: Transform Time-Series Into Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the number of time-steps $n$ each individual sequence $s^{i}$ should be comprised of:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of timesteps\n",
    "sequence_length = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the initial return of the return time-series which is usually not applicable and therefore 'nan':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_daily_returns = stock_data['RETURN'][1:len(stock_data['RETURN'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract individual time sequences of length $n$ from the obtained daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over distinct normalized closing prices\n",
    "for i in range(0, stock_daily_returns.shape[0] - sequence_length):\n",
    "\n",
    "    # extract normalized closing price sequence \n",
    "    single_stock_sequence_data = stock_daily_returns[i:i + sequence_length].T\n",
    "\n",
    "    # case: initial sequence\n",
    "    if i == 0:\n",
    "\n",
    "        # convert to numpy array and collect sequence of normalized closing prices\n",
    "        stock_sequence_data = np.array(single_stock_sequence_data)\n",
    "\n",
    "    # case: non-initial sequence\n",
    "    else:\n",
    "\n",
    "        # convert to numpy array and collect sequence of normalized closing prices\n",
    "        stock_sequence_data  = np.vstack((stock_sequence_data , np.array(single_stock_sequence_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the top five collected normalized daily closing prices sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03396552,  0.03515974, -0.01724138, -0.00438596,  0.03964758],\n",
       "       [ 0.03515974, -0.01724138, -0.00438596,  0.03964758,  0.00847458],\n",
       "       [-0.01724138, -0.00438596,  0.03964758,  0.00847458,  0.00420168],\n",
       "       [-0.00438596,  0.03964758,  0.00847458,  0.00420168, -0.01046025],\n",
       "       [ 0.03964758,  0.00847458,  0.00420168, -0.01046025,  0.01158562]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stock_sequence_data[0:5,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3: Prepare Sequences for Neural Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set split fraction to split return sequences into training (in-sample) and validation (out-of-sample) sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_fraction = 0.9\n",
    "split_row = int(stock_sequence_data.shape[0] * split_fraction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split obtained return sequences into training (in-sample) sequences $s^{i}_{train}$ and validation (out-of-sample) sequences $s^{i}_{valid}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = stock_sequence_data[:split_row,]\n",
    "valid_sequences = stock_sequence_data[split_row:,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine count (shape) of adjusted daily return train sequences $s^{i}_{train}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4068, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine count (shape) of adjusted daily return train sequences $s^{i}_{valid}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(453, 5)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_sequences.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate each training sequence $s^{i}$ into time-steps of input returns denoted by $s^{i}_{train, input}=\\{r_{t-n-1}, ..., r_{t-1}, r_{t}\\}$ and the time-step of the to be predicted target return denoted by $s^{i}_{train, target}=r_{t+1}$. In addition, we convert both the input returns as well as the target returns to PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences_input = torch.from_numpy(train_sequences[:, :-1]).float()\n",
    "train_sequences_target = torch.from_numpy(train_sequences[:, 1:]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate each validation sequence $s^{i}$ into time-steps of input returns denoted by $s^{i}_{valid, input}=\\{r_{t-n-1}, ..., r_{t-1}, r_{t}\\}$ and the time-step of the to be predicted target return denoted by $s^{i}_{valid, target}=r_{t+1}$. In addition, we convert both the input returns as well as the target returns to PyTorch tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_sequences_input = torch.from_numpy(valid_sequences[:, :-1]).float()\n",
    "valid_sequences_target = torch.from_numpy(valid_sequences[:, 1:]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to train a LSTM neural network, we tailor the dataset class provided by the PyTorch library. We overwrite the individual functions of the dataset class. So that our dataset will supply the neural network with the individual training sequences $s^{i}_{train, input}$ and corresponding targets $s^{i}_{train, target}$ throughout the training process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define daily returns dataset\n",
    "class DailyReturnsDataset(data.Dataset):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self, sequences, targets):\n",
    "\n",
    "        # init sequences and corresponding targets\n",
    "        self.sequences = sequences\n",
    "        self.targets = targets\n",
    "\n",
    "    # define length method \n",
    "    def __len__(self):\n",
    "\n",
    "        # returns the number of samples\n",
    "        return len(self.targets)\n",
    "\n",
    "    # define get item method\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # determine single sequence and corresponding target\n",
    "        sequence = self.sequences[index, :]\n",
    "        target = self.targets[index, :]\n",
    "\n",
    "        # return sequences and target\n",
    "        return sequence, target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have specified the daily returns dataset class we instantiate it using the new daily closing dataset using the prepared training input sequences $s^{i}_{train, input}$ and corresponding targets $s^{i}_{train, target}$: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = DailyReturnsDataset(train_sequences_input, train_sequences_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works by getting the 10th sequence and its corresponding targets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.0324, -0.0042,  0.0210,  0.0000]),\n",
       " tensor([-0.0042,  0.0210,  0.0000, -0.0196]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.__getitem__(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3.0 Neural Network Implementation and Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement the LSTM architecture of the to be learned time series model. Furthermore, we will specify the loss-function, learning-rate and optimization technique used in the network training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Implementation of the LSTM Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement the architecture of the LSTM neural network we want to utilize in order to predict future returns of our financial time series data. The neural network, which we name **'LSTM_NN'** consists in total of three layers. The first two layers correspond to LSTM cells while the third layer corresponds to a fully-connected linear layer. The individual gates of the LSTM cell are calculated according to the following functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "$i=(W_{ii}x+b_{ii}+W_{hi}h+b_{hi}), \\\\\n",
    "f=(W_{if}x+b_{if}+W_{hf}h+b_{hf}), \\\\\n",
    "g=tanh(W_{ig}x+b_{ig}+W_{hg}h+b_{hg}), \\\\\n",
    "o=(W_{io}x+b_{io}+W_{ho}h+b_{ho}), \\\\\n",
    "c=fc+ig, \\\\\n",
    "h=otanh(c).$ \n",
    "</center>\n",
    "\n",
    "(Source: https://pytorch.org/docs/stable/nn.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the two LSTM cells exhibit a hidden state 51 dimensions. The third linear squeezes the 51 hidden state dimensions of the second (last) LSTM cell into a single output dimension. The single output signal of the linear layer  refers to the future return predicted by the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the LSTMNet network architecture\n",
    "class LSTMNet(nn.Module):\n",
    "\n",
    "    # define class constructor\n",
    "    def __init__(self):\n",
    "\n",
    "        super(LSTMNet, self).__init__()\n",
    "\n",
    "        # define lstm nn architecture\n",
    "        self.lstm1 = nn.LSTMCell(1, 51)  # first lstm layer\n",
    "        self.lstm2 = nn.LSTMCell(51, 51)  # second lstm layer\n",
    "        self.linear = nn.Linear(51, 1)  # final linear layer\n",
    "\n",
    "    # define network forward pass\n",
    "    def forward(self, input):\n",
    "\n",
    "        # init predictions\n",
    "        predictions = []\n",
    "\n",
    "        # init the lstm hidden states\n",
    "        h_t = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "        h_t2 = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "\n",
    "        # init the lstm cell states\n",
    "        c_t = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "        c_t2 = torch.zeros(input.size(0), 51, dtype=torch.float)\n",
    "        \n",
    "        # iterate over distinct time steps\n",
    "        for i, input_t in enumerate(input.chunk(input.size(1), dim=1)):\n",
    "\n",
    "            # propagate through time step data\n",
    "            h_t, c_t = self.lstm1(input_t, (h_t, c_t))\n",
    "            h_t2, c_t2 = self.lstm2(h_t, (h_t2, c_t2))\n",
    "            prediction = self.linear(h_t2)\n",
    "            \n",
    "            # collect predictions\n",
    "            predictions += [prediction]\n",
    "\n",
    "        # stack predictions\n",
    "        predictions = torch.stack(predictions, 1).squeeze(2)\n",
    "\n",
    "        # return predictions\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, that we have implemented our first neural network we are ready to instantiate a model to be trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = LSTMNet()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is initialized, we can visualize the model structure and review the implemented network architecture by execution of the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG] MNISTNet architecture:\n",
      "\n",
      "LSTMNet(\n",
      "  (lstm1): LSTMCell(1, 51)\n",
      "  (lstm2): LSTMCell(51, 51)\n",
      "  (linear): Linear(in_features=51, out_features=1, bias=True)\n",
      ")\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the initialized architectures\n",
    "print('[LOG] MNISTNet architecture:\\n\\n{}\\n'.format(lstm_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like intended? Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Definition of the Training Loss Function and Learning Rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now good to train the network. However, prior to starting the training, we need to define an appropriate loss function. Remember, we aim to train our model to learn a set of model parameters $\\theta$ that minimize the prediction error of the true return $r_{t+1}$ and the by the model predicted return $\\hat{r}_{t+1}$ at a given time-step $t+1$ of sequence $s^{i}$. In other words, for a given sequence of historic returns we aim to learn a function $f_\\theta$ that is capable to predicts the return of the next timestep as faithfully as possible, as expressed by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\hat{r}_{t+1} = f_\\theta(r_{t}, r_{t-1}, ..., r_{t-n})$. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereby, the training objective is to learn a set of optimal model parameters $\\theta^*$ that optimize $\\min_{\\theta} \\|r_{t+1} - f_\\theta(r_{t}, r_{t-1}, ..., r_{t-n})\\|$ over all time-steps $t$ contained in the set of training sequences $s_{train}$. To achieve this optimization objective, one typically minimizes a loss function $\\mathcal{L_{\\theta}}$ while training the neural network. In this lab we use the **'Mean Squared Error (MSE)'** loss, as denoted by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> $\\mathcal{L}^{MSE}_{\\theta} (r_{t+1}, \\hat{r}_{t+1}) = \\frac{1}{N} \\sum_{i=1}^N \\| r_{t+1} - \\hat{r}_{t+1}\\|^{2}$, </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout the training process the PyTorch library will automatically calculate the loss magnitude, compute the gradient, and update the parameters $\\theta$ of the LSTM neural network. We will use the **\"Adaptive Moment Estimation Optimization\" (ADAM)** technique to optimize the network parameters. Furthermore, we specify a constant learning rate of $l = 1e-06$. For each training step the optimizer will update the model parameters $\\theta$ values according to degree of prediction error (the MSE loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-06 # set constant learning rate\n",
    "optimizer = optim.Adam(lstm_model.parameters(), lr=learning_rate) # define optimization technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully implemented and defined the three ANN building blocks let's take some time to review the `LSTMNet` model definition as well as the `MSE loss` function. Please, read the above code and comments carefully and don't hesitate to let us know any questions you might have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.0. Training the Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will train the LSTM neural network model (as implemented in the section above) using the prepared dataset of daily return sequences. Therefore, we will have a detailed look into the distinct training steps and monitor the training progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1. Preparing the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now start to learn a model by training the NN for `2'000 epochs` in `mini-batches of size of 128 sequences` per batch. This implies that the whole dataset will be fed to the NN 2'000 times in chunks of 128 sequences yielding to `32 mini-batches` (4'068 training sequences / 128 sequences per mini-batch) per epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the training parameters\n",
    "num_epochs = 2000 # number of training epochs\n",
    "mini_batch_size = 128 # size of the mini-batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, lets specify and instantiate a corresponding PyTorch data loader that feeds the image tensors to our neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = dataloader.DataLoader(train_dataset, batch_size=mini_batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2. Running the Network Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we start training the model. The training procedure of each mini-batch is performed as follows: \n",
    "\n",
    ">1. do a forward pass through the LSTMNet network, \n",
    ">2. compute the mean-squared prediction error $\\mathcal{L}^{MSE}_{\\theta} (r_{t+1}, \\hat{r}_{t+1}) = \\frac{1}{N} \\sum_{i=1}^N \\| r_{t+1} - \\hat{r}_{t+1}\\|^{2}$, \n",
    ">3. do a backward pass through the LSTMNet network, and \n",
    ">4. update the parameters of the network $f_\\theta(\\cdot)$.\n",
    "\n",
    "To ensure learning while training the LSTM model we will monitor whether the loss decreases with progressing training. Therefore, we obtain and evaluate the mean prediction performance over all mini-batches in each training epoch. Based on this evaluation we can conclude on the training progress and whether the loss is converging (indicating that the model might not improve any further).\n",
    "\n",
    "The following elements of the network training code below should be given particular attention:\n",
    " \n",
    ">- `loss.backward()` computes the gradients based on the magnitude of the reconstruction loss,\n",
    ">- `optimizer.step()` updates the network parameters based on the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:37:17] epoch: 0 train-loss: 0.0036956439798814245\n",
      "[LOG 20190630-20:37:17] epoch: 1 train-loss: 0.003645348668214865\n",
      "[LOG 20190630-20:37:18] epoch: 2 train-loss: 0.0035876172696589492\n",
      "[LOG 20190630-20:37:18] epoch: 3 train-loss: 0.0035272282184450887\n",
      "[LOG 20190630-20:37:19] epoch: 4 train-loss: 0.0034660340897971764\n",
      "[LOG 20190630-20:37:19] epoch: 5 train-loss: 0.003404784933081828\n",
      "[LOG 20190630-20:37:20] epoch: 6 train-loss: 0.0033438507962273434\n",
      "[LOG 20190630-20:37:20] epoch: 7 train-loss: 0.0032834291414474137\n",
      "[LOG 20190630-20:37:21] epoch: 8 train-loss: 0.0032236410115729086\n",
      "[LOG 20190630-20:37:21] epoch: 9 train-loss: 0.0031645568960811943\n",
      "[LOG 20190630-20:37:22] epoch: 10 train-loss: 0.00310622916003922\n",
      "[LOG 20190630-20:37:22] epoch: 10 new best train-loss: 0.00310622916003922 found\n",
      "[LOG 20190630-20:37:22] epoch: 11 train-loss: 0.0030486889227177016\n",
      "[LOG 20190630-20:37:23] epoch: 12 train-loss: 0.0029919592416263185\n",
      "[LOG 20190630-20:37:23] epoch: 13 train-loss: 0.0029360501357587054\n",
      "[LOG 20190630-20:37:24] epoch: 14 train-loss: 0.0028809713330701925\n",
      "[LOG 20190630-20:37:24] epoch: 15 train-loss: 0.0028267292072996497\n",
      "[LOG 20190630-20:37:25] epoch: 16 train-loss: 0.0027733197202906013\n",
      "[LOG 20190630-20:37:25] epoch: 17 train-loss: 0.0027207464809180237\n",
      "[LOG 20190630-20:37:26] epoch: 18 train-loss: 0.0026690028098528273\n",
      "[LOG 20190630-20:37:26] epoch: 19 train-loss: 0.002618085898575373\n",
      "[LOG 20190630-20:37:26] epoch: 20 train-loss: 0.002567992407421116\n",
      "[LOG 20190630-20:37:26] epoch: 20 new best train-loss: 0.002567992407421116 found\n",
      "[LOG 20190630-20:37:27] epoch: 21 train-loss: 0.002518714107281994\n",
      "[LOG 20190630-20:37:27] epoch: 22 train-loss: 0.002470246756274719\n",
      "[LOG 20190630-20:37:28] epoch: 23 train-loss: 0.002422580888378434\n",
      "[LOG 20190630-20:37:28] epoch: 24 train-loss: 0.0023757115995977074\n",
      "[LOG 20190630-20:37:29] epoch: 25 train-loss: 0.002329630369786173\n",
      "[LOG 20190630-20:37:29] epoch: 26 train-loss: 0.0022843296246719547\n",
      "[LOG 20190630-20:37:30] epoch: 27 train-loss: 0.0022398028668249026\n",
      "[LOG 20190630-20:37:30] epoch: 28 train-loss: 0.00219603968798765\n",
      "[LOG 20190630-20:37:31] epoch: 29 train-loss: 0.0021530344420170877\n",
      "[LOG 20190630-20:37:31] epoch: 30 train-loss: 0.0021107777356519364\n",
      "[LOG 20190630-20:37:31] epoch: 30 new best train-loss: 0.0021107777356519364 found\n",
      "[LOG 20190630-20:37:32] epoch: 31 train-loss: 0.0020692620455520228\n",
      "[LOG 20190630-20:37:32] epoch: 32 train-loss: 0.0020284798520151526\n",
      "[LOG 20190630-20:37:32] epoch: 33 train-loss: 0.0019884222274413332\n",
      "[LOG 20190630-20:37:33] epoch: 34 train-loss: 0.001949083074578084\n",
      "[LOG 20190630-20:37:33] epoch: 35 train-loss: 0.0019104534876532853\n",
      "[LOG 20190630-20:37:34] epoch: 36 train-loss: 0.0018725272639130708\n",
      "[LOG 20190630-20:37:34] epoch: 37 train-loss: 0.001835297287470894\n",
      "[LOG 20190630-20:37:35] epoch: 38 train-loss: 0.0017987558239838108\n",
      "[LOG 20190630-20:37:35] epoch: 39 train-loss: 0.001762897038133815\n",
      "[LOG 20190630-20:37:36] epoch: 40 train-loss: 0.0017277131046284921\n",
      "[LOG 20190630-20:37:36] epoch: 40 new best train-loss: 0.0017277131046284921 found\n",
      "[LOG 20190630-20:37:36] epoch: 41 train-loss: 0.0016931978352658916\n",
      "[LOG 20190630-20:37:37] epoch: 42 train-loss: 0.001659344277868513\n",
      "[LOG 20190630-20:37:37] epoch: 43 train-loss: 0.0016261457676591817\n",
      "[LOG 20190630-20:37:38] epoch: 44 train-loss: 0.0015935969131533056\n",
      "[LOG 20190630-20:37:38] epoch: 45 train-loss: 0.0015616900418535806\n",
      "[LOG 20190630-20:37:38] epoch: 46 train-loss: 0.001530418452603044\n",
      "[LOG 20190630-20:37:39] epoch: 47 train-loss: 0.001499777237768285\n",
      "[LOG 20190630-20:37:39] epoch: 48 train-loss: 0.0014697596780024469\n",
      "[LOG 20190630-20:37:40] epoch: 49 train-loss: 0.0014403589739231393\n",
      "[LOG 20190630-20:37:40] epoch: 50 train-loss: 0.0014115700250840746\n",
      "[LOG 20190630-20:37:40] epoch: 50 new best train-loss: 0.0014115700250840746 found\n",
      "[LOG 20190630-20:37:41] epoch: 51 train-loss: 0.00138338643591851\n",
      "[LOG 20190630-20:37:41] epoch: 52 train-loss: 0.0013558024584199302\n",
      "[LOG 20190630-20:37:42] epoch: 53 train-loss: 0.001328811969870003\n",
      "[LOG 20190630-20:37:42] epoch: 54 train-loss: 0.0013024096042499878\n",
      "[LOG 20190630-20:37:43] epoch: 55 train-loss: 0.0012765901010425296\n",
      "[LOG 20190630-20:37:43] epoch: 56 train-loss: 0.0012513470010162564\n",
      "[LOG 20190630-20:37:44] epoch: 57 train-loss: 0.0012266752873983933\n",
      "[LOG 20190630-20:37:44] epoch: 58 train-loss: 0.0012025703199469717\n",
      "[LOG 20190630-20:37:44] epoch: 59 train-loss: 0.0011790254793595523\n",
      "[LOG 20190630-20:37:45] epoch: 60 train-loss: 0.0011560364637261955\n",
      "[LOG 20190630-20:37:45] epoch: 60 new best train-loss: 0.0011560364637261955 found\n",
      "[LOG 20190630-20:37:45] epoch: 61 train-loss: 0.0011335984945617383\n",
      "[LOG 20190630-20:37:46] epoch: 62 train-loss: 0.0011117050180473598\n",
      "[LOG 20190630-20:37:46] epoch: 63 train-loss: 0.0010903520396823296\n",
      "[LOG 20190630-20:37:47] epoch: 64 train-loss: 0.001069534682756057\n",
      "[LOG 20190630-20:37:47] epoch: 65 train-loss: 0.0010492474430066068\n",
      "[LOG 20190630-20:37:48] epoch: 66 train-loss: 0.0010294860076101031\n",
      "[LOG 20190630-20:37:48] epoch: 67 train-loss: 0.0010102454198204214\n",
      "[LOG 20190630-20:37:49] epoch: 68 train-loss: 0.000991521106698201\n",
      "[LOG 20190630-20:37:49] epoch: 69 train-loss: 0.0009733081624290207\n",
      "[LOG 20190630-20:37:50] epoch: 70 train-loss: 0.000955602154135704\n",
      "[LOG 20190630-20:37:50] epoch: 70 new best train-loss: 0.000955602154135704 found\n",
      "[LOG 20190630-20:37:50] epoch: 71 train-loss: 0.0009383991855429485\n",
      "[LOG 20190630-20:37:50] epoch: 72 train-loss: 0.0009216933540301397\n",
      "[LOG 20190630-20:37:51] epoch: 73 train-loss: 0.0009054814399860334\n",
      "[LOG 20190630-20:37:51] epoch: 74 train-loss: 0.0008897589887055801\n",
      "[LOG 20190630-20:37:52] epoch: 75 train-loss: 0.0008745209943299415\n",
      "[LOG 20190630-20:37:52] epoch: 76 train-loss: 0.0008597638643550454\n",
      "[LOG 20190630-20:37:53] epoch: 77 train-loss: 0.00084548306040233\n",
      "[LOG 20190630-20:37:53] epoch: 78 train-loss: 0.0008316747516801115\n",
      "[LOG 20190630-20:37:54] epoch: 79 train-loss: 0.0008183339450624771\n",
      "[LOG 20190630-20:37:54] epoch: 80 train-loss: 0.0008054575373535044\n",
      "[LOG 20190630-20:37:54] epoch: 80 new best train-loss: 0.0008054575373535044 found\n",
      "[LOG 20190630-20:37:55] epoch: 81 train-loss: 0.0007930414449219825\n",
      "[LOG 20190630-20:37:55] epoch: 82 train-loss: 0.000781080627348274\n",
      "[LOG 20190630-20:37:56] epoch: 83 train-loss: 0.0007695719104958698\n",
      "[LOG 20190630-20:37:56] epoch: 84 train-loss: 0.0007585115290567046\n",
      "[LOG 20190630-20:37:57] epoch: 85 train-loss: 0.0007478948737116298\n",
      "[LOG 20190630-20:37:57] epoch: 86 train-loss: 0.000737717962692841\n",
      "[LOG 20190630-20:37:57] epoch: 87 train-loss: 0.0007279781784745865\n",
      "[LOG 20190630-20:37:58] epoch: 88 train-loss: 0.0007186707334767561\n",
      "[LOG 20190630-20:37:59] epoch: 89 train-loss: 0.0007097915986378212\n",
      "[LOG 20190630-20:37:59] epoch: 90 train-loss: 0.0007013370759523241\n",
      "[LOG 20190630-20:37:59] epoch: 90 new best train-loss: 0.0007013370759523241 found\n",
      "[LOG 20190630-20:38:00] epoch: 91 train-loss: 0.0006933043468961841\n",
      "[LOG 20190630-20:38:00] epoch: 92 train-loss: 0.0006856883728687535\n",
      "[LOG 20190630-20:38:01] epoch: 93 train-loss: 0.0006784857287129853\n",
      "[LOG 20190630-20:38:01] epoch: 94 train-loss: 0.0006716929501635605\n",
      "[LOG 20190630-20:38:02] epoch: 95 train-loss: 0.0006653061527686077\n",
      "[LOG 20190630-20:38:02] epoch: 96 train-loss: 0.0006593213583983015\n",
      "[LOG 20190630-20:38:03] epoch: 97 train-loss: 0.0006537347753692302\n",
      "[LOG 20190630-20:38:03] epoch: 98 train-loss: 0.0006485429648819263\n",
      "[LOG 20190630-20:38:03] epoch: 99 train-loss: 0.0006437413803723757\n",
      "[LOG 20190630-20:38:04] epoch: 100 train-loss: 0.0006393270323314937\n",
      "[LOG 20190630-20:38:04] epoch: 100 new best train-loss: 0.0006393270323314937 found\n",
      "[LOG 20190630-20:38:04] epoch: 101 train-loss: 0.0006352955215334077\n",
      "[LOG 20190630-20:38:05] epoch: 102 train-loss: 0.0006316433737083571\n",
      "[LOG 20190630-20:38:05] epoch: 103 train-loss: 0.0006283658622123767\n",
      "[LOG 20190630-20:38:06] epoch: 104 train-loss: 0.0006254599602470989\n",
      "[LOG 20190630-20:38:06] epoch: 105 train-loss: 0.0006229214304767083\n",
      "[LOG 20190630-20:38:07] epoch: 106 train-loss: 0.0006207462947713793\n",
      "[LOG 20190630-20:38:07] epoch: 107 train-loss: 0.0006189304594954592\n",
      "[LOG 20190630-20:38:08] epoch: 108 train-loss: 0.0006174703094075085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:38:08] epoch: 109 train-loss: 0.0006163620319057372\n",
      "[LOG 20190630-20:38:09] epoch: 110 train-loss: 0.0006156018444016809\n",
      "[LOG 20190630-20:38:09] epoch: 110 new best train-loss: 0.0006156018444016809 found\n",
      "[LOG 20190630-20:38:09] epoch: 111 train-loss: 0.0006151856450742343\n",
      "[LOG 20190630-20:38:10] epoch: 112 train-loss: 0.0006151097904876224\n",
      "[LOG 20190630-20:38:10] epoch: 113 train-loss: 0.0006153704453026876\n",
      "[LOG 20190630-20:38:11] epoch: 114 train-loss: 0.0006159639469842659\n",
      "[LOG 20190630-20:38:11] epoch: 115 train-loss: 0.0006168863501443411\n",
      "[LOG 20190630-20:38:11] epoch: 116 train-loss: 0.0006181337757880101\n",
      "[LOG 20190630-20:38:12] epoch: 117 train-loss: 0.0006197021602929453\n",
      "[LOG 20190630-20:38:12] epoch: 118 train-loss: 0.0006215876364876749\n",
      "[LOG 20190630-20:38:13] epoch: 119 train-loss: 0.000623786031610507\n",
      "[LOG 20190630-20:38:13] epoch: 120 train-loss: 0.0006262934311962454\n",
      "[LOG 20190630-20:38:14] epoch: 121 train-loss: 0.0006291056479312829\n",
      "[LOG 20190630-20:38:14] epoch: 122 train-loss: 0.000632218237115012\n",
      "[LOG 20190630-20:38:15] epoch: 123 train-loss: 0.0006356274971039966\n",
      "[LOG 20190630-20:38:15] epoch: 124 train-loss: 0.0006393288576873601\n",
      "[LOG 20190630-20:38:16] epoch: 125 train-loss: 0.0006433182625187328\n",
      "[LOG 20190630-20:38:16] epoch: 126 train-loss: 0.0006475909394794144\n",
      "[LOG 20190630-20:38:17] epoch: 127 train-loss: 0.0006521433142552269\n",
      "[LOG 20190630-20:38:17] epoch: 128 train-loss: 0.0006569708839379018\n",
      "[LOG 20190630-20:38:17] epoch: 129 train-loss: 0.000662068961901241\n",
      "[LOG 20190630-20:38:18] epoch: 130 train-loss: 0.0006674333453702275\n",
      "[LOG 20190630-20:38:18] epoch: 131 train-loss: 0.0006730595332555822\n",
      "[LOG 20190630-20:38:19] epoch: 132 train-loss: 0.0006789435656173737\n",
      "[LOG 20190630-20:38:19] epoch: 133 train-loss: 0.0006850805848443997\n",
      "[LOG 20190630-20:38:20] epoch: 134 train-loss: 0.0006914657542438363\n",
      "[LOG 20190630-20:38:20] epoch: 135 train-loss: 0.0006980948210184579\n",
      "[LOG 20190630-20:38:21] epoch: 136 train-loss: 0.000704963367752498\n",
      "[LOG 20190630-20:38:21] epoch: 137 train-loss: 0.0007120661957742414\n",
      "[LOG 20190630-20:38:22] epoch: 138 train-loss: 0.0007193985966296168\n",
      "[LOG 20190630-20:38:22] epoch: 139 train-loss: 0.000726955655409256\n",
      "[LOG 20190630-20:38:23] epoch: 140 train-loss: 0.0007347329337790143\n",
      "[LOG 20190630-20:38:23] epoch: 141 train-loss: 0.0007427247073792387\n",
      "[LOG 20190630-20:38:24] epoch: 142 train-loss: 0.0007509263723477488\n",
      "[LOG 20190630-20:38:24] epoch: 143 train-loss: 0.0007593328573420877\n",
      "[LOG 20190630-20:38:24] epoch: 144 train-loss: 0.0007679385507799452\n",
      "[LOG 20190630-20:38:25] epoch: 145 train-loss: 0.0007767380975565175\n",
      "[LOG 20190630-20:38:25] epoch: 146 train-loss: 0.0007857265209167963\n",
      "[LOG 20190630-20:38:26] epoch: 147 train-loss: 0.0007948985403345432\n",
      "[LOG 20190630-20:38:26] epoch: 148 train-loss: 0.0008042476474656723\n",
      "[LOG 20190630-20:38:27] epoch: 149 train-loss: 0.0008137681743392022\n",
      "[LOG 20190630-20:38:27] epoch: 150 train-loss: 0.0008234548331529368\n",
      "[LOG 20190630-20:38:28] epoch: 151 train-loss: 0.0008333021942235064\n",
      "[LOG 20190630-20:38:28] epoch: 152 train-loss: 0.0008433026760030771\n",
      "[LOG 20190630-20:38:29] epoch: 153 train-loss: 0.0008534505923307734\n",
      "[LOG 20190630-20:38:29] epoch: 154 train-loss: 0.0008637400005682139\n",
      "[LOG 20190630-20:38:30] epoch: 155 train-loss: 0.0008741648071008967\n",
      "[LOG 20190630-20:38:30] epoch: 156 train-loss: 0.0008847167355270358\n",
      "[LOG 20190630-20:38:30] epoch: 157 train-loss: 0.000895390336154378\n",
      "[LOG 20190630-20:38:31] epoch: 158 train-loss: 0.0009061791006388376\n",
      "[LOG 20190630-20:38:31] epoch: 159 train-loss: 0.0009170751854981063\n",
      "[LOG 20190630-20:38:32] epoch: 160 train-loss: 0.000928072116948897\n",
      "[LOG 20190630-20:38:32] epoch: 161 train-loss: 0.0009391622115799692\n",
      "[LOG 20190630-20:38:33] epoch: 162 train-loss: 0.0009503386972937733\n",
      "[LOG 20190630-20:38:33] epoch: 163 train-loss: 0.0009615932358428836\n",
      "[LOG 20190630-20:38:34] epoch: 164 train-loss: 0.000972919237028691\n",
      "[LOG 20190630-20:38:34] epoch: 165 train-loss: 0.0009843091156653827\n",
      "[LOG 20190630-20:38:35] epoch: 166 train-loss: 0.0009957542915799422\n",
      "[LOG 20190630-20:38:35] epoch: 167 train-loss: 0.0010072479908558307\n",
      "[LOG 20190630-20:38:36] epoch: 168 train-loss: 0.0010187819880229654\n",
      "[LOG 20190630-20:38:36] epoch: 169 train-loss: 0.0010303476483386476\n",
      "[LOG 20190630-20:38:37] epoch: 170 train-loss: 0.0010419372501928592\n",
      "[LOG 20190630-20:38:37] epoch: 171 train-loss: 0.0010535419860389084\n",
      "[LOG 20190630-20:38:37] epoch: 172 train-loss: 0.0010651544416759862\n",
      "[LOG 20190630-20:38:38] epoch: 173 train-loss: 0.001076765991456341\n",
      "[LOG 20190630-20:38:38] epoch: 174 train-loss: 0.0010883665654546348\n",
      "[LOG 20190630-20:38:39] epoch: 175 train-loss: 0.001099947519833222\n",
      "[LOG 20190630-20:38:39] epoch: 176 train-loss: 0.001111502586354618\n",
      "[LOG 20190630-20:38:40] epoch: 177 train-loss: 0.0011230183681618655\n",
      "[LOG 20190630-20:38:40] epoch: 178 train-loss: 0.0011344875838403823\n",
      "[LOG 20190630-20:38:41] epoch: 179 train-loss: 0.0011459012293926207\n",
      "[LOG 20190630-20:38:41] epoch: 180 train-loss: 0.001157247761511826\n",
      "[LOG 20190630-20:38:42] epoch: 181 train-loss: 0.0011685185108945007\n",
      "[LOG 20190630-20:38:42] epoch: 182 train-loss: 0.0011797037077485584\n",
      "[LOG 20190630-20:38:43] epoch: 183 train-loss: 0.0011907908046850935\n",
      "[LOG 20190630-20:38:43] epoch: 184 train-loss: 0.0012017712851957185\n",
      "[LOG 20190630-20:38:43] epoch: 185 train-loss: 0.0012126341607654467\n",
      "[LOG 20190630-20:38:44] epoch: 186 train-loss: 0.0012233685392857296\n",
      "[LOG 20190630-20:38:44] epoch: 187 train-loss: 0.0012339626537141157\n",
      "[LOG 20190630-20:38:45] epoch: 188 train-loss: 0.0012444077401596587\n",
      "[LOG 20190630-20:38:45] epoch: 189 train-loss: 0.0012546911457320675\n",
      "[LOG 20190630-20:38:46] epoch: 190 train-loss: 0.0012648026313399896\n",
      "[LOG 20190630-20:38:46] epoch: 191 train-loss: 0.0012747319706249982\n",
      "[LOG 20190630-20:38:47] epoch: 192 train-loss: 0.001284466950892238\n",
      "[LOG 20190630-20:38:47] epoch: 193 train-loss: 0.0012939982116222382\n",
      "[LOG 20190630-20:38:48] epoch: 194 train-loss: 0.0013033143186476082\n",
      "[LOG 20190630-20:38:48] epoch: 195 train-loss: 0.0013124053948558867\n",
      "[LOG 20190630-20:38:49] epoch: 196 train-loss: 0.0013212618105171714\n",
      "[LOG 20190630-20:38:49] epoch: 197 train-loss: 0.0013298727244546171\n",
      "[LOG 20190630-20:38:49] epoch: 198 train-loss: 0.0013382286815613043\n",
      "[LOG 20190630-20:38:50] epoch: 199 train-loss: 0.0013463204340951052\n",
      "[LOG 20190630-20:38:50] epoch: 200 train-loss: 0.0013541379157686606\n",
      "[LOG 20190630-20:38:51] epoch: 201 train-loss: 0.0013616736068797763\n",
      "[LOG 20190630-20:38:51] epoch: 202 train-loss: 0.0013689177103515249\n",
      "[LOG 20190630-20:38:52] epoch: 203 train-loss: 0.0013758639070147183\n",
      "[LOG 20190630-20:38:52] epoch: 204 train-loss: 0.001382503043714678\n",
      "[LOG 20190630-20:38:53] epoch: 205 train-loss: 0.001388826931361109\n",
      "[LOG 20190630-20:38:53] epoch: 206 train-loss: 0.0013948313717264682\n",
      "[LOG 20190630-20:38:54] epoch: 207 train-loss: 0.0014005063858348876\n",
      "[LOG 20190630-20:38:54] epoch: 208 train-loss: 0.001405848623107886\n",
      "[LOG 20190630-20:38:55] epoch: 209 train-loss: 0.0014108505129115656\n",
      "[LOG 20190630-20:38:55] epoch: 210 train-loss: 0.0014155088902043644\n",
      "[LOG 20190630-20:38:56] epoch: 211 train-loss: 0.00141981659544399\n",
      "[LOG 20190630-20:38:56] epoch: 212 train-loss: 0.0014237717296055052\n",
      "[LOG 20190630-20:38:57] epoch: 213 train-loss: 0.001427369745215401\n",
      "[LOG 20190630-20:38:57] epoch: 214 train-loss: 0.0014306070552265737\n",
      "[LOG 20190630-20:38:57] epoch: 215 train-loss: 0.0014334810475702398\n",
      "[LOG 20190630-20:38:58] epoch: 216 train-loss: 0.0014359905253513716\n",
      "[LOG 20190630-20:38:58] epoch: 217 train-loss: 0.0014381323489942588\n",
      "[LOG 20190630-20:38:59] epoch: 218 train-loss: 0.0014399067076737992\n",
      "[LOG 20190630-20:38:59] epoch: 219 train-loss: 0.0014413128556043375\n",
      "[LOG 20190630-20:39:00] epoch: 220 train-loss: 0.001442350596335018\n",
      "[LOG 20190630-20:39:00] epoch: 221 train-loss: 0.0014430196897592396\n",
      "[LOG 20190630-20:39:01] epoch: 222 train-loss: 0.001443322209524922\n",
      "[LOG 20190630-20:39:01] epoch: 223 train-loss: 0.0014432580865104683\n",
      "[LOG 20190630-20:39:02] epoch: 224 train-loss: 0.0014428294234676287\n",
      "[LOG 20190630-20:39:02] epoch: 225 train-loss: 0.0014420386323763523\n",
      "[LOG 20190630-20:39:03] epoch: 226 train-loss: 0.0014408876486413646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:39:03] epoch: 227 train-loss: 0.0014393800811376423\n",
      "[LOG 20190630-20:39:04] epoch: 228 train-loss: 0.0014375186256074812\n",
      "[LOG 20190630-20:39:04] epoch: 229 train-loss: 0.0014353060796565842\n",
      "[LOG 20190630-20:39:04] epoch: 230 train-loss: 0.001432746576028876\n",
      "[LOG 20190630-20:39:05] epoch: 231 train-loss: 0.0014298442474682815\n",
      "[LOG 20190630-20:39:05] epoch: 232 train-loss: 0.0014266024772950914\n",
      "[LOG 20190630-20:39:06] epoch: 233 train-loss: 0.0014230258420866448\n",
      "[LOG 20190630-20:39:06] epoch: 234 train-loss: 0.0014191190311976243\n",
      "[LOG 20190630-20:39:07] epoch: 235 train-loss: 0.001414886733982712\n",
      "[LOG 20190630-20:39:07] epoch: 236 train-loss: 0.0014103337634878699\n",
      "[LOG 20190630-20:39:08] epoch: 237 train-loss: 0.0014054650273465086\n",
      "[LOG 20190630-20:39:08] epoch: 238 train-loss: 0.0014002865900692996\n",
      "[LOG 20190630-20:39:09] epoch: 239 train-loss: 0.001394803239236353\n",
      "[LOG 20190630-20:39:09] epoch: 240 train-loss: 0.0013890208247175906\n",
      "[LOG 20190630-20:39:10] epoch: 241 train-loss: 0.00138294508724357\n",
      "[LOG 20190630-20:39:10] epoch: 242 train-loss: 0.0013765816802333575\n",
      "[LOG 20190630-20:39:10] epoch: 243 train-loss: 0.001369937053823378\n",
      "[LOG 20190630-20:39:11] epoch: 244 train-loss: 0.0013630188004754018\n",
      "[LOG 20190630-20:39:11] epoch: 245 train-loss: 0.0013558329628722277\n",
      "[LOG 20190630-20:39:12] epoch: 246 train-loss: 0.0013483861584973056\n",
      "[LOG 20190630-20:39:12] epoch: 247 train-loss: 0.0013406846737780143\n",
      "[LOG 20190630-20:39:13] epoch: 248 train-loss: 0.0013327366068551783\n",
      "[LOG 20190630-20:39:13] epoch: 249 train-loss: 0.0013245486188679934\n",
      "[LOG 20190630-20:39:14] epoch: 250 train-loss: 0.0013161282186047174\n",
      "[LOG 20190630-20:39:15] epoch: 251 train-loss: 0.0013074837806925643\n",
      "[LOG 20190630-20:39:15] epoch: 252 train-loss: 0.0012986212132091168\n",
      "[LOG 20190630-20:39:16] epoch: 253 train-loss: 0.0012895500112790614\n",
      "[LOG 20190630-20:39:16] epoch: 254 train-loss: 0.0012802751807612367\n",
      "[LOG 20190630-20:39:17] epoch: 255 train-loss: 0.0012708066933555529\n",
      "[LOG 20190630-20:39:17] epoch: 256 train-loss: 0.0012611518286576029\n",
      "[LOG 20190630-20:39:18] epoch: 257 train-loss: 0.0012513173969637137\n",
      "[LOG 20190630-20:39:18] epoch: 258 train-loss: 0.0012413123367878143\n",
      "[LOG 20190630-20:39:18] epoch: 259 train-loss: 0.0012311432728893124\n",
      "[LOG 20190630-20:39:19] epoch: 260 train-loss: 0.0012208184671180788\n",
      "[LOG 20190630-20:39:19] epoch: 261 train-loss: 0.0012103449353162432\n",
      "[LOG 20190630-20:39:20] epoch: 262 train-loss: 0.0011997302444797242\n",
      "[LOG 20190630-20:39:20] epoch: 263 train-loss: 0.0011889830384461675\n",
      "[LOG 20190630-20:39:21] epoch: 264 train-loss: 0.0011781100583903026\n",
      "[LOG 20190630-20:39:21] epoch: 265 train-loss: 0.0011671196043607779\n",
      "[LOG 20190630-20:39:22] epoch: 266 train-loss: 0.0011560184684640262\n",
      "[LOG 20190630-20:39:22] epoch: 267 train-loss: 0.001144815207226202\n",
      "[LOG 20190630-20:39:23] epoch: 268 train-loss: 0.001133516596382833\n",
      "[LOG 20190630-20:39:23] epoch: 269 train-loss: 0.0011221305376238888\n",
      "[LOG 20190630-20:39:24] epoch: 270 train-loss: 0.0011106637357443105\n",
      "[LOG 20190630-20:39:24] epoch: 271 train-loss: 0.00109912387051736\n",
      "[LOG 20190630-20:39:25] epoch: 272 train-loss: 0.0010875189473154023\n",
      "[LOG 20190630-20:39:25] epoch: 273 train-loss: 0.0010758563039416913\n",
      "[LOG 20190630-20:39:25] epoch: 274 train-loss: 0.0010641449007380288\n",
      "[LOG 20190630-20:39:26] epoch: 275 train-loss: 0.0010523916953388834\n",
      "[LOG 20190630-20:39:26] epoch: 276 train-loss: 0.0010406037890788866\n",
      "[LOG 20190630-20:39:27] epoch: 277 train-loss: 0.001028788825351512\n",
      "[LOG 20190630-20:39:27] epoch: 278 train-loss: 0.0010169552824663697\n",
      "[LOG 20190630-20:39:28] epoch: 279 train-loss: 0.0010051100834971294\n",
      "[LOG 20190630-20:39:28] epoch: 280 train-loss: 0.0009932614502758952\n",
      "[LOG 20190630-20:39:29] epoch: 281 train-loss: 0.0009814175446081208\n",
      "[LOG 20190630-20:39:29] epoch: 282 train-loss: 0.0009695846783870365\n",
      "[LOG 20190630-20:39:30] epoch: 283 train-loss: 0.0009577710097801173\n",
      "[LOG 20190630-20:39:30] epoch: 284 train-loss: 0.0009459845277888235\n",
      "[LOG 20190630-20:39:31] epoch: 285 train-loss: 0.0009342322446173057\n",
      "[LOG 20190630-20:39:31] epoch: 286 train-loss: 0.000922521467145998\n",
      "[LOG 20190630-20:39:31] epoch: 287 train-loss: 0.000910859436771716\n",
      "[LOG 20190630-20:39:32] epoch: 288 train-loss: 0.0008992533330456354\n",
      "[LOG 20190630-20:39:32] epoch: 289 train-loss: 0.0008877097152435454\n",
      "[LOG 20190630-20:39:33] epoch: 290 train-loss: 0.0008762350171309663\n",
      "[LOG 20190630-20:39:33] epoch: 291 train-loss: 0.0008648361199448118\n",
      "[LOG 20190630-20:39:34] epoch: 292 train-loss: 0.000853519537486136\n",
      "[LOG 20190630-20:39:34] epoch: 293 train-loss: 0.0008422908249485772\n",
      "[LOG 20190630-20:39:35] epoch: 294 train-loss: 0.0008311557503475342\n",
      "[LOG 20190630-20:39:35] epoch: 295 train-loss: 0.0008201199852919672\n",
      "[LOG 20190630-20:39:36] epoch: 296 train-loss: 0.0008091888594208285\n",
      "[LOG 20190630-20:39:36] epoch: 297 train-loss: 0.0007983676077856217\n",
      "[LOG 20190630-20:39:37] epoch: 298 train-loss: 0.0007876606468926184\n",
      "[LOG 20190630-20:39:37] epoch: 299 train-loss: 0.0007770729262119858\n",
      "[LOG 20190630-20:39:38] epoch: 300 train-loss: 0.0007666077290195972\n",
      "[LOG 20190630-20:39:38] epoch: 301 train-loss: 0.0007562693899672013\n",
      "[LOG 20190630-20:39:38] epoch: 302 train-loss: 0.0007460618016921217\n",
      "[LOG 20190630-20:39:39] epoch: 303 train-loss: 0.0007359889168583322\n",
      "[LOG 20190630-20:39:39] epoch: 304 train-loss: 0.0007260526381287491\n",
      "[LOG 20190630-20:39:40] epoch: 305 train-loss: 0.0007162565871112747\n",
      "[LOG 20190630-20:39:40] epoch: 306 train-loss: 0.0007066034231684171\n",
      "[LOG 20190630-20:39:41] epoch: 307 train-loss: 0.0006970958811507444\n",
      "[LOG 20190630-20:39:41] epoch: 308 train-loss: 0.0006877353480376769\n",
      "[LOG 20190630-20:39:42] epoch: 309 train-loss: 0.0006785244886486907\n",
      "[LOG 20190630-20:39:42] epoch: 310 train-loss: 0.000669465324790508\n",
      "[LOG 20190630-20:39:43] epoch: 311 train-loss: 0.000660558953313739\n",
      "[LOG 20190630-20:39:43] epoch: 312 train-loss: 0.000651807346912392\n",
      "[LOG 20190630-20:39:44] epoch: 313 train-loss: 0.000643211236820207\n",
      "[LOG 20190630-20:39:44] epoch: 314 train-loss: 0.0006347705812004278\n",
      "[LOG 20190630-20:39:44] epoch: 315 train-loss: 0.0006264877183639328\n",
      "[LOG 20190630-20:39:45] epoch: 316 train-loss: 0.0006183641407915275\n",
      "[LOG 20190630-20:39:45] epoch: 317 train-loss: 0.0006103991645431961\n",
      "[LOG 20190630-20:39:46] epoch: 318 train-loss: 0.0006025934089848306\n",
      "[LOG 20190630-20:39:46] epoch: 319 train-loss: 0.0005949474925728282\n",
      "[LOG 20190630-20:39:47] epoch: 320 train-loss: 0.0005874633443454513\n",
      "[LOG 20190630-20:39:47] epoch: 320 new best train-loss: 0.0005874633443454513 found\n",
      "[LOG 20190630-20:39:47] epoch: 321 train-loss: 0.0005801395791422692\n",
      "[LOG 20190630-20:39:48] epoch: 322 train-loss: 0.0005729765962314559\n",
      "[LOG 20190630-20:39:48] epoch: 323 train-loss: 0.0005659735470544547\n",
      "[LOG 20190630-20:39:49] epoch: 324 train-loss: 0.0005591320486928453\n",
      "[LOG 20190630-20:39:49] epoch: 325 train-loss: 0.0005524517200683476\n",
      "[LOG 20190630-20:39:50] epoch: 326 train-loss: 0.000545931185115478\n",
      "[LOG 20190630-20:39:50] epoch: 327 train-loss: 0.0005395712178142276\n",
      "[LOG 20190630-20:39:50] epoch: 328 train-loss: 0.000533372130121279\n",
      "[LOG 20190630-20:39:51] epoch: 329 train-loss: 0.0005273332271826803\n",
      "[LOG 20190630-20:39:51] epoch: 330 train-loss: 0.0005214537122810725\n",
      "[LOG 20190630-20:39:51] epoch: 330 new best train-loss: 0.0005214537122810725 found\n",
      "[LOG 20190630-20:39:52] epoch: 331 train-loss: 0.0005157332871021936\n",
      "[LOG 20190630-20:39:52] epoch: 332 train-loss: 0.0005101719261801918\n",
      "[LOG 20190630-20:39:53] epoch: 333 train-loss: 0.0005047685281169834\n",
      "[LOG 20190630-20:39:53] epoch: 334 train-loss: 0.0004995224817321287\n",
      "[LOG 20190630-20:39:54] epoch: 335 train-loss: 0.0004944338497807621\n",
      "[LOG 20190630-20:39:54] epoch: 336 train-loss: 0.0004895017264061607\n",
      "[LOG 20190630-20:39:55] epoch: 337 train-loss: 0.000484725707792677\n",
      "[LOG 20190630-20:39:55] epoch: 338 train-loss: 0.00048010514183260966\n",
      "[LOG 20190630-20:39:56] epoch: 339 train-loss: 0.0004756387688757968\n",
      "[LOG 20190630-20:39:56] epoch: 340 train-loss: 0.0004713262469522306\n",
      "[LOG 20190630-20:39:56] epoch: 340 new best train-loss: 0.0004713262469522306 found\n",
      "[LOG 20190630-20:39:56] epoch: 341 train-loss: 0.0004671662791224662\n",
      "[LOG 20190630-20:39:57] epoch: 342 train-loss: 0.00046315855161083164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:39:57] epoch: 343 train-loss: 0.00045930196029075887\n",
      "[LOG 20190630-20:39:58] epoch: 344 train-loss: 0.00045559525324279093\n",
      "[LOG 20190630-20:39:58] epoch: 345 train-loss: 0.0004520377160588396\n",
      "[LOG 20190630-20:39:59] epoch: 346 train-loss: 0.0004486282145990117\n",
      "[LOG 20190630-20:39:59] epoch: 347 train-loss: 0.0004453654960343556\n",
      "[LOG 20190630-20:40:00] epoch: 348 train-loss: 0.00044224810790183255\n",
      "[LOG 20190630-20:40:00] epoch: 349 train-loss: 0.0004392754181026248\n",
      "[LOG 20190630-20:40:01] epoch: 350 train-loss: 0.00043644614379445557\n",
      "[LOG 20190630-20:40:01] epoch: 350 new best train-loss: 0.00043644614379445557 found\n",
      "[LOG 20190630-20:40:01] epoch: 351 train-loss: 0.0004337583882261242\n",
      "[LOG 20190630-20:40:02] epoch: 352 train-loss: 0.0004312110818318615\n",
      "[LOG 20190630-20:40:02] epoch: 353 train-loss: 0.0004288029535928217\n",
      "[LOG 20190630-20:40:03] epoch: 354 train-loss: 0.00042653233413147973\n",
      "[LOG 20190630-20:40:03] epoch: 355 train-loss: 0.0004243977618898498\n",
      "[LOG 20190630-20:40:03] epoch: 356 train-loss: 0.0004223974528940744\n",
      "[LOG 20190630-20:40:04] epoch: 357 train-loss: 0.0004205301006550144\n",
      "[LOG 20190630-20:40:04] epoch: 358 train-loss: 0.00041879380069076433\n",
      "[LOG 20190630-20:40:05] epoch: 359 train-loss: 0.00041718675993251964\n",
      "[LOG 20190630-20:40:05] epoch: 360 train-loss: 0.00041570733628759626\n",
      "[LOG 20190630-20:40:05] epoch: 360 new best train-loss: 0.00041570733628759626 found\n",
      "[LOG 20190630-20:40:06] epoch: 361 train-loss: 0.00041435356388319633\n",
      "[LOG 20190630-20:40:06] epoch: 362 train-loss: 0.00041312357052447624\n",
      "[LOG 20190630-20:40:07] epoch: 363 train-loss: 0.00041201539170288015\n",
      "[LOG 20190630-20:40:07] epoch: 364 train-loss: 0.00041102706563833635\n",
      "[LOG 20190630-20:40:08] epoch: 365 train-loss: 0.0004101565045857569\n",
      "[LOG 20190630-20:40:08] epoch: 366 train-loss: 0.00040940166218206286\n",
      "[LOG 20190630-20:40:09] epoch: 367 train-loss: 0.00040876036928239046\n",
      "[LOG 20190630-20:40:09] epoch: 368 train-loss: 0.00040823039762472035\n",
      "[LOG 20190630-20:40:09] epoch: 369 train-loss: 0.0004078095726072206\n",
      "[LOG 20190630-20:40:10] epoch: 370 train-loss: 0.0004074955013493309\n",
      "[LOG 20190630-20:40:10] epoch: 370 new best train-loss: 0.0004074955013493309 found\n",
      "[LOG 20190630-20:40:10] epoch: 371 train-loss: 0.000407286046538502\n",
      "[LOG 20190630-20:40:11] epoch: 372 train-loss: 0.00040717878391660633\n",
      "[LOG 20190630-20:40:11] epoch: 373 train-loss: 0.0004071714420206263\n",
      "[LOG 20190630-20:40:12] epoch: 374 train-loss: 0.00040726138058744255\n",
      "[LOG 20190630-20:40:12] epoch: 375 train-loss: 0.0004074462490279984\n",
      "[LOG 20190630-20:40:13] epoch: 376 train-loss: 0.000407723614898714\n",
      "[LOG 20190630-20:40:13] epoch: 377 train-loss: 0.0004080908311152598\n",
      "[LOG 20190630-20:40:14] epoch: 378 train-loss: 0.0004085452687832003\n",
      "[LOG 20190630-20:40:14] epoch: 379 train-loss: 0.0004090847542101983\n",
      "[LOG 20190630-20:40:15] epoch: 380 train-loss: 0.0004097064661436889\n",
      "[LOG 20190630-20:40:15] epoch: 381 train-loss: 0.0004104079525859561\n",
      "[LOG 20190630-20:40:16] epoch: 382 train-loss: 0.00041118645322058\n",
      "[LOG 20190630-20:40:16] epoch: 383 train-loss: 0.00041203952559953905\n",
      "[LOG 20190630-20:40:16] epoch: 384 train-loss: 0.00041296401559520746\n",
      "[LOG 20190630-20:40:17] epoch: 385 train-loss: 0.00041395800826649065\n",
      "[LOG 20190630-20:40:17] epoch: 386 train-loss: 0.0004150183967794874\n",
      "[LOG 20190630-20:40:18] epoch: 387 train-loss: 0.0004161427887083846\n",
      "[LOG 20190630-20:40:18] epoch: 388 train-loss: 0.00041732829913598835\n",
      "[LOG 20190630-20:40:19] epoch: 389 train-loss: 0.00041857265341604943\n",
      "[LOG 20190630-20:40:19] epoch: 390 train-loss: 0.00041987301210610894\n",
      "[LOG 20190630-20:40:20] epoch: 391 train-loss: 0.0004212269204799668\n",
      "[LOG 20190630-20:40:20] epoch: 392 train-loss: 0.00042263121667929227\n",
      "[LOG 20190630-20:40:21] epoch: 393 train-loss: 0.00042408391027493053\n",
      "[LOG 20190630-20:40:21] epoch: 394 train-loss: 0.00042558229779388057\n",
      "[LOG 20190630-20:40:22] epoch: 395 train-loss: 0.0004271239986337605\n",
      "[LOG 20190630-20:40:22] epoch: 396 train-loss: 0.0004287060896785988\n",
      "[LOG 20190630-20:40:22] epoch: 397 train-loss: 0.00043032604435211397\n",
      "[LOG 20190630-20:40:23] epoch: 398 train-loss: 0.0004319817949181015\n",
      "[LOG 20190630-20:40:23] epoch: 399 train-loss: 0.0004336705610512581\n",
      "[LOG 20190630-20:40:24] epoch: 400 train-loss: 0.0004353897752480407\n",
      "[LOG 20190630-20:40:24] epoch: 401 train-loss: 0.0004371368790998531\n",
      "[LOG 20190630-20:40:25] epoch: 402 train-loss: 0.0004389098858155194\n",
      "[LOG 20190630-20:40:26] epoch: 403 train-loss: 0.00044070631884096656\n",
      "[LOG 20190630-20:40:26] epoch: 404 train-loss: 0.00044252391899135546\n",
      "[LOG 20190630-20:40:27] epoch: 405 train-loss: 0.00044435989866542513\n",
      "[LOG 20190630-20:40:27] epoch: 406 train-loss: 0.00044621248889598064\n",
      "[LOG 20190630-20:40:28] epoch: 407 train-loss: 0.00044807905032939743\n",
      "[LOG 20190630-20:40:29] epoch: 408 train-loss: 0.0004499579385992547\n",
      "[LOG 20190630-20:40:29] epoch: 409 train-loss: 0.0004518467449088348\n",
      "[LOG 20190630-20:40:30] epoch: 410 train-loss: 0.00045374269620879204\n",
      "[LOG 20190630-20:40:30] epoch: 411 train-loss: 0.00045564378388007754\n",
      "[LOG 20190630-20:40:31] epoch: 412 train-loss: 0.00045754832035527215\n",
      "[LOG 20190630-20:40:31] epoch: 413 train-loss: 0.0004594543943312601\n",
      "[LOG 20190630-20:40:32] epoch: 414 train-loss: 0.0004613600449374644\n",
      "[LOG 20190630-20:40:32] epoch: 415 train-loss: 0.000463262404082343\n",
      "[LOG 20190630-20:40:33] epoch: 416 train-loss: 0.0004651602230296703\n",
      "[LOG 20190630-20:40:34] epoch: 417 train-loss: 0.0004670520356739871\n",
      "[LOG 20190630-20:40:35] epoch: 418 train-loss: 0.0004689359616349975\n",
      "[LOG 20190630-20:40:36] epoch: 419 train-loss: 0.000470809968192043\n",
      "[LOG 20190630-20:40:36] epoch: 420 train-loss: 0.0004726724982901942\n",
      "[LOG 20190630-20:40:37] epoch: 421 train-loss: 0.00047452227045141626\n",
      "[LOG 20190630-20:40:38] epoch: 422 train-loss: 0.00047635750934205134\n",
      "[LOG 20190630-20:40:38] epoch: 423 train-loss: 0.00047817649647186045\n",
      "[LOG 20190630-20:40:39] epoch: 424 train-loss: 0.00047997733463489567\n",
      "[LOG 20190630-20:40:39] epoch: 425 train-loss: 0.0004817595527129015\n",
      "[LOG 20190630-20:40:40] epoch: 426 train-loss: 0.00048352103021898074\n",
      "[LOG 20190630-20:40:40] epoch: 427 train-loss: 0.00048526098726142664\n",
      "[LOG 20190630-20:40:41] epoch: 428 train-loss: 0.00048697782222006936\n",
      "[LOG 20190630-20:40:41] epoch: 429 train-loss: 0.000488670226332033\n",
      "[LOG 20190630-20:40:41] epoch: 430 train-loss: 0.0004903362523691612\n",
      "[LOG 20190630-20:40:42] epoch: 431 train-loss: 0.0004919748162137694\n",
      "[LOG 20190630-20:40:42] epoch: 432 train-loss: 0.0004935852402923047\n",
      "[LOG 20190630-20:40:43] epoch: 433 train-loss: 0.0004951649616486975\n",
      "[LOG 20190630-20:40:43] epoch: 434 train-loss: 0.0004967136801496963\n",
      "[LOG 20190630-20:40:44] epoch: 435 train-loss: 0.0004982303362339735\n",
      "[LOG 20190630-20:40:44] epoch: 436 train-loss: 0.0004997114856450935\n",
      "[LOG 20190630-20:40:45] epoch: 437 train-loss: 0.0005011566145185498\n",
      "[LOG 20190630-20:40:45] epoch: 438 train-loss: 0.0005025652963013272\n",
      "[LOG 20190630-20:40:46] epoch: 439 train-loss: 0.0005039364978074445\n",
      "[LOG 20190630-20:40:46] epoch: 440 train-loss: 0.0005052674696344184\n",
      "[LOG 20190630-20:40:47] epoch: 441 train-loss: 0.0005065574978289078\n",
      "[LOG 20190630-20:40:47] epoch: 442 train-loss: 0.0005078055901321932\n",
      "[LOG 20190630-20:40:48] epoch: 443 train-loss: 0.0005090106815259787\n",
      "[LOG 20190630-20:40:48] epoch: 444 train-loss: 0.0005101705182823935\n",
      "[LOG 20190630-20:40:49] epoch: 445 train-loss: 0.0005112856797495624\n",
      "[LOG 20190630-20:40:49] epoch: 446 train-loss: 0.0005123571463627741\n",
      "[LOG 20190630-20:40:50] epoch: 447 train-loss: 0.0005133832701176289\n",
      "[LOG 20190630-20:40:50] epoch: 448 train-loss: 0.0005143624230186106\n",
      "[LOG 20190630-20:40:51] epoch: 449 train-loss: 0.0005152937255843426\n",
      "[LOG 20190630-20:40:51] epoch: 450 train-loss: 0.0005161794042578549\n",
      "[LOG 20190630-20:40:52] epoch: 451 train-loss: 0.0005170183694644948\n",
      "[LOG 20190630-20:40:52] epoch: 452 train-loss: 0.0005178096071176697\n",
      "[LOG 20190630-20:40:53] epoch: 453 train-loss: 0.0005185550535315997\n",
      "[LOG 20190630-20:40:53] epoch: 454 train-loss: 0.0005192548451304901\n",
      "[LOG 20190630-20:40:54] epoch: 455 train-loss: 0.0005199079596422962\n",
      "[LOG 20190630-20:40:54] epoch: 456 train-loss: 0.0005205179568292806\n",
      "[LOG 20190630-20:40:55] epoch: 457 train-loss: 0.000521083456078486\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:40:56] epoch: 458 train-loss: 0.0005216054423726746\n",
      "[LOG 20190630-20:40:56] epoch: 459 train-loss: 0.0005220858092798153\n",
      "[LOG 20190630-20:40:57] epoch: 460 train-loss: 0.0005225238364801044\n",
      "[LOG 20190630-20:40:57] epoch: 461 train-loss: 0.0005229231182966032\n",
      "[LOG 20190630-20:40:58] epoch: 462 train-loss: 0.000523281690220756\n",
      "[LOG 20190630-20:40:58] epoch: 463 train-loss: 0.0005236025026533753\n",
      "[LOG 20190630-20:40:59] epoch: 464 train-loss: 0.0005238856901996769\n",
      "[LOG 20190630-20:40:59] epoch: 465 train-loss: 0.0005241328926786082\n",
      "[LOG 20190630-20:41:00] epoch: 466 train-loss: 0.0005243442801656784\n",
      "[LOG 20190630-20:41:01] epoch: 467 train-loss: 0.0005245212887530215\n",
      "[LOG 20190630-20:41:01] epoch: 468 train-loss: 0.0005246644059297978\n",
      "[LOG 20190630-20:41:01] epoch: 469 train-loss: 0.0005247756425887928\n",
      "[LOG 20190630-20:41:02] epoch: 470 train-loss: 0.0005248550669421093\n",
      "[LOG 20190630-20:41:02] epoch: 471 train-loss: 0.000524904307894758\n",
      "[LOG 20190630-20:41:03] epoch: 472 train-loss: 0.0005249224195722491\n",
      "[LOG 20190630-20:41:03] epoch: 473 train-loss: 0.000524912011314882\n",
      "[LOG 20190630-20:41:04] epoch: 474 train-loss: 0.000524872902133211\n",
      "[LOG 20190630-20:41:04] epoch: 475 train-loss: 0.0005248064790066564\n",
      "[LOG 20190630-20:41:05] epoch: 476 train-loss: 0.000524712184414966\n",
      "[LOG 20190630-20:41:05] epoch: 477 train-loss: 0.0005245921847745194\n",
      "[LOG 20190630-20:41:06] epoch: 478 train-loss: 0.0005244456760920002\n",
      "[LOG 20190630-20:41:06] epoch: 479 train-loss: 0.000524273690643895\n",
      "[LOG 20190630-20:41:07] epoch: 480 train-loss: 0.0005240752579993568\n",
      "[LOG 20190630-20:41:07] epoch: 481 train-loss: 0.0005238528947302257\n",
      "[LOG 20190630-20:41:08] epoch: 482 train-loss: 0.0005236059932940407\n",
      "[LOG 20190630-20:41:08] epoch: 483 train-loss: 0.0005233353331277613\n",
      "[LOG 20190630-20:41:09] epoch: 484 train-loss: 0.0005230396136539639\n",
      "[LOG 20190630-20:41:09] epoch: 485 train-loss: 0.0005227203519098111\n",
      "[LOG 20190630-20:41:10] epoch: 486 train-loss: 0.0005223770058364607\n",
      "[LOG 20190630-20:41:10] epoch: 487 train-loss: 0.0005220099519647192\n",
      "[LOG 20190630-20:41:10] epoch: 488 train-loss: 0.0005216182953518\n",
      "[LOG 20190630-20:41:11] epoch: 489 train-loss: 0.0005212031173869036\n",
      "[LOG 20190630-20:41:11] epoch: 490 train-loss: 0.0005207638369029155\n",
      "[LOG 20190630-20:41:12] epoch: 491 train-loss: 0.000520300392054196\n",
      "[LOG 20190630-20:41:12] epoch: 492 train-loss: 0.0005198123790250975\n",
      "[LOG 20190630-20:41:13] epoch: 493 train-loss: 0.000519299989719002\n",
      "[LOG 20190630-20:41:13] epoch: 494 train-loss: 0.0005187626602491946\n",
      "[LOG 20190630-20:41:14] epoch: 495 train-loss: 0.0005181998467378435\n",
      "[LOG 20190630-20:41:14] epoch: 496 train-loss: 0.0005176114464120474\n",
      "[LOG 20190630-20:41:15] epoch: 497 train-loss: 0.0005169976257093367\n",
      "[LOG 20190630-20:41:15] epoch: 498 train-loss: 0.0005163579326108447\n",
      "[LOG 20190630-20:41:16] epoch: 499 train-loss: 0.0005156924698894727\n",
      "[LOG 20190630-20:41:16] epoch: 500 train-loss: 0.0005150003125891089\n",
      "[LOG 20190630-20:41:17] epoch: 501 train-loss: 0.0005142817908563302\n",
      "[LOG 20190630-20:41:17] epoch: 502 train-loss: 0.0005135360852364101\n",
      "[LOG 20190630-20:41:18] epoch: 503 train-loss: 0.0005127636732140672\n",
      "[LOG 20190630-20:41:18] epoch: 504 train-loss: 0.0005119627439853502\n",
      "[LOG 20190630-20:41:19] epoch: 505 train-loss: 0.0005111344853503397\n",
      "[LOG 20190630-20:41:19] epoch: 506 train-loss: 0.0005102773311591591\n",
      "[LOG 20190630-20:41:20] epoch: 507 train-loss: 0.000509392176354595\n",
      "[LOG 20190630-20:41:20] epoch: 508 train-loss: 0.0005084774775241385\n",
      "[LOG 20190630-20:41:20] epoch: 509 train-loss: 0.000507534496136941\n",
      "[LOG 20190630-20:41:21] epoch: 510 train-loss: 0.0005065614805062069\n",
      "[LOG 20190630-20:41:22] epoch: 511 train-loss: 0.0005055602359789191\n",
      "[LOG 20190630-20:41:22] epoch: 512 train-loss: 0.0005045279549449333\n",
      "[LOG 20190630-20:41:23] epoch: 513 train-loss: 0.000503466291775112\n",
      "[LOG 20190630-20:41:23] epoch: 514 train-loss: 0.0005023739995522192\n",
      "[LOG 20190630-20:41:24] epoch: 515 train-loss: 0.0005012507799619925\n",
      "[LOG 20190630-20:41:24] epoch: 516 train-loss: 0.0005000977507734206\n",
      "[LOG 20190630-20:41:25] epoch: 517 train-loss: 0.0004989131666661706\n",
      "[LOG 20190630-20:41:25] epoch: 518 train-loss: 0.0004976987474947236\n",
      "[LOG 20190630-20:41:26] epoch: 519 train-loss: 0.000496453040796041\n",
      "[LOG 20190630-20:41:27] epoch: 520 train-loss: 0.0004951778983013355\n",
      "[LOG 20190630-20:41:28] epoch: 521 train-loss: 0.0004938708179906826\n",
      "[LOG 20190630-20:41:28] epoch: 522 train-loss: 0.0004925338271277724\n",
      "[LOG 20190630-20:41:29] epoch: 523 train-loss: 0.0004911663172606495\n",
      "[LOG 20190630-20:41:30] epoch: 524 train-loss: 0.0004897683038507239\n",
      "[LOG 20190630-20:41:31] epoch: 525 train-loss: 0.0004883409083049628\n",
      "[LOG 20190630-20:41:31] epoch: 526 train-loss: 0.0004868830983468797\n",
      "[LOG 20190630-20:41:32] epoch: 527 train-loss: 0.000485396955809847\n",
      "[LOG 20190630-20:41:33] epoch: 528 train-loss: 0.00048388089180662064\n",
      "[LOG 20190630-20:41:33] epoch: 529 train-loss: 0.00048233682218778995\n",
      "[LOG 20190630-20:41:34] epoch: 530 train-loss: 0.0004807645045730169\n",
      "[LOG 20190630-20:41:34] epoch: 531 train-loss: 0.00047916439552864176\n",
      "[LOG 20190630-20:41:35] epoch: 532 train-loss: 0.0004775381980834936\n",
      "[LOG 20190630-20:41:35] epoch: 533 train-loss: 0.0004758838695124723\n",
      "[LOG 20190630-20:41:36] epoch: 534 train-loss: 0.00047420448709090124\n",
      "[LOG 20190630-20:41:37] epoch: 535 train-loss: 0.00047249922772607533\n",
      "[LOG 20190630-20:41:37] epoch: 536 train-loss: 0.00047076943792490056\n",
      "[LOG 20190630-20:41:38] epoch: 537 train-loss: 0.00046901664973120205\n",
      "[LOG 20190630-20:41:38] epoch: 538 train-loss: 0.0004672392792599567\n",
      "[LOG 20190630-20:41:39] epoch: 539 train-loss: 0.0004654403260246909\n",
      "[LOG 20190630-20:41:39] epoch: 540 train-loss: 0.00046361893555513234\n",
      "[LOG 20190630-20:41:39] epoch: 541 train-loss: 0.00046177648937373306\n",
      "[LOG 20190630-20:41:40] epoch: 542 train-loss: 0.00045991437673365\n",
      "[LOG 20190630-20:41:40] epoch: 543 train-loss: 0.0004580318072839873\n",
      "[LOG 20190630-20:41:41] epoch: 544 train-loss: 0.0004561315945466049\n",
      "[LOG 20190630-20:41:41] epoch: 545 train-loss: 0.00045421275490298285\n",
      "[LOG 20190630-20:41:42] epoch: 546 train-loss: 0.0004522769017967221\n",
      "[LOG 20190630-20:41:42] epoch: 547 train-loss: 0.00045032556727164774\n",
      "[LOG 20190630-20:41:43] epoch: 548 train-loss: 0.0004483576226448349\n",
      "[LOG 20190630-20:41:43] epoch: 549 train-loss: 0.00044637629434873816\n",
      "[LOG 20190630-20:41:44] epoch: 550 train-loss: 0.0004443811967576039\n",
      "[LOG 20190630-20:41:44] epoch: 551 train-loss: 0.00044237243992029107\n",
      "[LOG 20190630-20:41:45] epoch: 552 train-loss: 0.0004403529010232887\n",
      "[LOG 20190630-20:41:45] epoch: 553 train-loss: 0.0004383211748972826\n",
      "[LOG 20190630-20:41:46] epoch: 554 train-loss: 0.000436280040503334\n",
      "[LOG 20190630-20:41:46] epoch: 555 train-loss: 0.000434229715665424\n",
      "[LOG 20190630-20:41:47] epoch: 556 train-loss: 0.00043216996346018277\n",
      "[LOG 20190630-20:41:47] epoch: 557 train-loss: 0.000430104186762037\n",
      "[LOG 20190630-20:41:48] epoch: 558 train-loss: 0.00042803134965652134\n",
      "[LOG 20190630-20:41:48] epoch: 559 train-loss: 0.000425952855948708\n",
      "[LOG 20190630-20:41:49] epoch: 560 train-loss: 0.0004238705892021244\n",
      "[LOG 20190630-20:41:49] epoch: 561 train-loss: 0.0004217839464217832\n",
      "[LOG 20190630-20:41:50] epoch: 562 train-loss: 0.0004196948480057472\n",
      "[LOG 20190630-20:41:50] epoch: 563 train-loss: 0.0004176039728918113\n",
      "[LOG 20190630-20:41:51] epoch: 564 train-loss: 0.0004155113160777546\n",
      "[LOG 20190630-20:41:51] epoch: 565 train-loss: 0.00041341980067954864\n",
      "[LOG 20190630-20:41:52] epoch: 566 train-loss: 0.0004113288914595614\n",
      "[LOG 20190630-20:41:52] epoch: 567 train-loss: 0.0004092392850907345\n",
      "[LOG 20190630-20:41:53] epoch: 568 train-loss: 0.0004071531848239829\n",
      "[LOG 20190630-20:41:53] epoch: 569 train-loss: 0.00040506999221179285\n",
      "[LOG 20190630-20:41:54] epoch: 570 train-loss: 0.0004029910987810581\n",
      "[LOG 20190630-20:41:54] epoch: 570 new best train-loss: 0.0004029910987810581 found\n",
      "[LOG 20190630-20:41:54] epoch: 571 train-loss: 0.00040091769869832206\n",
      "[LOG 20190630-20:41:55] epoch: 572 train-loss: 0.00039885028718344984\n",
      "[LOG 20190630-20:41:55] epoch: 573 train-loss: 0.00039679035853623645\n",
      "[LOG 20190630-20:41:56] epoch: 574 train-loss: 0.0003947375830648525\n",
      "[LOG 20190630-20:41:57] epoch: 575 train-loss: 0.0003926929666704382\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:41:57] epoch: 576 train-loss: 0.00039065921100700507\n",
      "[LOG 20190630-20:41:58] epoch: 577 train-loss: 0.0003886346066792612\n",
      "[LOG 20190630-20:41:58] epoch: 578 train-loss: 0.0003866209840452939\n",
      "[LOG 20190630-20:41:59] epoch: 579 train-loss: 0.00038461981057480443\n",
      "[LOG 20190630-20:41:59] epoch: 580 train-loss: 0.00038263004762484343\n",
      "[LOG 20190630-20:41:59] epoch: 580 new best train-loss: 0.00038263004762484343 found\n",
      "[LOG 20190630-20:42:00] epoch: 581 train-loss: 0.00038065379794716137\n",
      "[LOG 20190630-20:42:01] epoch: 582 train-loss: 0.0003786921147366229\n",
      "[LOG 20190630-20:42:01] epoch: 583 train-loss: 0.0003767439620787627\n",
      "[LOG 20190630-20:42:02] epoch: 584 train-loss: 0.0003748108188119659\n",
      "[LOG 20190630-20:42:02] epoch: 585 train-loss: 0.0003728941487679549\n",
      "[LOG 20190630-20:42:03] epoch: 586 train-loss: 0.0003709926313604228\n",
      "[LOG 20190630-20:42:03] epoch: 587 train-loss: 0.0003691079027703381\n",
      "[LOG 20190630-20:42:04] epoch: 588 train-loss: 0.00036724115943798097\n",
      "[LOG 20190630-20:42:04] epoch: 589 train-loss: 0.0003653917242445459\n",
      "[LOG 20190630-20:42:05] epoch: 590 train-loss: 0.00036356096461531706\n",
      "[LOG 20190630-20:42:05] epoch: 590 new best train-loss: 0.00036356096461531706 found\n",
      "[LOG 20190630-20:42:05] epoch: 591 train-loss: 0.00036174919523546123\n",
      "[LOG 20190630-20:42:06] epoch: 592 train-loss: 0.000359956362444791\n",
      "[LOG 20190630-20:42:06] epoch: 593 train-loss: 0.0003581839728212799\n",
      "[LOG 20190630-20:42:07] epoch: 594 train-loss: 0.0003564316798474465\n",
      "[LOG 20190630-20:42:07] epoch: 595 train-loss: 0.0003546990355971502\n",
      "[LOG 20190630-20:42:08] epoch: 596 train-loss: 0.00035298798684380017\n",
      "[LOG 20190630-20:42:08] epoch: 597 train-loss: 0.00035129871730532614\n",
      "[LOG 20190630-20:42:09] epoch: 598 train-loss: 0.0003496301715131267\n",
      "[LOG 20190630-20:42:09] epoch: 599 train-loss: 0.00034798407841662993\n",
      "[LOG 20190630-20:42:10] epoch: 600 train-loss: 0.0003463606944933417\n",
      "[LOG 20190630-20:42:10] epoch: 600 new best train-loss: 0.0003463606944933417 found\n",
      "[LOG 20190630-20:42:10] epoch: 601 train-loss: 0.0003447592594056914\n",
      "[LOG 20190630-20:42:11] epoch: 602 train-loss: 0.00034318039479330764\n",
      "[LOG 20190630-20:42:11] epoch: 603 train-loss: 0.00034162528731940256\n",
      "[LOG 20190630-20:42:12] epoch: 604 train-loss: 0.0003400928421797289\n",
      "[LOG 20190630-20:42:12] epoch: 605 train-loss: 0.0003385838722351764\n",
      "[LOG 20190630-20:42:13] epoch: 606 train-loss: 0.00033709866511344444\n",
      "[LOG 20190630-20:42:13] epoch: 607 train-loss: 0.0003356370993969904\n",
      "[LOG 20190630-20:42:14] epoch: 608 train-loss: 0.00033419964415770664\n",
      "[LOG 20190630-20:42:14] epoch: 609 train-loss: 0.0003327871090732515\n",
      "[LOG 20190630-20:42:15] epoch: 610 train-loss: 0.00033139820675387455\n",
      "[LOG 20190630-20:42:15] epoch: 610 new best train-loss: 0.00033139820675387455 found\n",
      "[LOG 20190630-20:42:15] epoch: 611 train-loss: 0.000330033053387524\n",
      "[LOG 20190630-20:42:16] epoch: 612 train-loss: 0.0003286934525021934\n",
      "[LOG 20190630-20:42:16] epoch: 613 train-loss: 0.00032737819606154517\n",
      "[LOG 20190630-20:42:17] epoch: 614 train-loss: 0.00032608685955892724\n",
      "[LOG 20190630-20:42:17] epoch: 615 train-loss: 0.0003248206303396728\n",
      "[LOG 20190630-20:42:18] epoch: 616 train-loss: 0.0003235793853946234\n",
      "[LOG 20190630-20:42:18] epoch: 617 train-loss: 0.00032236249398920336\n",
      "[LOG 20190630-20:42:19] epoch: 618 train-loss: 0.00032117057162395213\n",
      "[LOG 20190630-20:42:19] epoch: 619 train-loss: 0.0003200034498149762\n",
      "[LOG 20190630-20:42:20] epoch: 620 train-loss: 0.00031886090368971054\n",
      "[LOG 20190630-20:42:20] epoch: 620 new best train-loss: 0.00031886090368971054 found\n",
      "[LOG 20190630-20:42:20] epoch: 621 train-loss: 0.0003177428868639254\n",
      "[LOG 20190630-20:42:21] epoch: 622 train-loss: 0.000316650192189627\n",
      "[LOG 20190630-20:42:21] epoch: 623 train-loss: 0.0003155818906179775\n",
      "[LOG 20190630-20:42:22] epoch: 624 train-loss: 0.00031453783253709844\n",
      "[LOG 20190630-20:42:22] epoch: 625 train-loss: 0.0003135190213470196\n",
      "[LOG 20190630-20:42:23] epoch: 626 train-loss: 0.0003125247324078373\n",
      "[LOG 20190630-20:42:23] epoch: 627 train-loss: 0.00031155458873399766\n",
      "[LOG 20190630-20:42:24] epoch: 628 train-loss: 0.00031060887113198987\n",
      "[LOG 20190630-20:42:25] epoch: 629 train-loss: 0.0003096880777775368\n",
      "[LOG 20190630-20:42:25] epoch: 630 train-loss: 0.00030879116661708395\n",
      "[LOG 20190630-20:42:25] epoch: 630 new best train-loss: 0.00030879116661708395 found\n",
      "[LOG 20190630-20:42:26] epoch: 631 train-loss: 0.00030791829885856714\n",
      "[LOG 20190630-20:42:26] epoch: 632 train-loss: 0.00030706953975823126\n",
      "[LOG 20190630-20:42:26] epoch: 633 train-loss: 0.0003062447733555018\n",
      "[LOG 20190630-20:42:27] epoch: 634 train-loss: 0.00030544370883944794\n",
      "[LOG 20190630-20:42:28] epoch: 635 train-loss: 0.000304666615647875\n",
      "[LOG 20190630-20:42:28] epoch: 636 train-loss: 0.0003039133855509135\n",
      "[LOG 20190630-20:42:29] epoch: 637 train-loss: 0.0003031832784472499\n",
      "[LOG 20190630-20:42:29] epoch: 638 train-loss: 0.00030247634822444525\n",
      "[LOG 20190630-20:42:30] epoch: 639 train-loss: 0.00030179294094523357\n",
      "[LOG 20190630-20:42:30] epoch: 640 train-loss: 0.0003011327701187838\n",
      "[LOG 20190630-20:42:30] epoch: 640 new best train-loss: 0.0003011327701187838 found\n",
      "[LOG 20190630-20:42:31] epoch: 641 train-loss: 0.0003004952764058544\n",
      "[LOG 20190630-20:42:32] epoch: 642 train-loss: 0.00029988034089001303\n",
      "[LOG 20190630-20:42:32] epoch: 643 train-loss: 0.0002992883826209436\n",
      "[LOG 20190630-20:42:33] epoch: 644 train-loss: 0.0002987188561291987\n",
      "[LOG 20190630-20:42:34] epoch: 645 train-loss: 0.0002981715699661436\n",
      "[LOG 20190630-20:42:34] epoch: 646 train-loss: 0.0002976465441406617\n",
      "[LOG 20190630-20:42:35] epoch: 647 train-loss: 0.00029714377706113737\n",
      "[LOG 20190630-20:42:35] epoch: 648 train-loss: 0.0002966625497720088\n",
      "[LOG 20190630-20:42:36] epoch: 649 train-loss: 0.00029620288046317\n",
      "[LOG 20190630-20:42:36] epoch: 650 train-loss: 0.0002957649037398369\n",
      "[LOG 20190630-20:42:36] epoch: 650 new best train-loss: 0.0002957649037398369 found\n",
      "[LOG 20190630-20:42:37] epoch: 651 train-loss: 0.0002953482653538231\n",
      "[LOG 20190630-20:42:37] epoch: 652 train-loss: 0.0002949526697193505\n",
      "[LOG 20190630-20:42:38] epoch: 653 train-loss: 0.0002945778680896183\n",
      "[LOG 20190630-20:42:38] epoch: 654 train-loss: 0.000294223946866623\n",
      "[LOG 20190630-20:42:39] epoch: 655 train-loss: 0.0002938906525287166\n",
      "[LOG 20190630-20:42:39] epoch: 656 train-loss: 0.000293577690172242\n",
      "[LOG 20190630-20:42:40] epoch: 657 train-loss: 0.00029328484470170224\n",
      "[LOG 20190630-20:42:40] epoch: 658 train-loss: 0.00029301206586751505\n",
      "[LOG 20190630-20:42:41] epoch: 659 train-loss: 0.00029275907104420185\n",
      "[LOG 20190630-20:42:41] epoch: 660 train-loss: 0.0002925255957961781\n",
      "[LOG 20190630-20:42:41] epoch: 660 new best train-loss: 0.0002925255957961781 found\n",
      "[LOG 20190630-20:42:42] epoch: 661 train-loss: 0.0002923114143413841\n",
      "[LOG 20190630-20:42:43] epoch: 662 train-loss: 0.0002921165082625521\n",
      "[LOG 20190630-20:42:43] epoch: 663 train-loss: 0.00029194053558967425\n",
      "[LOG 20190630-20:42:44] epoch: 664 train-loss: 0.00029178325848988607\n",
      "[LOG 20190630-20:42:44] epoch: 665 train-loss: 0.0002916444661877904\n",
      "[LOG 20190630-20:42:45] epoch: 666 train-loss: 0.00029152405181775976\n",
      "[LOG 20190630-20:42:46] epoch: 667 train-loss: 0.00029142171456442156\n",
      "[LOG 20190630-20:42:47] epoch: 668 train-loss: 0.00029133721341167984\n",
      "[LOG 20190630-20:42:47] epoch: 669 train-loss: 0.0002912703341735323\n",
      "[LOG 20190630-20:42:48] epoch: 670 train-loss: 0.0002912208981342701\n",
      "[LOG 20190630-20:42:48] epoch: 670 new best train-loss: 0.0002912208981342701 found\n",
      "[LOG 20190630-20:42:49] epoch: 671 train-loss: 0.0002911886488163873\n",
      "[LOG 20190630-20:42:49] epoch: 672 train-loss: 0.0002911733481596457\n",
      "[LOG 20190630-20:42:50] epoch: 673 train-loss: 0.0002911747451435076\n",
      "[LOG 20190630-20:42:50] epoch: 674 train-loss: 0.0002911926560500433\n",
      "[LOG 20190630-20:42:51] epoch: 675 train-loss: 0.00029122680416548974\n",
      "[LOG 20190630-20:42:51] epoch: 676 train-loss: 0.0002912769436989038\n",
      "[LOG 20190630-20:42:52] epoch: 677 train-loss: 0.0002913429489126429\n",
      "[LOG 20190630-20:42:52] epoch: 678 train-loss: 0.0002914244753355888\n",
      "[LOG 20190630-20:42:53] epoch: 679 train-loss: 0.00029152121578590595\n",
      "[LOG 20190630-20:42:53] epoch: 680 train-loss: 0.0002916330420248414\n",
      "[LOG 20190630-20:42:54] epoch: 681 train-loss: 0.00029175966187722224\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:42:54] epoch: 682 train-loss: 0.0002919009314155119\n",
      "[LOG 20190630-20:42:55] epoch: 683 train-loss: 0.0002920564515989099\n",
      "[LOG 20190630-20:42:55] epoch: 684 train-loss: 0.00029222604280221276\n",
      "[LOG 20190630-20:42:56] epoch: 685 train-loss: 0.00029240945377750904\n",
      "[LOG 20190630-20:42:57] epoch: 686 train-loss: 0.0002926064778421278\n",
      "[LOG 20190630-20:42:57] epoch: 687 train-loss: 0.0002928167036770901\n",
      "[LOG 20190630-20:42:58] epoch: 688 train-loss: 0.00029303983842510206\n",
      "[LOG 20190630-20:42:58] epoch: 689 train-loss: 0.00029327591255423613\n",
      "[LOG 20190630-20:42:59] epoch: 690 train-loss: 0.00029352471824495296\n",
      "[LOG 20190630-20:42:59] epoch: 691 train-loss: 0.0002937857266260835\n",
      "[LOG 20190630-20:43:00] epoch: 692 train-loss: 0.00029405851864794386\n",
      "[LOG 20190630-20:43:01] epoch: 693 train-loss: 0.00029434309840326023\n",
      "[LOG 20190630-20:43:01] epoch: 694 train-loss: 0.0002946393251477275\n",
      "[LOG 20190630-20:43:02] epoch: 695 train-loss: 0.0002949469701434282\n",
      "[LOG 20190630-20:43:02] epoch: 696 train-loss: 0.0002952655415811023\n",
      "[LOG 20190630-20:43:03] epoch: 697 train-loss: 0.0002955949121314916\n",
      "[LOG 20190630-20:43:04] epoch: 698 train-loss: 0.0002959347314117622\n",
      "[LOG 20190630-20:43:04] epoch: 699 train-loss: 0.0002962846067475766\n",
      "[LOG 20190630-20:43:05] epoch: 700 train-loss: 0.00029664450767086237\n",
      "[LOG 20190630-20:43:05] epoch: 701 train-loss: 0.00029701381640734326\n",
      "[LOG 20190630-20:43:06] epoch: 702 train-loss: 0.0002973926475533517\n",
      "[LOG 20190630-20:43:06] epoch: 703 train-loss: 0.00029778081216136343\n",
      "[LOG 20190630-20:43:07] epoch: 704 train-loss: 0.0002981780678510404\n",
      "[LOG 20190630-20:43:07] epoch: 705 train-loss: 0.0002985838877975766\n",
      "[LOG 20190630-20:43:08] epoch: 706 train-loss: 0.0002989977217566775\n",
      "[LOG 20190630-20:43:08] epoch: 707 train-loss: 0.0002994198221131228\n",
      "[LOG 20190630-20:43:09] epoch: 708 train-loss: 0.00029984983007125265\n",
      "[LOG 20190630-20:43:09] epoch: 709 train-loss: 0.0003002875612310163\n",
      "[LOG 20190630-20:43:10] epoch: 710 train-loss: 0.00030073254129092675\n",
      "[LOG 20190630-20:43:10] epoch: 711 train-loss: 0.0003011843416516058\n",
      "[LOG 20190630-20:43:11] epoch: 712 train-loss: 0.0003016431744526926\n",
      "[LOG 20190630-20:43:12] epoch: 713 train-loss: 0.00030210891441129206\n",
      "[LOG 20190630-20:43:12] epoch: 714 train-loss: 0.0003025811079169216\n",
      "[LOG 20190630-20:43:13] epoch: 715 train-loss: 0.0003030590683010814\n",
      "[LOG 20190630-20:43:13] epoch: 716 train-loss: 0.0003035426973383437\n",
      "[LOG 20190630-20:43:14] epoch: 717 train-loss: 0.0003040322753804503\n",
      "[LOG 20190630-20:43:15] epoch: 718 train-loss: 0.00030452748501375027\n",
      "[LOG 20190630-20:43:16] epoch: 719 train-loss: 0.0003050279885883356\n",
      "[LOG 20190630-20:43:17] epoch: 720 train-loss: 0.00030553314900316764\n",
      "[LOG 20190630-20:43:18] epoch: 721 train-loss: 0.000306042842566967\n",
      "[LOG 20190630-20:43:18] epoch: 722 train-loss: 0.0003065573791900533\n",
      "[LOG 20190630-20:43:19] epoch: 723 train-loss: 0.000307076648596194\n",
      "[LOG 20190630-20:43:20] epoch: 724 train-loss: 0.00030760012077735155\n",
      "[LOG 20190630-20:43:20] epoch: 725 train-loss: 0.00030812722229711653\n",
      "[LOG 20190630-20:43:21] epoch: 726 train-loss: 0.0003086577758040221\n",
      "[LOG 20190630-20:43:21] epoch: 727 train-loss: 0.00030919213759261766\n",
      "[LOG 20190630-20:43:22] epoch: 728 train-loss: 0.00030973012576396286\n",
      "[LOG 20190630-20:43:22] epoch: 729 train-loss: 0.00031027136856209836\n",
      "[LOG 20190630-20:43:23] epoch: 730 train-loss: 0.0003108153389348445\n",
      "[LOG 20190630-20:43:23] epoch: 731 train-loss: 0.0003113621214652085\n",
      "[LOG 20190630-20:43:24] epoch: 732 train-loss: 0.0003119118073300342\n",
      "[LOG 20190630-20:43:24] epoch: 733 train-loss: 0.00031246402727447276\n",
      "[LOG 20190630-20:43:25] epoch: 734 train-loss: 0.0003130186628368392\n",
      "[LOG 20190630-20:43:25] epoch: 735 train-loss: 0.0003135750462206488\n",
      "[LOG 20190630-20:43:26] epoch: 736 train-loss: 0.0003141330887501681\n",
      "[LOG 20190630-20:43:26] epoch: 737 train-loss: 0.00031469311056753213\n",
      "[LOG 20190630-20:43:27] epoch: 738 train-loss: 0.0003152551792027225\n",
      "[LOG 20190630-20:43:27] epoch: 739 train-loss: 0.0003158191318561876\n",
      "[LOG 20190630-20:43:28] epoch: 740 train-loss: 0.00031638381369702984\n",
      "[LOG 20190630-20:43:28] epoch: 741 train-loss: 0.00031694939866611094\n",
      "[LOG 20190630-20:43:29] epoch: 742 train-loss: 0.00031751605683894013\n",
      "[LOG 20190630-20:43:29] epoch: 743 train-loss: 0.0003180835205967014\n",
      "[LOG 20190630-20:43:30] epoch: 744 train-loss: 0.0003186516778441728\n",
      "[LOG 20190630-20:43:30] epoch: 745 train-loss: 0.00031922072821544134\n",
      "[LOG 20190630-20:43:31] epoch: 746 train-loss: 0.00031978956030798145\n",
      "[LOG 20190630-20:43:31] epoch: 747 train-loss: 0.0003203584735729237\n",
      "[LOG 20190630-20:43:31] epoch: 748 train-loss: 0.0003209275621429697\n",
      "[LOG 20190630-20:43:32] epoch: 749 train-loss: 0.0003214968987776956\n",
      "[LOG 20190630-20:43:32] epoch: 750 train-loss: 0.0003220657561087137\n",
      "[LOG 20190630-20:43:33] epoch: 751 train-loss: 0.0003226340777473524\n",
      "[LOG 20190630-20:43:33] epoch: 752 train-loss: 0.00032320084005732497\n",
      "[LOG 20190630-20:43:34] epoch: 753 train-loss: 0.0003237668524889159\n",
      "[LOG 20190630-20:43:34] epoch: 754 train-loss: 0.0003243319606553996\n",
      "[LOG 20190630-20:43:35] epoch: 755 train-loss: 0.0003248963000714866\n",
      "[LOG 20190630-20:43:35] epoch: 756 train-loss: 0.0003254595540056471\n",
      "[LOG 20190630-20:43:36] epoch: 757 train-loss: 0.0003260206233335339\n",
      "[LOG 20190630-20:43:36] epoch: 758 train-loss: 0.0003265797499807377\n",
      "[LOG 20190630-20:43:37] epoch: 759 train-loss: 0.000327137542399214\n",
      "[LOG 20190630-20:43:37] epoch: 760 train-loss: 0.00032769357449069503\n",
      "[LOG 20190630-20:43:38] epoch: 761 train-loss: 0.0003282472914634127\n",
      "[LOG 20190630-20:43:38] epoch: 762 train-loss: 0.0003287991191882611\n",
      "[LOG 20190630-20:43:39] epoch: 763 train-loss: 0.0003293476768249093\n",
      "[LOG 20190630-20:43:39] epoch: 764 train-loss: 0.00032989386818371713\n",
      "[LOG 20190630-20:43:40] epoch: 765 train-loss: 0.00033043780422303826\n",
      "[LOG 20190630-20:43:40] epoch: 766 train-loss: 0.00033097972209361615\n",
      "[LOG 20190630-20:43:41] epoch: 767 train-loss: 0.0003315189403565455\n",
      "[LOG 20190630-20:43:41] epoch: 768 train-loss: 0.0003320552598324866\n",
      "[LOG 20190630-20:43:42] epoch: 769 train-loss: 0.00033258848043260514\n",
      "[LOG 20190630-20:43:42] epoch: 770 train-loss: 0.00033311864558527304\n",
      "[LOG 20190630-20:43:43] epoch: 771 train-loss: 0.0003336461854814843\n",
      "[LOG 20190630-20:43:43] epoch: 772 train-loss: 0.00033417144436498347\n",
      "[LOG 20190630-20:43:44] epoch: 773 train-loss: 0.0003346939586208464\n",
      "[LOG 20190630-20:43:45] epoch: 774 train-loss: 0.0003352126398112887\n",
      "[LOG 20190630-20:43:45] epoch: 775 train-loss: 0.00033572795541658706\n",
      "[LOG 20190630-20:43:46] epoch: 776 train-loss: 0.00033624042202973214\n",
      "[LOG 20190630-20:43:46] epoch: 777 train-loss: 0.00033674994028842775\n",
      "[LOG 20190630-20:43:47] epoch: 778 train-loss: 0.0003372568601207604\n",
      "[LOG 20190630-20:43:47] epoch: 779 train-loss: 0.00033776050941014546\n",
      "[LOG 20190630-20:43:48] epoch: 780 train-loss: 0.00033826027038230677\n",
      "[LOG 20190630-20:43:49] epoch: 781 train-loss: 0.00033875640406222374\n",
      "[LOG 20190630-20:43:49] epoch: 782 train-loss: 0.0003392491785234597\n",
      "[LOG 20190630-20:43:50] epoch: 783 train-loss: 0.0003397388140911062\n",
      "[LOG 20190630-20:43:51] epoch: 784 train-loss: 0.0003402246429686784\n",
      "[LOG 20190630-20:43:51] epoch: 785 train-loss: 0.0003407071558285679\n",
      "[LOG 20190630-20:43:52] epoch: 786 train-loss: 0.0003411854693240457\n",
      "[LOG 20190630-20:43:53] epoch: 787 train-loss: 0.0003416590707274736\n",
      "[LOG 20190630-20:43:53] epoch: 788 train-loss: 0.00034212891682727786\n",
      "[LOG 20190630-20:43:54] epoch: 789 train-loss: 0.0003425952047564351\n",
      "[LOG 20190630-20:43:54] epoch: 790 train-loss: 0.0003430570973250724\n",
      "[LOG 20190630-20:43:55] epoch: 791 train-loss: 0.0003435148655626108\n",
      "[LOG 20190630-20:43:56] epoch: 792 train-loss: 0.0003439681968302466\n",
      "[LOG 20190630-20:43:56] epoch: 793 train-loss: 0.0003444153428517893\n",
      "[LOG 20190630-20:43:57] epoch: 794 train-loss: 0.0003448577763265348\n",
      "[LOG 20190630-20:43:57] epoch: 795 train-loss: 0.0003452953401392733\n",
      "[LOG 20190630-20:43:58] epoch: 796 train-loss: 0.0003457277321103902\n",
      "[LOG 20190630-20:43:59] epoch: 797 train-loss: 0.0003461546536982496\n",
      "[LOG 20190630-20:44:00] epoch: 798 train-loss: 0.0003465763140866329\n",
      "[LOG 20190630-20:44:00] epoch: 799 train-loss: 0.0003469911303000117\n",
      "[LOG 20190630-20:44:01] epoch: 800 train-loss: 0.0003474000784535747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:44:01] epoch: 801 train-loss: 0.0003478033336250519\n",
      "[LOG 20190630-20:44:02] epoch: 802 train-loss: 0.00034820018663594965\n",
      "[LOG 20190630-20:44:03] epoch: 803 train-loss: 0.0003485908516722702\n",
      "[LOG 20190630-20:44:03] epoch: 804 train-loss: 0.00034897478462880827\n",
      "[LOG 20190630-20:44:04] epoch: 805 train-loss: 0.00034935097846755525\n",
      "[LOG 20190630-20:44:04] epoch: 806 train-loss: 0.0003497193531529774\n",
      "[LOG 20190630-20:44:05] epoch: 807 train-loss: 0.00035008072723030637\n",
      "[LOG 20190630-20:44:05] epoch: 808 train-loss: 0.0003504348210299213\n",
      "[LOG 20190630-20:44:06] epoch: 809 train-loss: 0.0003507818835259968\n",
      "[LOG 20190630-20:44:06] epoch: 810 train-loss: 0.00035112122282043856\n",
      "[LOG 20190630-20:44:07] epoch: 811 train-loss: 0.00035145261290381313\n",
      "[LOG 20190630-20:44:08] epoch: 812 train-loss: 0.0003517748373269569\n",
      "[LOG 20190630-20:44:08] epoch: 813 train-loss: 0.00035208916870033136\n",
      "[LOG 20190630-20:44:09] epoch: 814 train-loss: 0.0003523951704664796\n",
      "[LOG 20190630-20:44:09] epoch: 815 train-loss: 0.0003526926361701044\n",
      "[LOG 20190630-20:44:10] epoch: 816 train-loss: 0.0003529817608978192\n",
      "[LOG 20190630-20:44:10] epoch: 817 train-loss: 0.00035326230272403336\n",
      "[LOG 20190630-20:44:11] epoch: 818 train-loss: 0.0003535332421051862\n",
      "[LOG 20190630-20:44:11] epoch: 819 train-loss: 0.0003537944417075778\n",
      "[LOG 20190630-20:44:12] epoch: 820 train-loss: 0.00035404704613029025\n",
      "[LOG 20190630-20:44:12] epoch: 821 train-loss: 0.0003542906279108138\n",
      "[LOG 20190630-20:44:13] epoch: 822 train-loss: 0.00035452531574264867\n",
      "[LOG 20190630-20:44:14] epoch: 823 train-loss: 0.0003547507849361864\n",
      "[LOG 20190630-20:44:14] epoch: 824 train-loss: 0.0003549667385414068\n",
      "[LOG 20190630-20:44:15] epoch: 825 train-loss: 0.0003551718418748351\n",
      "[LOG 20190630-20:44:15] epoch: 826 train-loss: 0.0003553671326699259\n",
      "[LOG 20190630-20:44:16] epoch: 827 train-loss: 0.000355553042027168\n",
      "[LOG 20190630-20:44:17] epoch: 828 train-loss: 0.00035572950491769006\n",
      "[LOG 20190630-20:44:18] epoch: 829 train-loss: 0.0003558962935130694\n",
      "[LOG 20190630-20:44:18] epoch: 830 train-loss: 0.00035605347466116655\n",
      "[LOG 20190630-20:44:19] epoch: 831 train-loss: 0.00035620052949525416\n",
      "[LOG 20190630-20:44:20] epoch: 832 train-loss: 0.00035633622064779047\n",
      "[LOG 20190630-20:44:20] epoch: 833 train-loss: 0.00035646133710542927\n",
      "[LOG 20190630-20:44:21] epoch: 834 train-loss: 0.0003565763804544986\n",
      "[LOG 20190630-20:44:21] epoch: 835 train-loss: 0.0003566809850781283\n",
      "[LOG 20190630-20:44:22] epoch: 836 train-loss: 0.00035677513278642436\n",
      "[LOG 20190630-20:44:23] epoch: 837 train-loss: 0.00035685886678038514\n",
      "[LOG 20190630-20:44:23] epoch: 838 train-loss: 0.0003569317168512498\n",
      "[LOG 20190630-20:44:24] epoch: 839 train-loss: 0.0003569928730939864\n",
      "[LOG 20190630-20:44:24] epoch: 840 train-loss: 0.00035704237734535127\n",
      "[LOG 20190630-20:44:25] epoch: 841 train-loss: 0.000357080884896277\n",
      "[LOG 20190630-20:44:25] epoch: 842 train-loss: 0.00035710857264348306\n",
      "[LOG 20190630-20:44:26] epoch: 843 train-loss: 0.00035712512635655\n",
      "[LOG 20190630-20:44:26] epoch: 844 train-loss: 0.00035713036413653754\n",
      "[LOG 20190630-20:44:27] epoch: 845 train-loss: 0.00035712445742319687\n",
      "[LOG 20190630-20:44:27] epoch: 846 train-loss: 0.000357107128820644\n",
      "[LOG 20190630-20:44:28] epoch: 847 train-loss: 0.0003570771282284113\n",
      "[LOG 20190630-20:44:29] epoch: 848 train-loss: 0.00035703524144992116\n",
      "[LOG 20190630-20:44:29] epoch: 849 train-loss: 0.00035698169813258573\n",
      "[LOG 20190630-20:44:30] epoch: 850 train-loss: 0.00035691705852514133\n",
      "[LOG 20190630-20:44:30] epoch: 851 train-loss: 0.00035684104841493536\n",
      "[LOG 20190630-20:44:31] epoch: 852 train-loss: 0.00035675374056154396\n",
      "[LOG 20190630-20:44:31] epoch: 853 train-loss: 0.00035665495624925825\n",
      "[LOG 20190630-20:44:32] epoch: 854 train-loss: 0.0003565450124369818\n",
      "[LOG 20190630-20:44:32] epoch: 855 train-loss: 0.00035642252760226256\n",
      "[LOG 20190630-20:44:33] epoch: 856 train-loss: 0.0003562889664863178\n",
      "[LOG 20190630-20:44:33] epoch: 857 train-loss: 0.00035614422949947766\n",
      "[LOG 20190630-20:44:34] epoch: 858 train-loss: 0.0003559884967216931\n",
      "[LOG 20190630-20:44:34] epoch: 859 train-loss: 0.00035582205146056367\n",
      "[LOG 20190630-20:44:35] epoch: 860 train-loss: 0.0003556454339559423\n",
      "[LOG 20190630-20:44:36] epoch: 861 train-loss: 0.0003554582981450949\n",
      "[LOG 20190630-20:44:36] epoch: 862 train-loss: 0.0003552606781340728\n",
      "[LOG 20190630-20:44:37] epoch: 863 train-loss: 0.0003550530473148683\n",
      "[LOG 20190630-20:44:37] epoch: 864 train-loss: 0.00035483457759255543\n",
      "[LOG 20190630-20:44:38] epoch: 865 train-loss: 0.00035460629715089453\n",
      "[LOG 20190630-20:44:39] epoch: 866 train-loss: 0.0003543690663718735\n",
      "[LOG 20190630-20:44:39] epoch: 867 train-loss: 0.0003541230398695916\n",
      "[LOG 20190630-20:44:40] epoch: 868 train-loss: 0.0003538681448844727\n",
      "[LOG 20190630-20:44:41] epoch: 869 train-loss: 0.00035360508672965807\n",
      "[LOG 20190630-20:44:41] epoch: 870 train-loss: 0.00035333389041625196\n",
      "[LOG 20190630-20:44:42] epoch: 871 train-loss: 0.0003530546791807865\n",
      "[LOG 20190630-20:44:42] epoch: 872 train-loss: 0.00035276715925647295\n",
      "[LOG 20190630-20:44:43] epoch: 873 train-loss: 0.000352472296981432\n",
      "[LOG 20190630-20:44:43] epoch: 874 train-loss: 0.0003521705402818043\n",
      "[LOG 20190630-20:44:44] epoch: 875 train-loss: 0.0003518620583236043\n",
      "[LOG 20190630-20:44:44] epoch: 876 train-loss: 0.0003515470152706257\n",
      "[LOG 20190630-20:44:45] epoch: 877 train-loss: 0.0003512253761073225\n",
      "[LOG 20190630-20:44:45] epoch: 878 train-loss: 0.0003508972788495157\n",
      "[LOG 20190630-20:44:46] epoch: 879 train-loss: 0.0003505629995288473\n",
      "[LOG 20190630-20:44:46] epoch: 880 train-loss: 0.0003502232357277535\n",
      "[LOG 20190630-20:44:47] epoch: 881 train-loss: 0.00034987866388291877\n",
      "[LOG 20190630-20:44:47] epoch: 882 train-loss: 0.00034952894384332467\n",
      "[LOG 20190630-20:44:47] epoch: 883 train-loss: 0.00034917430843961483\n",
      "[LOG 20190630-20:44:48] epoch: 884 train-loss: 0.00034881461533586844\n",
      "[LOG 20190630-20:44:49] epoch: 885 train-loss: 0.00034845047957787756\n",
      "[LOG 20190630-20:44:50] epoch: 886 train-loss: 0.0003480823945665179\n",
      "[LOG 20190630-20:44:50] epoch: 887 train-loss: 0.00034771051446114143\n",
      "[LOG 20190630-20:44:51] epoch: 888 train-loss: 0.0003473352508081007\n",
      "[LOG 20190630-20:44:51] epoch: 889 train-loss: 0.00034695696672315535\n",
      "[LOG 20190630-20:44:52] epoch: 890 train-loss: 0.00034657526680348383\n",
      "[LOG 20190630-20:44:52] epoch: 891 train-loss: 0.0003461907510882156\n",
      "[LOG 20190630-20:44:53] epoch: 892 train-loss: 0.00034580294413899537\n",
      "[LOG 20190630-20:44:53] epoch: 893 train-loss: 0.0003454128047906124\n",
      "[LOG 20190630-20:44:54] epoch: 894 train-loss: 0.0003450205235822068\n",
      "[LOG 20190630-20:44:54] epoch: 895 train-loss: 0.00034462632515896985\n",
      "[LOG 20190630-20:44:55] epoch: 896 train-loss: 0.000344230471910123\n",
      "[LOG 20190630-20:44:55] epoch: 897 train-loss: 0.00034383336969767697\n",
      "[LOG 20190630-20:44:56] epoch: 898 train-loss: 0.00034343469042141805\n",
      "[LOG 20190630-20:44:57] epoch: 899 train-loss: 0.000343034720799551\n",
      "[LOG 20190630-20:44:57] epoch: 900 train-loss: 0.0003426340897476621\n",
      "[LOG 20190630-20:44:58] epoch: 901 train-loss: 0.0003422328804845165\n",
      "[LOG 20190630-20:44:58] epoch: 902 train-loss: 0.0003418311055156664\n",
      "[LOG 20190630-20:44:59] epoch: 903 train-loss: 0.00034142867480113637\n",
      "[LOG 20190630-20:44:59] epoch: 904 train-loss: 0.00034102554263881757\n",
      "[LOG 20190630-20:44:59] epoch: 905 train-loss: 0.0003406223313504597\n",
      "[LOG 20190630-20:45:00] epoch: 906 train-loss: 0.0003402192148769245\n",
      "[LOG 20190630-20:45:00] epoch: 907 train-loss: 0.00033981615274569776\n",
      "[LOG 20190630-20:45:01] epoch: 908 train-loss: 0.00033941341985155304\n",
      "[LOG 20190630-20:45:02] epoch: 909 train-loss: 0.0003390112030956516\n",
      "[LOG 20190630-20:45:02] epoch: 910 train-loss: 0.00033860941289276525\n",
      "[LOG 20190630-20:45:03] epoch: 911 train-loss: 0.00033820859357547306\n",
      "[LOG 20190630-20:45:03] epoch: 912 train-loss: 0.00033780843455133436\n",
      "[LOG 20190630-20:45:04] epoch: 913 train-loss: 0.00033740914409463585\n",
      "[LOG 20190630-20:45:04] epoch: 914 train-loss: 0.00033701092502269603\n",
      "[LOG 20190630-20:45:05] epoch: 915 train-loss: 0.0003366139383160771\n",
      "[LOG 20190630-20:45:05] epoch: 916 train-loss: 0.0003362183331319102\n",
      "[LOG 20190630-20:45:06] epoch: 917 train-loss: 0.00033582401988496713\n",
      "[LOG 20190630-20:45:06] epoch: 918 train-loss: 0.00033543103745614644\n",
      "[LOG 20190630-20:45:07] epoch: 919 train-loss: 0.0003350394631524978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:45:07] epoch: 920 train-loss: 0.00033464947568973\n",
      "[LOG 20190630-20:45:07] epoch: 921 train-loss: 0.00033426131267333403\n",
      "[LOG 20190630-20:45:08] epoch: 922 train-loss: 0.00033387479970770073\n",
      "[LOG 20190630-20:45:08] epoch: 923 train-loss: 0.00033349014188388537\n",
      "[LOG 20190630-20:45:09] epoch: 924 train-loss: 0.00033310725393675966\n",
      "[LOG 20190630-20:45:09] epoch: 925 train-loss: 0.00033272687164753734\n",
      "[LOG 20190630-20:45:10] epoch: 926 train-loss: 0.0003323486894259986\n",
      "[LOG 20190630-20:45:10] epoch: 927 train-loss: 0.0003319729353279399\n",
      "[LOG 20190630-20:45:11] epoch: 928 train-loss: 0.00033159933377646666\n",
      "[LOG 20190630-20:45:11] epoch: 929 train-loss: 0.00033122818467745674\n",
      "[LOG 20190630-20:45:12] epoch: 930 train-loss: 0.00033085945642596926\n",
      "[LOG 20190630-20:45:12] epoch: 931 train-loss: 0.00033049332478185534\n",
      "[LOG 20190630-20:45:13] epoch: 932 train-loss: 0.0003301299209397257\n",
      "[LOG 20190630-20:45:14] epoch: 933 train-loss: 0.0003297694086086267\n",
      "[LOG 20190630-20:45:14] epoch: 934 train-loss: 0.0003294122579973191\n",
      "[LOG 20190630-20:45:15] epoch: 935 train-loss: 0.00032905839407249005\n",
      "[LOG 20190630-20:45:15] epoch: 936 train-loss: 0.00032870782388272346\n",
      "[LOG 20190630-20:45:16] epoch: 937 train-loss: 0.0003283601236034883\n",
      "[LOG 20190630-20:45:16] epoch: 938 train-loss: 0.00032801578367980255\n",
      "[LOG 20190630-20:45:17] epoch: 939 train-loss: 0.00032767493121355074\n",
      "[LOG 20190630-20:45:17] epoch: 940 train-loss: 0.00032733778584770334\n",
      "[LOG 20190630-20:45:18] epoch: 941 train-loss: 0.00032700410156394355\n",
      "[LOG 20190630-20:45:18] epoch: 942 train-loss: 0.00032667370169292553\n",
      "[LOG 20190630-20:45:19] epoch: 943 train-loss: 0.0003263469213834469\n",
      "[LOG 20190630-20:45:19] epoch: 944 train-loss: 0.00032602370310996776\n",
      "[LOG 20190630-20:45:20] epoch: 945 train-loss: 0.0003257043006215099\n",
      "[LOG 20190630-20:45:21] epoch: 946 train-loss: 0.0003253888235121849\n",
      "[LOG 20190630-20:45:21] epoch: 947 train-loss: 0.0003250774666412326\n",
      "[LOG 20190630-20:45:22] epoch: 948 train-loss: 0.0003247699225994438\n",
      "[LOG 20190630-20:45:22] epoch: 949 train-loss: 0.00032446637283101154\n",
      "[LOG 20190630-20:45:23] epoch: 950 train-loss: 0.00032416683802694024\n",
      "[LOG 20190630-20:45:23] epoch: 951 train-loss: 0.00032387113196818973\n",
      "[LOG 20190630-20:45:24] epoch: 952 train-loss: 0.00032357917166336847\n",
      "[LOG 20190630-20:45:25] epoch: 953 train-loss: 0.0003232914143609378\n",
      "[LOG 20190630-20:45:25] epoch: 954 train-loss: 0.00032300793122885807\n",
      "[LOG 20190630-20:45:26] epoch: 955 train-loss: 0.00032272872385874507\n",
      "[LOG 20190630-20:45:26] epoch: 956 train-loss: 0.0003224537097139546\n",
      "[LOG 20190630-20:45:27] epoch: 957 train-loss: 0.00032218275555351283\n",
      "[LOG 20190630-20:45:28] epoch: 958 train-loss: 0.0003219157549665397\n",
      "[LOG 20190630-20:45:28] epoch: 959 train-loss: 0.00032165285983865033\n",
      "[LOG 20190630-20:45:29] epoch: 960 train-loss: 0.00032139398444996914\n",
      "[LOG 20190630-20:45:29] epoch: 961 train-loss: 0.00032113926249621727\n",
      "[LOG 20190630-20:45:30] epoch: 962 train-loss: 0.00032088903162730276\n",
      "[LOG 20190630-20:45:30] epoch: 963 train-loss: 0.00032064307015389204\n",
      "[LOG 20190630-20:45:31] epoch: 964 train-loss: 0.0003204008874035935\n",
      "[LOG 20190630-20:45:31] epoch: 965 train-loss: 0.0003201626127520285\n",
      "[LOG 20190630-20:45:32] epoch: 966 train-loss: 0.00031992836807148706\n",
      "[LOG 20190630-20:45:32] epoch: 967 train-loss: 0.0003196983084308158\n",
      "[LOG 20190630-20:45:33] epoch: 968 train-loss: 0.00031947258889886143\n",
      "[LOG 20190630-20:45:33] epoch: 969 train-loss: 0.0003192507676885725\n",
      "[LOG 20190630-20:45:34] epoch: 970 train-loss: 0.0003190329207427567\n",
      "[LOG 20190630-20:45:34] epoch: 971 train-loss: 0.0003188191046774591\n",
      "[LOG 20190630-20:45:35] epoch: 972 train-loss: 0.0003186092119449313\n",
      "[LOG 20190630-20:45:35] epoch: 973 train-loss: 0.0003184035488175141\n",
      "[LOG 20190630-20:45:36] epoch: 974 train-loss: 0.00031820177400732064\n",
      "[LOG 20190630-20:45:36] epoch: 975 train-loss: 0.0003180039709604898\n",
      "[LOG 20190630-20:45:37] epoch: 976 train-loss: 0.0003178100103014003\n",
      "[LOG 20190630-20:45:37] epoch: 977 train-loss: 0.00031761980130795564\n",
      "[LOG 20190630-20:45:38] epoch: 978 train-loss: 0.0003174338410190103\n",
      "[LOG 20190630-20:45:38] epoch: 979 train-loss: 0.0003172518079281872\n",
      "[LOG 20190630-20:45:39] epoch: 980 train-loss: 0.00031707338825981424\n",
      "[LOG 20190630-20:45:40] epoch: 981 train-loss: 0.0003168987987010041\n",
      "[LOG 20190630-20:45:40] epoch: 982 train-loss: 0.00031672812338001677\n",
      "[LOG 20190630-20:45:41] epoch: 983 train-loss: 0.00031656145893066423\n",
      "[LOG 20190630-20:45:41] epoch: 984 train-loss: 0.00031639855319554044\n",
      "[LOG 20190630-20:45:41] epoch: 985 train-loss: 0.0003162392069953057\n",
      "[LOG 20190630-20:45:42] epoch: 986 train-loss: 0.00031608372864866396\n",
      "[LOG 20190630-20:45:42] epoch: 987 train-loss: 0.00031593239350513613\n",
      "[LOG 20190630-20:45:43] epoch: 988 train-loss: 0.0003157845878831722\n",
      "[LOG 20190630-20:45:43] epoch: 989 train-loss: 0.00031564034748043923\n",
      "[LOG 20190630-20:45:44] epoch: 990 train-loss: 0.0003154998000809428\n",
      "[LOG 20190630-20:45:44] epoch: 991 train-loss: 0.000315363327445084\n",
      "[LOG 20190630-20:45:45] epoch: 992 train-loss: 0.00031523050779469486\n",
      "[LOG 20190630-20:45:45] epoch: 993 train-loss: 0.00031510123972111614\n",
      "[LOG 20190630-20:45:46] epoch: 994 train-loss: 0.0003149757210394455\n",
      "[LOG 20190630-20:45:46] epoch: 995 train-loss: 0.0003148539747144241\n",
      "[LOG 20190630-20:45:47] epoch: 996 train-loss: 0.00031473571903006814\n",
      "[LOG 20190630-20:45:47] epoch: 997 train-loss: 0.0003146209917304077\n",
      "[LOG 20190630-20:45:48] epoch: 998 train-loss: 0.0003145098287404835\n",
      "[LOG 20190630-20:45:48] epoch: 999 train-loss: 0.00031440264388038486\n",
      "[LOG 20190630-20:45:49] epoch: 1000 train-loss: 0.00031429909859070904\n",
      "[LOG 20190630-20:45:49] epoch: 1001 train-loss: 0.000314198932301224\n",
      "[LOG 20190630-20:45:50] epoch: 1002 train-loss: 0.00031410225346917287\n",
      "[LOG 20190630-20:45:50] epoch: 1003 train-loss: 0.0003140091714612936\n",
      "[LOG 20190630-20:45:51] epoch: 1004 train-loss: 0.00031391951506520854\n",
      "[LOG 20190630-20:45:51] epoch: 1005 train-loss: 0.0003138333299830265\n",
      "[LOG 20190630-20:45:52] epoch: 1006 train-loss: 0.0003137507228530012\n",
      "[LOG 20190630-20:45:52] epoch: 1007 train-loss: 0.000313671579988295\n",
      "[LOG 20190630-20:45:53] epoch: 1008 train-loss: 0.00031359579088530154\n",
      "[LOG 20190630-20:45:53] epoch: 1009 train-loss: 0.0003135234480851068\n",
      "[LOG 20190630-20:45:54] epoch: 1010 train-loss: 0.00031345463958132314\n",
      "[LOG 20190630-20:45:54] epoch: 1011 train-loss: 0.000313389049097168\n",
      "[LOG 20190630-20:45:55] epoch: 1012 train-loss: 0.0003133266682198155\n",
      "[LOG 20190630-20:45:55] epoch: 1013 train-loss: 0.00031326770204032073\n",
      "[LOG 20190630-20:45:56] epoch: 1014 train-loss: 0.0003132120864393073\n",
      "[LOG 20190630-20:45:56] epoch: 1015 train-loss: 0.00031315957494371105\n",
      "[LOG 20190630-20:45:57] epoch: 1016 train-loss: 0.0003131101548206061\n",
      "[LOG 20190630-20:45:58] epoch: 1017 train-loss: 0.00031306391088037344\n",
      "[LOG 20190630-20:45:58] epoch: 1018 train-loss: 0.00031302076172323723\n",
      "[LOG 20190630-20:45:59] epoch: 1019 train-loss: 0.00031298056887862913\n",
      "[LOG 20190630-20:45:59] epoch: 1020 train-loss: 0.000312943561539214\n",
      "[LOG 20190630-20:46:00] epoch: 1021 train-loss: 0.00031290966967389977\n",
      "[LOG 20190630-20:46:00] epoch: 1022 train-loss: 0.00031287859451367694\n",
      "[LOG 20190630-20:46:01] epoch: 1023 train-loss: 0.00031285033401218243\n",
      "[LOG 20190630-20:46:02] epoch: 1024 train-loss: 0.0003128250791633036\n",
      "[LOG 20190630-20:46:02] epoch: 1025 train-loss: 0.00031280256712307164\n",
      "[LOG 20190630-20:46:03] epoch: 1026 train-loss: 0.0003127827696971508\n",
      "[LOG 20190630-20:46:03] epoch: 1027 train-loss: 0.0003127657951154106\n",
      "[LOG 20190630-20:46:04] epoch: 1028 train-loss: 0.00031275143669518\n",
      "[LOG 20190630-20:46:04] epoch: 1029 train-loss: 0.00031273965623768163\n",
      "[LOG 20190630-20:46:05] epoch: 1030 train-loss: 0.0003127306179067091\n",
      "[LOG 20190630-20:46:05] epoch: 1031 train-loss: 0.00031272412388716475\n",
      "[LOG 20190630-20:46:06] epoch: 1032 train-loss: 0.0003127200679955422\n",
      "[LOG 20190630-20:46:06] epoch: 1033 train-loss: 0.0003127185607354477\n",
      "[LOG 20190630-20:46:07] epoch: 1034 train-loss: 0.00031271941543309367\n",
      "[LOG 20190630-20:46:07] epoch: 1035 train-loss: 0.00031272256023839873\n",
      "[LOG 20190630-20:46:08] epoch: 1036 train-loss: 0.0003127280788248754\n",
      "[LOG 20190630-20:46:08] epoch: 1037 train-loss: 0.00031273589956981596\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:46:09] epoch: 1038 train-loss: 0.00031274583102458564\n",
      "[LOG 20190630-20:46:09] epoch: 1039 train-loss: 0.0003127579775537015\n",
      "[LOG 20190630-20:46:10] epoch: 1040 train-loss: 0.00031277236735149927\n",
      "[LOG 20190630-20:46:10] epoch: 1041 train-loss: 0.0003127887150640163\n",
      "[LOG 20190630-20:46:11] epoch: 1042 train-loss: 0.0003128070543425565\n",
      "[LOG 20190630-20:46:11] epoch: 1043 train-loss: 0.00031282754366657173\n",
      "[LOG 20190630-20:46:12] epoch: 1044 train-loss: 0.0003128499295144138\n",
      "[LOG 20190630-20:46:13] epoch: 1045 train-loss: 0.0003128741329874174\n",
      "[LOG 20190630-20:46:13] epoch: 1046 train-loss: 0.0003129002382138424\n",
      "[LOG 20190630-20:46:14] epoch: 1047 train-loss: 0.0003129281310521037\n",
      "[LOG 20190630-20:46:15] epoch: 1048 train-loss: 0.00031295780422624375\n",
      "[LOG 20190630-20:46:15] epoch: 1049 train-loss: 0.00031298923977374216\n",
      "[LOG 20190630-20:46:16] epoch: 1050 train-loss: 0.00031302232355301385\n",
      "[LOG 20190630-20:46:16] epoch: 1051 train-loss: 0.00031305697552852507\n",
      "[LOG 20190630-20:46:17] epoch: 1052 train-loss: 0.0003130932338990533\n",
      "[LOG 20190630-20:46:18] epoch: 1053 train-loss: 0.0003131310547814792\n",
      "[LOG 20190630-20:46:18] epoch: 1054 train-loss: 0.00031317027105615125\n",
      "[LOG 20190630-20:46:19] epoch: 1055 train-loss: 0.00031321095730163506\n",
      "[LOG 20190630-20:46:19] epoch: 1056 train-loss: 0.0003132531421670137\n",
      "[LOG 20190630-20:46:20] epoch: 1057 train-loss: 0.00031329659964285383\n",
      "[LOG 20190630-20:46:21] epoch: 1058 train-loss: 0.0003133413501927862\n",
      "[LOG 20190630-20:46:21] epoch: 1059 train-loss: 0.0003133874408831616\n",
      "[LOG 20190630-20:46:22] epoch: 1060 train-loss: 0.0003134347159630124\n",
      "[LOG 20190630-20:46:22] epoch: 1061 train-loss: 0.0003134831229090196\n",
      "[LOG 20190630-20:46:23] epoch: 1062 train-loss: 0.00031353273720924335\n",
      "[LOG 20190630-20:46:23] epoch: 1063 train-loss: 0.00031358346859633457\n",
      "[LOG 20190630-20:46:24] epoch: 1064 train-loss: 0.0003136352302135492\n",
      "[LOG 20190630-20:46:24] epoch: 1065 train-loss: 0.00031368806617138034\n",
      "[LOG 20190630-20:46:25] epoch: 1066 train-loss: 0.00031374191235045146\n",
      "[LOG 20190630-20:46:26] epoch: 1067 train-loss: 0.0003137966630220035\n",
      "[LOG 20190630-20:46:26] epoch: 1068 train-loss: 0.0003138523038614949\n",
      "[LOG 20190630-20:46:27] epoch: 1069 train-loss: 0.00031390888534588157\n",
      "[LOG 20190630-20:46:27] epoch: 1070 train-loss: 0.00031396625672641676\n",
      "[LOG 20190630-20:46:28] epoch: 1071 train-loss: 0.0003140243682082655\n",
      "[LOG 20190630-20:46:28] epoch: 1072 train-loss: 0.0003140833052839298\n",
      "[LOG 20190630-20:46:29] epoch: 1073 train-loss: 0.0003141429440347565\n",
      "[LOG 20190630-20:46:29] epoch: 1074 train-loss: 0.0003142032469440892\n",
      "[LOG 20190630-20:46:30] epoch: 1075 train-loss: 0.0003142642256079853\n",
      "[LOG 20190630-20:46:31] epoch: 1076 train-loss: 0.0003143259234548168\n",
      "[LOG 20190630-20:46:31] epoch: 1077 train-loss: 0.00031438820701623627\n",
      "[LOG 20190630-20:46:32] epoch: 1078 train-loss: 0.00031445103104488226\n",
      "[LOG 20190630-20:46:32] epoch: 1079 train-loss: 0.00031451445147467894\n",
      "[LOG 20190630-20:46:33] epoch: 1080 train-loss: 0.0003145783646232303\n",
      "[LOG 20190630-20:46:34] epoch: 1081 train-loss: 0.0003146427127376228\n",
      "[LOG 20190630-20:46:34] epoch: 1082 train-loss: 0.00031470750559492444\n",
      "[LOG 20190630-20:46:35] epoch: 1083 train-loss: 0.0003147727743453288\n",
      "[LOG 20190630-20:46:36] epoch: 1084 train-loss: 0.00031483840962209797\n",
      "[LOG 20190630-20:46:36] epoch: 1085 train-loss: 0.0003149043775465543\n",
      "[LOG 20190630-20:46:37] epoch: 1086 train-loss: 0.0003149707285956538\n",
      "[LOG 20190630-20:46:37] epoch: 1087 train-loss: 0.00031503743889516045\n",
      "[LOG 20190630-20:46:38] epoch: 1088 train-loss: 0.00031510442931903526\n",
      "[LOG 20190630-20:46:39] epoch: 1089 train-loss: 0.00031517169486505736\n",
      "[LOG 20190630-20:46:39] epoch: 1090 train-loss: 0.00031523926804766234\n",
      "[LOG 20190630-20:46:40] epoch: 1091 train-loss: 0.0003153070497319277\n",
      "[LOG 20190630-20:46:40] epoch: 1092 train-loss: 0.0003153750151341228\n",
      "[LOG 20190630-20:46:41] epoch: 1093 train-loss: 0.0003154431501570798\n",
      "[LOG 20190630-20:46:41] epoch: 1094 train-loss: 0.0003155114898163447\n",
      "[LOG 20190630-20:46:42] epoch: 1095 train-loss: 0.00031557999932374514\n",
      "[LOG 20190630-20:46:42] epoch: 1096 train-loss: 0.0003156486141051573\n",
      "[LOG 20190630-20:46:43] epoch: 1097 train-loss: 0.00031571732029078703\n",
      "[LOG 20190630-20:46:44] epoch: 1098 train-loss: 0.0003157861547151697\n",
      "[LOG 20190630-20:46:44] epoch: 1099 train-loss: 0.0003158550514399394\n",
      "[LOG 20190630-20:46:45] epoch: 1100 train-loss: 0.00031592397021995566\n",
      "[LOG 20190630-20:46:45] epoch: 1101 train-loss: 0.0003159928717195726\n",
      "[LOG 20190630-20:46:46] epoch: 1102 train-loss: 0.00031606181141796696\n",
      "[LOG 20190630-20:46:46] epoch: 1103 train-loss: 0.0003161307231493993\n",
      "[LOG 20190630-20:46:46] epoch: 1104 train-loss: 0.0003161995830396336\n",
      "[LOG 20190630-20:46:47] epoch: 1105 train-loss: 0.00031626835789211327\n",
      "[LOG 20190630-20:46:47] epoch: 1106 train-loss: 0.0003163371020491468\n",
      "[LOG 20190630-20:46:48] epoch: 1107 train-loss: 0.00031640572797186906\n",
      "[LOG 20190630-20:46:48] epoch: 1108 train-loss: 0.0003164742206536175\n",
      "[LOG 20190630-20:46:49] epoch: 1109 train-loss: 0.00031654250437895826\n",
      "[LOG 20190630-20:46:49] epoch: 1110 train-loss: 0.0003166106437220151\n",
      "[LOG 20190630-20:46:50] epoch: 1111 train-loss: 0.00031667862617723586\n",
      "[LOG 20190630-20:46:50] epoch: 1112 train-loss: 0.00031674640945311694\n",
      "[LOG 20190630-20:46:51] epoch: 1113 train-loss: 0.0003168139585341123\n",
      "[LOG 20190630-20:46:51] epoch: 1114 train-loss: 0.0003168812415879074\n",
      "[LOG 20190630-20:46:52] epoch: 1115 train-loss: 0.0003169483268266049\n",
      "[LOG 20190630-20:46:52] epoch: 1116 train-loss: 0.0003170151394442655\n",
      "[LOG 20190630-20:46:53] epoch: 1117 train-loss: 0.00031708164351584855\n",
      "[LOG 20190630-20:46:53] epoch: 1118 train-loss: 0.00031714782085146\n",
      "[LOG 20190630-20:46:53] epoch: 1119 train-loss: 0.000317213608241218\n",
      "[LOG 20190630-20:46:54] epoch: 1120 train-loss: 0.00031727908572065644\n",
      "[LOG 20190630-20:46:54] epoch: 1121 train-loss: 0.000317344195991609\n",
      "[LOG 20190630-20:46:55] epoch: 1122 train-loss: 0.0003174089015374193\n",
      "[LOG 20190630-20:46:55] epoch: 1123 train-loss: 0.00031747319530950335\n",
      "[LOG 20190630-20:46:56] epoch: 1124 train-loss: 0.00031753706571180373\n",
      "[LOG 20190630-20:46:56] epoch: 1125 train-loss: 0.00031760051001583633\n",
      "[LOG 20190630-20:46:57] epoch: 1126 train-loss: 0.0003176634888859553\n",
      "[LOG 20190630-20:46:57] epoch: 1127 train-loss: 0.000317726002776908\n",
      "[LOG 20190630-20:46:58] epoch: 1128 train-loss: 0.00031778800780557503\n",
      "[LOG 20190630-20:46:58] epoch: 1129 train-loss: 0.00031784949374014104\n",
      "[LOG 20190630-20:46:59] epoch: 1130 train-loss: 0.0003179105183335196\n",
      "[LOG 20190630-20:46:59] epoch: 1131 train-loss: 0.0003179710395215807\n",
      "[LOG 20190630-20:47:00] epoch: 1132 train-loss: 0.0003180310216066573\n",
      "[LOG 20190630-20:47:00] epoch: 1133 train-loss: 0.0003180905050612637\n",
      "[LOG 20190630-20:47:00] epoch: 1134 train-loss: 0.00031814946282793244\n",
      "[LOG 20190630-20:47:01] epoch: 1135 train-loss: 0.0003182078962709056\n",
      "[LOG 20190630-20:47:01] epoch: 1136 train-loss: 0.0003182658203968458\n",
      "[LOG 20190630-20:47:02] epoch: 1137 train-loss: 0.000318323209285154\n",
      "[LOG 20190630-20:47:02] epoch: 1138 train-loss: 0.00031838007544138236\n",
      "[LOG 20190630-20:47:03] epoch: 1139 train-loss: 0.0003184364322805777\n",
      "[LOG 20190630-20:47:03] epoch: 1140 train-loss: 0.000318492277756377\n",
      "[LOG 20190630-20:47:04] epoch: 1141 train-loss: 0.00031854763096816896\n",
      "[LOG 20190630-20:47:04] epoch: 1142 train-loss: 0.000318602502147769\n",
      "[LOG 20190630-20:47:05] epoch: 1143 train-loss: 0.0003186568631008413\n",
      "[LOG 20190630-20:47:05] epoch: 1144 train-loss: 0.0003187107267876854\n",
      "[LOG 20190630-20:47:06] epoch: 1145 train-loss: 0.00031876409684628015\n",
      "[LOG 20190630-20:47:06] epoch: 1146 train-loss: 0.00031881694030744256\n",
      "[LOG 20190630-20:47:07] epoch: 1147 train-loss: 0.00031886925626167795\n",
      "[LOG 20190630-20:47:07] epoch: 1148 train-loss: 0.00031892105926090153\n",
      "[LOG 20190630-20:47:07] epoch: 1149 train-loss: 0.000318972392960859\n",
      "[LOG 20190630-20:47:08] epoch: 1150 train-loss: 0.00031902324440125085\n",
      "[LOG 20190630-20:47:08] epoch: 1151 train-loss: 0.0003190735976659198\n",
      "[LOG 20190630-20:47:09] epoch: 1152 train-loss: 0.0003191234693531442\n",
      "[LOG 20190630-20:47:09] epoch: 1153 train-loss: 0.0003191728833371599\n",
      "[LOG 20190630-20:47:10] epoch: 1154 train-loss: 0.00031922186030897137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:47:10] epoch: 1155 train-loss: 0.0003192704016328207\n",
      "[LOG 20190630-20:47:11] epoch: 1156 train-loss: 0.00031931853391142795\n",
      "[LOG 20190630-20:47:11] epoch: 1157 train-loss: 0.0003193662096236949\n",
      "[LOG 20190630-20:47:12] epoch: 1158 train-loss: 0.0003194134401383053\n",
      "[LOG 20190630-20:47:12] epoch: 1159 train-loss: 0.00031946016770234564\n",
      "[LOG 20190630-20:47:13] epoch: 1160 train-loss: 0.0003195064311967144\n",
      "[LOG 20190630-20:47:13] epoch: 1161 train-loss: 0.0003195522692749364\n",
      "[LOG 20190630-20:47:14] epoch: 1162 train-loss: 0.00031959768034539593\n",
      "[LOG 20190630-20:47:14] epoch: 1163 train-loss: 0.00031964262370820506\n",
      "[LOG 20190630-20:47:14] epoch: 1164 train-loss: 0.00031968711209628964\n",
      "[LOG 20190630-20:47:15] epoch: 1165 train-loss: 0.00031973116597328044\n",
      "[LOG 20190630-20:47:15] epoch: 1166 train-loss: 0.00031977477533473575\n",
      "[LOG 20190630-20:47:16] epoch: 1167 train-loss: 0.0003198179122136935\n",
      "[LOG 20190630-20:47:16] epoch: 1168 train-loss: 0.0003198605807028798\n",
      "[LOG 20190630-20:47:17] epoch: 1169 train-loss: 0.00031990278534976824\n",
      "[LOG 20190630-20:47:17] epoch: 1170 train-loss: 0.00031994452047001687\n",
      "[LOG 20190630-20:47:18] epoch: 1171 train-loss: 0.0003199858035713987\n",
      "[LOG 20190630-20:47:18] epoch: 1172 train-loss: 0.0003200266392013873\n",
      "[LOG 20190630-20:47:19] epoch: 1173 train-loss: 0.0003200670107617043\n",
      "[LOG 20190630-20:47:19] epoch: 1174 train-loss: 0.0003201069480383012\n",
      "[LOG 20190630-20:47:20] epoch: 1175 train-loss: 0.00032014642852118413\n",
      "[LOG 20190630-20:47:20] epoch: 1176 train-loss: 0.0003201854308372276\n",
      "[LOG 20190630-20:47:20] epoch: 1177 train-loss: 0.00032022393520492187\n",
      "[LOG 20190630-20:47:21] epoch: 1178 train-loss: 0.00032026197277446045\n",
      "[LOG 20190630-20:47:21] epoch: 1179 train-loss: 0.00032029950807554997\n",
      "[LOG 20190630-20:47:22] epoch: 1180 train-loss: 0.0003203365404260694\n",
      "[LOG 20190630-20:47:22] epoch: 1181 train-loss: 0.0003203730943823757\n",
      "[LOG 20190630-20:47:23] epoch: 1182 train-loss: 0.0003204091699444689\n",
      "[LOG 20190630-20:47:23] epoch: 1183 train-loss: 0.0003204447366442764\n",
      "[LOG 20190630-20:47:24] epoch: 1184 train-loss: 0.0003204797926628089\n",
      "[LOG 20190630-20:47:24] epoch: 1185 train-loss: 0.00032051437324298604\n",
      "[LOG 20190630-20:47:25] epoch: 1186 train-loss: 0.0003205484822501603\n",
      "[LOG 20190630-20:47:25] epoch: 1187 train-loss: 0.0003205821094525163\n",
      "[LOG 20190630-20:47:26] epoch: 1188 train-loss: 0.0003206152209713764\n",
      "[LOG 20190630-20:47:26] epoch: 1189 train-loss: 0.00032064783499663463\n",
      "[LOG 20190630-20:47:27] epoch: 1190 train-loss: 0.00032067996767182194\n",
      "[LOG 20190630-20:47:27] epoch: 1191 train-loss: 0.0003207115914847236\n",
      "[LOG 20190630-20:47:27] epoch: 1192 train-loss: 0.0003207427041616029\n",
      "[LOG 20190630-20:47:28] epoch: 1193 train-loss: 0.00032077331979962764\n",
      "[LOG 20190630-20:47:28] epoch: 1194 train-loss: 0.00032080344999485533\n",
      "[LOG 20190630-20:47:29] epoch: 1195 train-loss: 0.0003208330970210227\n",
      "[LOG 20190630-20:47:29] epoch: 1196 train-loss: 0.0003208622481452039\n",
      "[LOG 20190630-20:47:30] epoch: 1197 train-loss: 0.00032089089654618874\n",
      "[LOG 20190630-20:47:30] epoch: 1198 train-loss: 0.0003209190317647881\n",
      "[LOG 20190630-20:47:31] epoch: 1199 train-loss: 0.0003209466638054437\n",
      "[LOG 20190630-20:47:31] epoch: 1200 train-loss: 0.0003209738085843128\n",
      "[LOG 20190630-20:47:32] epoch: 1201 train-loss: 0.00032100045382321696\n",
      "[LOG 20190630-20:47:32] epoch: 1202 train-loss: 0.0003210266049791244\n",
      "[LOG 20190630-20:47:33] epoch: 1203 train-loss: 0.0003210522672816296\n",
      "[LOG 20190630-20:47:33] epoch: 1204 train-loss: 0.0003210774366380065\n",
      "[LOG 20190630-20:47:33] epoch: 1205 train-loss: 0.00032110210213431856\n",
      "[LOG 20190630-20:47:34] epoch: 1206 train-loss: 0.0003211262746845023\n",
      "[LOG 20190630-20:47:34] epoch: 1207 train-loss: 0.00032114992995957437\n",
      "[LOG 20190630-20:47:35] epoch: 1208 train-loss: 0.0003211730729617557\n",
      "[LOG 20190630-20:47:35] epoch: 1209 train-loss: 0.00032119569141286775\n",
      "[LOG 20190630-20:47:36] epoch: 1210 train-loss: 0.00032121781146088324\n",
      "[LOG 20190630-20:47:36] epoch: 1211 train-loss: 0.00032123941491590813\n",
      "[LOG 20190630-20:47:37] epoch: 1212 train-loss: 0.00032126051792147337\n",
      "[LOG 20190630-20:47:37] epoch: 1213 train-loss: 0.00032128109069162747\n",
      "[LOG 20190630-20:47:38] epoch: 1214 train-loss: 0.00032130115255313285\n",
      "[LOG 20190630-20:47:38] epoch: 1215 train-loss: 0.00032132072556123603\n",
      "[LOG 20190630-20:47:39] epoch: 1216 train-loss: 0.00032133982494997326\n",
      "[LOG 20190630-20:47:39] epoch: 1217 train-loss: 0.0003213584520835866\n",
      "[LOG 20190630-20:47:40] epoch: 1218 train-loss: 0.000321376588772182\n",
      "[LOG 20190630-20:47:40] epoch: 1219 train-loss: 0.0003213942266029335\n",
      "[LOG 20190630-20:47:40] epoch: 1220 train-loss: 0.00032141137194230396\n",
      "[LOG 20190630-20:47:41] epoch: 1221 train-loss: 0.00032142802251655667\n",
      "[LOG 20190630-20:47:41] epoch: 1222 train-loss: 0.00032144418059942836\n",
      "[LOG 20190630-20:47:42] epoch: 1223 train-loss: 0.0003214598459635454\n",
      "[LOG 20190630-20:47:42] epoch: 1224 train-loss: 0.00032147499700840854\n",
      "[LOG 20190630-20:47:43] epoch: 1225 train-loss: 0.0003214896478311857\n",
      "[LOG 20190630-20:47:43] epoch: 1226 train-loss: 0.00032150380661732925\n",
      "[LOG 20190630-20:47:44] epoch: 1227 train-loss: 0.0003215174901924911\n",
      "[LOG 20190630-20:47:44] epoch: 1228 train-loss: 0.0003215306944639451\n",
      "[LOG 20190630-20:47:45] epoch: 1229 train-loss: 0.00032154341351997573\n",
      "[LOG 20190630-20:47:45] epoch: 1230 train-loss: 0.00032155566850633477\n",
      "[LOG 20190630-20:47:46] epoch: 1231 train-loss: 0.00032156743577616\n",
      "[LOG 20190630-20:47:46] epoch: 1232 train-loss: 0.0003215787314729823\n",
      "[LOG 20190630-20:47:46] epoch: 1233 train-loss: 0.00032158955309569137\n",
      "[LOG 20190630-20:47:47] epoch: 1234 train-loss: 0.00032159991496882867\n",
      "[LOG 20190630-20:47:47] epoch: 1235 train-loss: 0.00032160980958906293\n",
      "[LOG 20190630-20:47:48] epoch: 1236 train-loss: 0.000321619241276494\n",
      "[LOG 20190630-20:47:48] epoch: 1237 train-loss: 0.00032162819979930646\n",
      "[LOG 20190630-20:47:49] epoch: 1238 train-loss: 0.00032163669425244734\n",
      "[LOG 20190630-20:47:49] epoch: 1239 train-loss: 0.00032164472850126913\n",
      "[LOG 20190630-20:47:50] epoch: 1240 train-loss: 0.00032165229868041934\n",
      "[LOG 20190630-20:47:50] epoch: 1241 train-loss: 0.00032165940933737147\n",
      "[LOG 20190630-20:47:51] epoch: 1242 train-loss: 0.00032166607070394093\n",
      "[LOG 20190630-20:47:51] epoch: 1243 train-loss: 0.0003216722809611383\n",
      "[LOG 20190630-20:47:52] epoch: 1244 train-loss: 0.00032167805829885765\n",
      "[LOG 20190630-20:47:52] epoch: 1245 train-loss: 0.00032168340726457245\n",
      "[LOG 20190630-20:47:53] epoch: 1246 train-loss: 0.00032168832126444613\n",
      "[LOG 20190630-20:47:53] epoch: 1247 train-loss: 0.0003216928314486722\n",
      "[LOG 20190630-20:47:53] epoch: 1248 train-loss: 0.0003216969373625034\n",
      "[LOG 20190630-20:47:54] epoch: 1249 train-loss: 0.00032170063104786095\n",
      "[LOG 20190630-20:47:54] epoch: 1250 train-loss: 0.0003217039147784817\n",
      "[LOG 20190630-20:47:55] epoch: 1251 train-loss: 0.00032170680697163334\n",
      "[LOG 20190630-20:47:55] epoch: 1252 train-loss: 0.0003217093121747894\n",
      "[LOG 20190630-20:47:56] epoch: 1253 train-loss: 0.0003217114749531902\n",
      "[LOG 20190630-20:47:56] epoch: 1254 train-loss: 0.0003217132516510901\n",
      "[LOG 20190630-20:47:57] epoch: 1255 train-loss: 0.0003217146361293999\n",
      "[LOG 20190630-20:47:57] epoch: 1256 train-loss: 0.0003217156397568033\n",
      "[LOG 20190630-20:47:58] epoch: 1257 train-loss: 0.00032171628822652565\n",
      "[LOG 20190630-20:47:58] epoch: 1258 train-loss: 0.0003217165831301827\n",
      "[LOG 20190630-20:47:59] epoch: 1259 train-loss: 0.0003217165162823221\n",
      "[LOG 20190630-20:47:59] epoch: 1260 train-loss: 0.0003217161042812222\n",
      "[LOG 20190630-20:47:59] epoch: 1261 train-loss: 0.0003217153143850737\n",
      "[LOG 20190630-20:48:00] epoch: 1262 train-loss: 0.0003217142111680005\n",
      "[LOG 20190630-20:48:00] epoch: 1263 train-loss: 0.0003217127618881932\n",
      "[LOG 20190630-20:48:01] epoch: 1264 train-loss: 0.00032171100406230835\n",
      "[LOG 20190630-20:48:01] epoch: 1265 train-loss: 0.0003217089367808512\n",
      "[LOG 20190630-20:48:02] epoch: 1266 train-loss: 0.0003217065325316071\n",
      "[LOG 20190630-20:48:02] epoch: 1267 train-loss: 0.0003217038056391175\n",
      "[LOG 20190630-20:48:03] epoch: 1268 train-loss: 0.0003217007692910556\n",
      "[LOG 20190630-20:48:03] epoch: 1269 train-loss: 0.00032169741734833224\n",
      "[LOG 20190630-20:48:04] epoch: 1270 train-loss: 0.0003216937502656947\n",
      "[LOG 20190630-20:48:04] epoch: 1271 train-loss: 0.00032168978850677377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:48:04] epoch: 1272 train-loss: 0.0003216855159280385\n",
      "[LOG 20190630-20:48:05] epoch: 1273 train-loss: 0.0003216809686819033\n",
      "[LOG 20190630-20:48:05] epoch: 1274 train-loss: 0.00032167615108846803\n",
      "[LOG 20190630-20:48:06] epoch: 1275 train-loss: 0.00032167103995561774\n",
      "[LOG 20190630-20:48:06] epoch: 1276 train-loss: 0.0003216656607492041\n",
      "[LOG 20190630-20:48:07] epoch: 1277 train-loss: 0.00032166000301003805\n",
      "[LOG 20190630-20:48:08] epoch: 1278 train-loss: 0.0003216540728772088\n",
      "[LOG 20190630-20:48:08] epoch: 1279 train-loss: 0.00032164789581656805\n",
      "[LOG 20190630-20:48:09] epoch: 1280 train-loss: 0.00032164145409296907\n",
      "[LOG 20190630-20:48:09] epoch: 1281 train-loss: 0.0003216347622583271\n",
      "[LOG 20190630-20:48:10] epoch: 1282 train-loss: 0.0003216278048512322\n",
      "[LOG 20190630-20:48:10] epoch: 1283 train-loss: 0.0003216205830085528\n",
      "[LOG 20190630-20:48:11] epoch: 1284 train-loss: 0.0003216131356111873\n",
      "[LOG 20190630-20:48:11] epoch: 1285 train-loss: 0.00032160543355530535\n",
      "[LOG 20190630-20:48:12] epoch: 1286 train-loss: 0.0003215974852537329\n",
      "[LOG 20190630-20:48:12] epoch: 1287 train-loss: 0.0003215893066226272\n",
      "[LOG 20190630-20:48:13] epoch: 1288 train-loss: 0.0003215808917502727\n",
      "[LOG 20190630-20:48:13] epoch: 1289 train-loss: 0.00032157223608919594\n",
      "[LOG 20190630-20:48:14] epoch: 1290 train-loss: 0.0003215633366835391\n",
      "[LOG 20190630-20:48:14] epoch: 1291 train-loss: 0.0003215542005818861\n",
      "[LOG 20190630-20:48:14] epoch: 1292 train-loss: 0.00032154484460988897\n",
      "[LOG 20190630-20:48:15] epoch: 1293 train-loss: 0.00032153525899047963\n",
      "[LOG 20190630-20:48:15] epoch: 1294 train-loss: 0.0003215254528186051\n",
      "[LOG 20190630-20:48:16] epoch: 1295 train-loss: 0.00032151544064618065\n",
      "[LOG 20190630-20:48:16] epoch: 1296 train-loss: 0.00032150522793017444\n",
      "[LOG 20190630-20:48:17] epoch: 1297 train-loss: 0.0003214948164895759\n",
      "[LOG 20190630-20:48:17] epoch: 1298 train-loss: 0.00032148419018085406\n",
      "[LOG 20190630-20:48:18] epoch: 1299 train-loss: 0.0003214733603726927\n",
      "[LOG 20190630-20:48:18] epoch: 1300 train-loss: 0.00032146232410923403\n",
      "[LOG 20190630-20:48:19] epoch: 1301 train-loss: 0.00032145109366865654\n",
      "[LOG 20190630-20:48:19] epoch: 1302 train-loss: 0.00032143965222530824\n",
      "[LOG 20190630-20:48:20] epoch: 1303 train-loss: 0.00032142800955625717\n",
      "[LOG 20190630-20:48:20] epoch: 1304 train-loss: 0.0003214161688447348\n",
      "[LOG 20190630-20:48:21] epoch: 1305 train-loss: 0.00032140414714376675\n",
      "[LOG 20190630-20:48:21] epoch: 1306 train-loss: 0.0003213919637801155\n",
      "[LOG 20190630-20:48:21] epoch: 1307 train-loss: 0.00032137961898115464\n",
      "[LOG 20190630-20:48:22] epoch: 1308 train-loss: 0.0003213671020603215\n",
      "[LOG 20190630-20:48:22] epoch: 1309 train-loss: 0.0003213544102891319\n",
      "[LOG 20190630-20:48:23] epoch: 1310 train-loss: 0.0003213415448044543\n",
      "[LOG 20190630-20:48:23] epoch: 1311 train-loss: 0.000321328522204567\n",
      "[LOG 20190630-20:48:24] epoch: 1312 train-loss: 0.0003213153572687588\n",
      "[LOG 20190630-20:48:24] epoch: 1313 train-loss: 0.0003213020502244035\n",
      "[LOG 20190630-20:48:25] epoch: 1314 train-loss: 0.00032128859925251163\n",
      "[LOG 20190630-20:48:25] epoch: 1315 train-loss: 0.00032127500389833585\n",
      "[LOG 20190630-20:48:26] epoch: 1316 train-loss: 0.00032126126416187617\n",
      "[LOG 20190630-20:48:26] epoch: 1317 train-loss: 0.0003212473936855531\n",
      "[LOG 20190630-20:48:27] epoch: 1318 train-loss: 0.00032123335199685243\n",
      "[LOG 20190630-20:48:27] epoch: 1319 train-loss: 0.0003212191768398043\n",
      "[LOG 20190630-20:48:27] epoch: 1320 train-loss: 0.0003212048843579396\n",
      "[LOG 20190630-20:48:28] epoch: 1321 train-loss: 0.0003211904677300481\n",
      "[LOG 20190630-20:48:28] epoch: 1322 train-loss: 0.0003211759385521873\n",
      "[LOG 20190630-20:48:29] epoch: 1323 train-loss: 0.0003211612820450682\n",
      "[LOG 20190630-20:48:29] epoch: 1324 train-loss: 0.00032114651730807964\n",
      "[LOG 20190630-20:48:30] epoch: 1325 train-loss: 0.00032113166366798396\n",
      "[LOG 20190630-20:48:30] epoch: 1326 train-loss: 0.00032111670770973433\n",
      "[LOG 20190630-20:48:31] epoch: 1327 train-loss: 0.0003211016598925198\n",
      "[LOG 20190630-20:48:31] epoch: 1328 train-loss: 0.0003210865197615931\n",
      "[LOG 20190630-20:48:32] epoch: 1329 train-loss: 0.0003210712911823066\n",
      "[LOG 20190630-20:48:32] epoch: 1330 train-loss: 0.00032105598279486003\n",
      "[LOG 20190630-20:48:33] epoch: 1331 train-loss: 0.0003210405213849299\n",
      "[LOG 20190630-20:48:33] epoch: 1332 train-loss: 0.0003210249744824978\n",
      "[LOG 20190630-20:48:34] epoch: 1333 train-loss: 0.000321009367098668\n",
      "[LOG 20190630-20:48:34] epoch: 1334 train-loss: 0.00032099369195748295\n",
      "[LOG 20190630-20:48:34] epoch: 1335 train-loss: 0.0003209779490589426\n",
      "[LOG 20190630-20:48:35] epoch: 1336 train-loss: 0.0003209621440873889\n",
      "[LOG 20190630-20:48:35] epoch: 1337 train-loss: 0.00032094627317746927\n",
      "[LOG 20190630-20:48:36] epoch: 1338 train-loss: 0.0003209303481526149\n",
      "[LOG 20190630-20:48:36] epoch: 1339 train-loss: 0.0003209143808362569\n",
      "[LOG 20190630-20:48:37] epoch: 1340 train-loss: 0.000320898373502132\n",
      "[LOG 20190630-20:48:37] epoch: 1341 train-loss: 0.0003208823320619558\n",
      "[LOG 20190630-20:48:38] epoch: 1342 train-loss: 0.0003208661992175621\n",
      "[LOG 20190630-20:48:38] epoch: 1343 train-loss: 0.0003208499913398555\n",
      "[LOG 20190630-20:48:39] epoch: 1344 train-loss: 0.0003208337627711444\n",
      "[LOG 20190630-20:48:39] epoch: 1345 train-loss: 0.00032081751510304457\n",
      "[LOG 20190630-20:48:40] epoch: 1346 train-loss: 0.00032080124879030336\n",
      "[LOG 20190630-20:48:40] epoch: 1347 train-loss: 0.0003207849720183731\n",
      "[LOG 20190630-20:48:40] epoch: 1348 train-loss: 0.0003207686932000797\n",
      "[LOG 20190630-20:48:41] epoch: 1349 train-loss: 0.0003207524184745125\n",
      "[LOG 20190630-20:48:41] epoch: 1350 train-loss: 0.00032073616944217065\n",
      "[LOG 20190630-20:48:42] epoch: 1351 train-loss: 0.0003207199461030541\n",
      "[LOG 20190630-20:48:42] epoch: 1352 train-loss: 0.0003207037416359526\n",
      "[LOG 20190630-20:48:43] epoch: 1353 train-loss: 0.0003206874689567485\n",
      "[LOG 20190630-20:48:43] epoch: 1354 train-loss: 0.0003206711903658288\n",
      "[LOG 20190630-20:48:44] epoch: 1355 train-loss: 0.00032065493655863975\n",
      "[LOG 20190630-20:48:44] epoch: 1356 train-loss: 0.0003206387093541707\n",
      "[LOG 20190630-20:48:45] epoch: 1357 train-loss: 0.00032062252125797386\n",
      "[LOG 20190630-20:48:45] epoch: 1358 train-loss: 0.00032060635976449703\n",
      "[LOG 20190630-20:48:46] epoch: 1359 train-loss: 0.00032059024033515016\n",
      "[LOG 20190630-20:48:46] epoch: 1360 train-loss: 0.0003205741513738758\n",
      "[LOG 20190630-20:48:47] epoch: 1361 train-loss: 0.00032055810220299463\n",
      "[LOG 20190630-20:48:47] epoch: 1362 train-loss: 0.00032054208850240684\n",
      "[LOG 20190630-20:48:47] epoch: 1363 train-loss: 0.0003205260381946573\n",
      "[LOG 20190630-20:48:48] epoch: 1364 train-loss: 0.00032051002608568524\n",
      "[LOG 20190630-20:48:48] epoch: 1365 train-loss: 0.00032049406604528485\n",
      "[LOG 20190630-20:48:49] epoch: 1366 train-loss: 0.000320478171488503\n",
      "[LOG 20190630-20:48:49] epoch: 1367 train-loss: 0.00032046231308413553\n",
      "[LOG 20190630-20:48:50] epoch: 1368 train-loss: 0.00032044651220530795\n",
      "[LOG 20190630-20:48:50] epoch: 1369 train-loss: 0.0003204307640771731\n",
      "[LOG 20190630-20:48:51] epoch: 1370 train-loss: 0.0003204150825695251\n",
      "[LOG 20190630-20:48:51] epoch: 1371 train-loss: 0.0003203994608611538\n",
      "[LOG 20190630-20:48:52] epoch: 1372 train-loss: 0.00032038381618804124\n",
      "[LOG 20190630-20:48:52] epoch: 1373 train-loss: 0.00032036822449299507\n",
      "[LOG 20190630-20:48:53] epoch: 1374 train-loss: 0.00032035268327490485\n",
      "[LOG 20190630-20:48:53] epoch: 1375 train-loss: 0.0003203372127700277\n",
      "[LOG 20190630-20:48:53] epoch: 1376 train-loss: 0.00032032180683927436\n",
      "[LOG 20190630-20:48:54] epoch: 1377 train-loss: 0.00032030647480496555\n",
      "[LOG 20190630-20:48:54] epoch: 1378 train-loss: 0.0003202912171218486\n",
      "[LOG 20190630-20:48:55] epoch: 1379 train-loss: 0.0003202759805844835\n",
      "[LOG 20190630-20:48:55] epoch: 1380 train-loss: 0.0003202607956609427\n",
      "[LOG 20190630-20:48:56] epoch: 1381 train-loss: 0.000320245708735456\n",
      "[LOG 20190630-20:48:56] epoch: 1382 train-loss: 0.0003202307116225711\n",
      "[LOG 20190630-20:48:57] epoch: 1383 train-loss: 0.00032021574588725343\n",
      "[LOG 20190630-20:48:57] epoch: 1384 train-loss: 0.00032020084040595975\n",
      "[LOG 20190630-20:48:58] epoch: 1385 train-loss: 0.000320186019052926\n",
      "[LOG 20190630-20:48:58] epoch: 1386 train-loss: 0.00032017128819461504\n",
      "[LOG 20190630-20:48:59] epoch: 1387 train-loss: 0.0003201566037205339\n",
      "[LOG 20190630-20:48:59] epoch: 1388 train-loss: 0.00032014200382946\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:48:59] epoch: 1389 train-loss: 0.00032012751012189256\n",
      "[LOG 20190630-20:49:00] epoch: 1390 train-loss: 0.00032011311327551084\n",
      "[LOG 20190630-20:49:00] epoch: 1391 train-loss: 0.0003200987664513377\n",
      "[LOG 20190630-20:49:01] epoch: 1392 train-loss: 0.00032008450239118247\n",
      "[LOG 20190630-20:49:01] epoch: 1393 train-loss: 0.00032007034451453364\n",
      "[LOG 20190630-20:49:02] epoch: 1394 train-loss: 0.0003200562814527075\n",
      "[LOG 20190630-20:49:02] epoch: 1395 train-loss: 0.0003200422743248055\n",
      "[LOG 20190630-20:49:03] epoch: 1396 train-loss: 0.00032002836292122083\n",
      "[LOG 20190630-20:49:03] epoch: 1397 train-loss: 0.0003200145506525587\n",
      "[LOG 20190630-20:49:04] epoch: 1398 train-loss: 0.0003200008422936662\n",
      "[LOG 20190630-20:49:04] epoch: 1399 train-loss: 0.00031998719850889756\n",
      "[LOG 20190630-20:49:05] epoch: 1400 train-loss: 0.00031997363953450986\n",
      "[LOG 20190630-20:49:05] epoch: 1401 train-loss: 0.0003199601840151445\n",
      "[LOG 20190630-20:49:06] epoch: 1402 train-loss: 0.0003199468333150435\n",
      "[LOG 20190630-20:49:06] epoch: 1403 train-loss: 0.0003199335517365398\n",
      "[LOG 20190630-20:49:06] epoch: 1404 train-loss: 0.00031992036383599043\n",
      "[LOG 20190630-20:49:07] epoch: 1405 train-loss: 0.0003199073005362152\n",
      "[LOG 20190630-20:49:07] epoch: 1406 train-loss: 0.0003198943561528722\n",
      "[LOG 20190630-20:49:08] epoch: 1407 train-loss: 0.00031988147907213715\n",
      "[LOG 20190630-20:49:08] epoch: 1408 train-loss: 0.000319868729548034\n",
      "[LOG 20190630-20:49:09] epoch: 1409 train-loss: 0.00031985610917217855\n",
      "[LOG 20190630-20:49:09] epoch: 1410 train-loss: 0.0003198435615558992\n",
      "[LOG 20190630-20:49:10] epoch: 1411 train-loss: 0.00031983114195099915\n",
      "[LOG 20190630-20:49:10] epoch: 1412 train-loss: 0.0003198188665010093\n",
      "[LOG 20190630-20:49:11] epoch: 1413 train-loss: 0.0003198067411176453\n",
      "[LOG 20190630-20:49:11] epoch: 1414 train-loss: 0.00031979470668375143\n",
      "[LOG 20190630-20:49:12] epoch: 1415 train-loss: 0.00031978281390365737\n",
      "[LOG 20190630-20:49:12] epoch: 1416 train-loss: 0.0003197710570930212\n",
      "[LOG 20190630-20:49:12] epoch: 1417 train-loss: 0.00031975941260498075\n",
      "[LOG 20190630-20:49:13] epoch: 1418 train-loss: 0.00031974784815247403\n",
      "[LOG 20190630-20:49:13] epoch: 1419 train-loss: 0.0003197364253537671\n",
      "[LOG 20190630-20:49:14] epoch: 1420 train-loss: 0.0003197251571691595\n",
      "[LOG 20190630-20:49:14] epoch: 1421 train-loss: 0.0003197139667463489\n",
      "[LOG 20190630-20:49:15] epoch: 1422 train-loss: 0.00031970292639016407\n",
      "[LOG 20190630-20:49:15] epoch: 1423 train-loss: 0.00031969203132575785\n",
      "[LOG 20190630-20:49:16] epoch: 1424 train-loss: 0.0003196812479018263\n",
      "[LOG 20190630-20:49:16] epoch: 1425 train-loss: 0.00031967060135684733\n",
      "[LOG 20190630-20:49:17] epoch: 1426 train-loss: 0.00031966010487849417\n",
      "[LOG 20190630-20:49:17] epoch: 1427 train-loss: 0.0003196497436874779\n",
      "[LOG 20190630-20:49:18] epoch: 1428 train-loss: 0.00031963946639734786\n",
      "[LOG 20190630-20:49:18] epoch: 1429 train-loss: 0.00031962933780960157\n",
      "[LOG 20190630-20:49:18] epoch: 1430 train-loss: 0.0003196193624717125\n",
      "[LOG 20190630-20:49:19] epoch: 1431 train-loss: 0.00031960950423126633\n",
      "[LOG 20190630-20:49:19] epoch: 1432 train-loss: 0.0003195998128830979\n",
      "[LOG 20190630-20:49:20] epoch: 1433 train-loss: 0.0003195902695551922\n",
      "[LOG 20190630-20:49:20] epoch: 1434 train-loss: 0.0003195808474174555\n",
      "[LOG 20190630-20:49:21] epoch: 1435 train-loss: 0.0003195715598849347\n",
      "[LOG 20190630-20:49:21] epoch: 1436 train-loss: 0.0003195624560703436\n",
      "[LOG 20190630-20:49:22] epoch: 1437 train-loss: 0.0003195535146005568\n",
      "[LOG 20190630-20:49:22] epoch: 1438 train-loss: 0.0003195446970494231\n",
      "[LOG 20190630-20:49:23] epoch: 1439 train-loss: 0.0003195360659447033\n",
      "[LOG 20190630-20:49:23] epoch: 1440 train-loss: 0.00031952758240549883\n",
      "[LOG 20190630-20:49:24] epoch: 1441 train-loss: 0.00031951921482686885\n",
      "[LOG 20190630-20:49:24] epoch: 1442 train-loss: 0.000319511006182438\n",
      "[LOG 20190630-20:49:25] epoch: 1443 train-loss: 0.0003195029830749263\n",
      "[LOG 20190630-20:49:25] epoch: 1444 train-loss: 0.000319495081612331\n",
      "[LOG 20190630-20:49:26] epoch: 1445 train-loss: 0.0003194873538632237\n",
      "[LOG 20190630-20:49:27] epoch: 1446 train-loss: 0.0003194798084678041\n",
      "[LOG 20190630-20:49:27] epoch: 1447 train-loss: 0.0003194723801698274\n",
      "[LOG 20190630-20:49:27] epoch: 1448 train-loss: 0.00031946513422553835\n",
      "[LOG 20190630-20:49:28] epoch: 1449 train-loss: 0.0003194580849594786\n",
      "[LOG 20190630-20:49:28] epoch: 1450 train-loss: 0.0003194511170931946\n",
      "[LOG 20190630-20:49:29] epoch: 1451 train-loss: 0.00031944432248565136\n",
      "[LOG 20190630-20:49:29] epoch: 1452 train-loss: 0.00031943771728037973\n",
      "[LOG 20190630-20:49:30] epoch: 1453 train-loss: 0.00031943123758537695\n",
      "[LOG 20190630-20:49:30] epoch: 1454 train-loss: 0.0003194249716216291\n",
      "[LOG 20190630-20:49:31] epoch: 1455 train-loss: 0.0003194189141595416\n",
      "[LOG 20190630-20:49:31] epoch: 1456 train-loss: 0.0003194129762960074\n",
      "[LOG 20190630-20:49:32] epoch: 1457 train-loss: 0.00031940723147272365\n",
      "[LOG 20190630-20:49:32] epoch: 1458 train-loss: 0.00031940168332766916\n",
      "[LOG 20190630-20:49:33] epoch: 1459 train-loss: 0.000319396230224811\n",
      "[LOG 20190630-20:49:33] epoch: 1460 train-loss: 0.00031939099267219717\n",
      "[LOG 20190630-20:49:34] epoch: 1461 train-loss: 0.00031938594747771276\n",
      "[LOG 20190630-20:49:34] epoch: 1462 train-loss: 0.00031938102642925514\n",
      "[LOG 20190630-20:49:35] epoch: 1463 train-loss: 0.0003193763084254897\n",
      "[LOG 20190630-20:49:35] epoch: 1464 train-loss: 0.0003193716972873517\n",
      "[LOG 20190630-20:49:35] epoch: 1465 train-loss: 0.0003193672371253342\n",
      "[LOG 20190630-20:49:36] epoch: 1466 train-loss: 0.00031936300251800276\n",
      "[LOG 20190630-20:49:36] epoch: 1467 train-loss: 0.00031935890820022905\n",
      "[LOG 20190630-20:49:37] epoch: 1468 train-loss: 0.0003193550041942217\n",
      "[LOG 20190630-20:49:37] epoch: 1469 train-loss: 0.00031935130914462206\n",
      "[LOG 20190630-20:49:38] epoch: 1470 train-loss: 0.00031934773164721264\n",
      "[LOG 20190630-20:49:38] epoch: 1471 train-loss: 0.00031934436515257403\n",
      "[LOG 20190630-20:49:39] epoch: 1472 train-loss: 0.000319341203521617\n",
      "[LOG 20190630-20:49:39] epoch: 1473 train-loss: 0.00031933818377183343\n",
      "[LOG 20190630-20:49:40] epoch: 1474 train-loss: 0.00031933538343764667\n",
      "[LOG 20190630-20:49:40] epoch: 1475 train-loss: 0.0003193327186181705\n",
      "[LOG 20190630-20:49:41] epoch: 1476 train-loss: 0.00031933020409269375\n",
      "[LOG 20190630-20:49:41] epoch: 1477 train-loss: 0.0003193278980688774\n",
      "[LOG 20190630-20:49:41] epoch: 1478 train-loss: 0.0003193257307430031\n",
      "[LOG 20190630-20:49:42] epoch: 1479 train-loss: 0.0003193237637333368\n",
      "[LOG 20190630-20:49:42] epoch: 1480 train-loss: 0.0003193219879449316\n",
      "[LOG 20190630-20:49:43] epoch: 1481 train-loss: 0.0003193203740465833\n",
      "[LOG 20190630-20:49:43] epoch: 1482 train-loss: 0.0003193190166257409\n",
      "[LOG 20190630-20:49:44] epoch: 1483 train-loss: 0.0003193178240508132\n",
      "[LOG 20190630-20:49:44] epoch: 1484 train-loss: 0.0003193168411144143\n",
      "[LOG 20190630-20:49:45] epoch: 1485 train-loss: 0.00031931605235513416\n",
      "[LOG 20190630-20:49:45] epoch: 1486 train-loss: 0.00031931538273966\n",
      "[LOG 20190630-20:49:46] epoch: 1487 train-loss: 0.00031931492571857234\n",
      "[LOG 20190630-20:49:46] epoch: 1488 train-loss: 0.00031931462740431016\n",
      "[LOG 20190630-20:49:47] epoch: 1489 train-loss: 0.00031931454418554495\n",
      "[LOG 20190630-20:49:47] epoch: 1490 train-loss: 0.0003193146733337926\n",
      "[LOG 20190630-20:49:47] epoch: 1491 train-loss: 0.0003193149445905874\n",
      "[LOG 20190630-20:49:48] epoch: 1492 train-loss: 0.0003193154336713633\n",
      "[LOG 20190630-20:49:48] epoch: 1493 train-loss: 0.0003193160466707923\n",
      "[LOG 20190630-20:49:49] epoch: 1494 train-loss: 0.00031931685703057155\n",
      "[LOG 20190630-20:49:49] epoch: 1495 train-loss: 0.0003193178592937329\n",
      "[LOG 20190630-20:49:50] epoch: 1496 train-loss: 0.0003193189982084732\n",
      "[LOG 20190630-20:49:50] epoch: 1497 train-loss: 0.000319320382686783\n",
      "[LOG 20190630-20:49:51] epoch: 1498 train-loss: 0.0003193219465629227\n",
      "[LOG 20190630-20:49:51] epoch: 1499 train-loss: 0.0003193237396317272\n",
      "[LOG 20190630-20:49:52] epoch: 1500 train-loss: 0.00031932570209391997\n",
      "[LOG 20190630-20:49:52] epoch: 1501 train-loss: 0.00031932786350807874\n",
      "[LOG 20190630-20:49:53] epoch: 1502 train-loss: 0.00031933026252772834\n",
      "[LOG 20190630-20:49:53] epoch: 1503 train-loss: 0.00031933282684804\n",
      "[LOG 20190630-20:49:53] epoch: 1504 train-loss: 0.0003193356033079908\n",
      "[LOG 20190630-20:49:54] epoch: 1505 train-loss: 0.00031933852710608335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:49:54] epoch: 1506 train-loss: 0.00031934171147440793\n",
      "[LOG 20190630-20:49:55] epoch: 1507 train-loss: 0.00031934507956066227\n",
      "[LOG 20190630-20:49:55] epoch: 1508 train-loss: 0.00031934866592564504\n",
      "[LOG 20190630-20:49:56] epoch: 1509 train-loss: 0.0003193524366906786\n",
      "[LOG 20190630-20:49:56] epoch: 1510 train-loss: 0.00031935640845404123\n",
      "[LOG 20190630-20:49:57] epoch: 1511 train-loss: 0.00031936062032400514\n",
      "[LOG 20190630-20:49:57] epoch: 1512 train-loss: 0.0003193649918102892\n",
      "[LOG 20190630-20:49:58] epoch: 1513 train-loss: 0.0003193696009020641\n",
      "[LOG 20190630-20:49:58] epoch: 1514 train-loss: 0.00031937434209794446\n",
      "[LOG 20190630-20:49:59] epoch: 1515 train-loss: 0.00031937933044901\n",
      "[LOG 20190630-20:49:59] epoch: 1516 train-loss: 0.0003193845052464894\n",
      "[LOG 20190630-20:50:00] epoch: 1517 train-loss: 0.00031938992424329626\n",
      "[LOG 20190630-20:50:00] epoch: 1518 train-loss: 0.00031939551786308584\n",
      "[LOG 20190630-20:50:00] epoch: 1519 train-loss: 0.0003194013386291772\n",
      "[LOG 20190630-20:50:01] epoch: 1520 train-loss: 0.00031940735630087147\n",
      "[LOG 20190630-20:50:01] epoch: 1521 train-loss: 0.00031941362317411404\n",
      "[LOG 20190630-20:50:02] epoch: 1522 train-loss: 0.00031942003943186137\n",
      "[LOG 20190630-20:50:02] epoch: 1523 train-loss: 0.0003194266739683371\n",
      "[LOG 20190630-20:50:03] epoch: 1524 train-loss: 0.0003194335242824309\n",
      "[LOG 20190630-20:50:03] epoch: 1525 train-loss: 0.00031944058082444826\n",
      "[LOG 20190630-20:50:04] epoch: 1526 train-loss: 0.00031944782426762686\n",
      "[LOG 20190630-20:50:04] epoch: 1527 train-loss: 0.00031945527052812395\n",
      "[LOG 20190630-20:50:05] epoch: 1528 train-loss: 0.00031946290914675046\n",
      "[LOG 20190630-20:50:05] epoch: 1529 train-loss: 0.0003194707819602627\n",
      "[LOG 20190630-20:50:06] epoch: 1530 train-loss: 0.0003194788660039194\n",
      "[LOG 20190630-20:50:06] epoch: 1531 train-loss: 0.0003194871549112577\n",
      "[LOG 20190630-20:50:07] epoch: 1532 train-loss: 0.00031949564277056197\n",
      "[LOG 20190630-20:50:07] epoch: 1533 train-loss: 0.00031950434913596837\n",
      "[LOG 20190630-20:50:08] epoch: 1534 train-loss: 0.00031951326127455104\n",
      "[LOG 20190630-20:50:08] epoch: 1535 train-loss: 0.00031952236213328433\n",
      "[LOG 20190630-20:50:09] epoch: 1536 train-loss: 0.00031953161624187487\n",
      "[LOG 20190630-20:50:09] epoch: 1537 train-loss: 0.0003195411131855508\n",
      "[LOG 20190630-20:50:10] epoch: 1538 train-loss: 0.00031955078907230927\n",
      "[LOG 20190630-20:50:10] epoch: 1539 train-loss: 0.0003195606921053695\n",
      "[LOG 20190630-20:50:11] epoch: 1540 train-loss: 0.00031957071678334614\n",
      "[LOG 20190630-20:50:12] epoch: 1541 train-loss: 0.00031958090175976395\n",
      "[LOG 20190630-20:50:12] epoch: 1542 train-loss: 0.0003195912327100814\n",
      "[LOG 20190630-20:50:13] epoch: 1543 train-loss: 0.0003196017992195266\n",
      "[LOG 20190630-20:50:13] epoch: 1544 train-loss: 0.0003196125462636701\n",
      "[LOG 20190630-20:50:14] epoch: 1545 train-loss: 0.00031962349362402165\n",
      "[LOG 20190630-20:50:14] epoch: 1546 train-loss: 0.00031963464721229684\n",
      "[LOG 20190630-20:50:15] epoch: 1547 train-loss: 0.0003196459817900177\n",
      "[LOG 20190630-20:50:15] epoch: 1548 train-loss: 0.000319657517820815\n",
      "[LOG 20190630-20:50:16] epoch: 1549 train-loss: 0.0003196692055098538\n",
      "[LOG 20190630-20:50:16] epoch: 1550 train-loss: 0.0003196811424004409\n",
      "[LOG 20190630-20:50:17] epoch: 1551 train-loss: 0.0003196932650553208\n",
      "[LOG 20190630-20:50:17] epoch: 1552 train-loss: 0.00031970559052751923\n",
      "[LOG 20190630-20:50:18] epoch: 1553 train-loss: 0.0003197181176801678\n",
      "[LOG 20190630-20:50:18] epoch: 1554 train-loss: 0.0003197308301423618\n",
      "[LOG 20190630-20:50:18] epoch: 1555 train-loss: 0.0003197437986273144\n",
      "[LOG 20190630-20:50:19] epoch: 1556 train-loss: 0.00031975694423636014\n",
      "[LOG 20190630-20:50:19] epoch: 1557 train-loss: 0.0003197702960733295\n",
      "[LOG 20190630-20:50:20] epoch: 1558 train-loss: 0.0003197838443611545\n",
      "[LOG 20190630-20:50:20] epoch: 1559 train-loss: 0.00031979755544853106\n",
      "[LOG 20190630-20:50:21] epoch: 1560 train-loss: 0.0003198114854967571\n",
      "[LOG 20190630-20:50:21] epoch: 1561 train-loss: 0.0003198255867573607\n",
      "[LOG 20190630-20:50:22] epoch: 1562 train-loss: 0.0003198398960648774\n",
      "[LOG 20190630-20:50:22] epoch: 1563 train-loss: 0.00031985442774384865\n",
      "[LOG 20190630-20:50:23] epoch: 1564 train-loss: 0.00031986908220460464\n",
      "[LOG 20190630-20:50:23] epoch: 1565 train-loss: 0.0003198838935531967\n",
      "[LOG 20190630-20:50:24] epoch: 1566 train-loss: 0.0003198989331849589\n",
      "[LOG 20190630-20:50:24] epoch: 1567 train-loss: 0.0003199141372078884\n",
      "[LOG 20190630-20:50:24] epoch: 1568 train-loss: 0.00031992957997317717\n",
      "[LOG 20190630-20:50:25] epoch: 1569 train-loss: 0.00031994517621569685\n",
      "[LOG 20190630-20:50:25] epoch: 1570 train-loss: 0.0003199609586772567\n",
      "[LOG 20190630-20:50:26] epoch: 1571 train-loss: 0.0003199769216735149\n",
      "[LOG 20190630-20:50:26] epoch: 1572 train-loss: 0.00031999308839658624\n",
      "[LOG 20190630-20:50:27] epoch: 1573 train-loss: 0.0003200094413386978\n",
      "[LOG 20190630-20:50:27] epoch: 1574 train-loss: 0.0003200260057383275\n",
      "[LOG 20190630-20:50:28] epoch: 1575 train-loss: 0.00032004278364183847\n",
      "[LOG 20190630-20:50:28] epoch: 1576 train-loss: 0.00032005975117499474\n",
      "[LOG 20190630-20:50:29] epoch: 1577 train-loss: 0.0003200768974238599\n",
      "[LOG 20190630-20:50:29] epoch: 1578 train-loss: 0.00032009423080126\n",
      "[LOG 20190630-20:50:30] epoch: 1579 train-loss: 0.00032011172243073815\n",
      "[LOG 20190630-20:50:30] epoch: 1580 train-loss: 0.0003201293466190691\n",
      "[LOG 20190630-20:50:31] epoch: 1581 train-loss: 0.00032014712564887304\n",
      "[LOG 20190630-20:50:31] epoch: 1582 train-loss: 0.0003201650943083223\n",
      "[LOG 20190630-20:50:31] epoch: 1583 train-loss: 0.0003201832159902551\n",
      "[LOG 20190630-20:50:32] epoch: 1584 train-loss: 0.00032020149797062913\n",
      "[LOG 20190630-20:50:32] epoch: 1585 train-loss: 0.00032021997913034284\n",
      "[LOG 20190630-20:50:33] epoch: 1586 train-loss: 0.00032023862240748713\n",
      "[LOG 20190630-20:50:33] epoch: 1587 train-loss: 0.0003202574048373208\n",
      "[LOG 20190630-20:50:34] epoch: 1588 train-loss: 0.0003202762470664311\n",
      "[LOG 20190630-20:50:34] epoch: 1589 train-loss: 0.00032029519820753194\n",
      "[LOG 20190630-20:50:35] epoch: 1590 train-loss: 0.0003203143082828319\n",
      "[LOG 20190630-20:50:35] epoch: 1591 train-loss: 0.0003203335875241464\n",
      "[LOG 20190630-20:50:36] epoch: 1592 train-loss: 0.00032035293224907946\n",
      "[LOG 20190630-20:50:36] epoch: 1593 train-loss: 0.0003203723872502451\n",
      "[LOG 20190630-20:50:37] epoch: 1594 train-loss: 0.00032039195684774313\n",
      "[LOG 20190630-20:50:37] epoch: 1595 train-loss: 0.0003204116735560092\n",
      "[LOG 20190630-20:50:38] epoch: 1596 train-loss: 0.00032043150054050784\n",
      "[LOG 20190630-20:50:38] epoch: 1597 train-loss: 0.0003204514605386066\n",
      "[LOG 20190630-20:50:38] epoch: 1598 train-loss: 0.0003204715521860635\n",
      "[LOG 20190630-20:50:39] epoch: 1599 train-loss: 0.000320491798220246\n",
      "[LOG 20190630-20:50:39] epoch: 1600 train-loss: 0.000320512178404897\n",
      "[LOG 20190630-20:50:40] epoch: 1601 train-loss: 0.0003205326993338531\n",
      "[LOG 20190630-20:50:40] epoch: 1602 train-loss: 0.0003205533375876257\n",
      "[LOG 20190630-20:50:41] epoch: 1603 train-loss: 0.00032057408225227846\n",
      "[LOG 20190630-20:50:41] epoch: 1604 train-loss: 0.0003205949055882229\n",
      "[LOG 20190630-20:50:42] epoch: 1605 train-loss: 0.0003206158226021216\n",
      "[LOG 20190630-20:50:42] epoch: 1606 train-loss: 0.0003206368530754844\n",
      "[LOG 20190630-20:50:43] epoch: 1607 train-loss: 0.0003206580015557847\n",
      "[LOG 20190630-20:50:43] epoch: 1608 train-loss: 0.00032067925258161267\n",
      "[LOG 20190630-20:50:44] epoch: 1609 train-loss: 0.0003207006295724568\n",
      "[LOG 20190630-20:50:44] epoch: 1610 train-loss: 0.00032072211024569697\n",
      "[LOG 20190630-20:50:44] epoch: 1611 train-loss: 0.0003207436873253755\n",
      "[LOG 20190630-20:50:45] epoch: 1612 train-loss: 0.0003207653453500825\n",
      "[LOG 20190630-20:50:45] epoch: 1613 train-loss: 0.00032078706749416597\n",
      "[LOG 20190630-20:50:46] epoch: 1614 train-loss: 0.0003208088639894413\n",
      "[LOG 20190630-20:50:46] epoch: 1615 train-loss: 0.0003208307239219721\n",
      "[LOG 20190630-20:50:47] epoch: 1616 train-loss: 0.00032085265183923184\n",
      "[LOG 20190630-20:50:47] epoch: 1617 train-loss: 0.000320874677527172\n",
      "[LOG 20190630-20:50:48] epoch: 1618 train-loss: 0.0003208967605132784\n",
      "[LOG 20190630-20:50:48] epoch: 1619 train-loss: 0.00032091888692775683\n",
      "[LOG 20190630-20:50:49] epoch: 1620 train-loss: 0.00032094107973534847\n",
      "[LOG 20190630-20:50:49] epoch: 1621 train-loss: 0.0003209633186997962\n",
      "[LOG 20190630-20:50:50] epoch: 1622 train-loss: 0.0003209855731256539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:50:50] epoch: 1623 train-loss: 0.0003210078848496778\n",
      "[LOG 20190630-20:50:51] epoch: 1624 train-loss: 0.00032103024682328396\n",
      "[LOG 20190630-20:50:51] epoch: 1625 train-loss: 0.0003210526338079944\n",
      "[LOG 20190630-20:50:51] epoch: 1626 train-loss: 0.00032107503602674115\n",
      "[LOG 20190630-20:50:52] epoch: 1627 train-loss: 0.000321097432561146\n",
      "[LOG 20190630-20:50:52] epoch: 1628 train-loss: 0.0003211198700228124\n",
      "[LOG 20190630-20:50:53] epoch: 1629 train-loss: 0.00032114230975821556\n",
      "[LOG 20190630-20:50:53] epoch: 1630 train-loss: 0.0003211647656371497\n",
      "[LOG 20190630-20:50:54] epoch: 1631 train-loss: 0.0003211872447081987\n",
      "[LOG 20190630-20:50:54] epoch: 1632 train-loss: 0.0003212097092273325\n",
      "[LOG 20190630-20:50:55] epoch: 1633 train-loss: 0.00032123220171342837\n",
      "[LOG 20190630-20:50:55] epoch: 1634 train-loss: 0.00032125468669619295\n",
      "[LOG 20190630-20:50:56] epoch: 1635 train-loss: 0.0003212771703147155\n",
      "[LOG 20190630-20:50:56] epoch: 1636 train-loss: 0.0003212996057300188\n",
      "[LOG 20190630-20:50:57] epoch: 1637 train-loss: 0.0003213219747522089\n",
      "[LOG 20190630-20:50:57] epoch: 1638 train-loss: 0.0003213443096683477\n",
      "[LOG 20190630-20:50:57] epoch: 1639 train-loss: 0.0003213666104784352\n",
      "[LOG 20190630-20:50:58] epoch: 1640 train-loss: 0.00032138890219357563\n",
      "[LOG 20190630-20:50:58] epoch: 1641 train-loss: 0.000321411156846807\n",
      "[LOG 20190630-20:50:59] epoch: 1642 train-loss: 0.0003214334130916541\n",
      "[LOG 20190630-20:50:59] epoch: 1643 train-loss: 0.0003214556022612669\n",
      "[LOG 20190630-20:51:00] epoch: 1644 train-loss: 0.0003214777093489829\n",
      "[LOG 20190630-20:51:00] epoch: 1645 train-loss: 0.0003214997489067173\n",
      "[LOG 20190630-20:51:01] epoch: 1646 train-loss: 0.0003215217225260858\n",
      "[LOG 20190630-20:51:01] epoch: 1647 train-loss: 0.00032154359860214754\n",
      "[LOG 20190630-20:51:02] epoch: 1648 train-loss: 0.0003215654048744909\n",
      "[LOG 20190630-20:51:02] epoch: 1649 train-loss: 0.00032158709791474394\n",
      "[LOG 20190630-20:51:03] epoch: 1650 train-loss: 0.00032160870773623174\n",
      "[LOG 20190630-20:51:03] epoch: 1651 train-loss: 0.00032163021114683943\n",
      "[LOG 20190630-20:51:03] epoch: 1652 train-loss: 0.00032165161428565625\n",
      "[LOG 20190630-20:51:04] epoch: 1653 train-loss: 0.00032167291305995604\n",
      "[LOG 20190630-20:51:04] epoch: 1654 train-loss: 0.00032169406563298253\n",
      "[LOG 20190630-20:51:05] epoch: 1655 train-loss: 0.0003217150340333319\n",
      "[LOG 20190630-20:51:05] epoch: 1656 train-loss: 0.00032173585555028694\n",
      "[LOG 20190630-20:51:06] epoch: 1657 train-loss: 0.00032175653200283705\n",
      "[LOG 20190630-20:51:06] epoch: 1658 train-loss: 0.0003217770026822109\n",
      "[LOG 20190630-20:51:07] epoch: 1659 train-loss: 0.0003217972514448775\n",
      "[LOG 20190630-20:51:07] epoch: 1660 train-loss: 0.0003218173535515234\n",
      "[LOG 20190630-20:51:08] epoch: 1661 train-loss: 0.0003218373060462909\n",
      "[LOG 20190630-20:51:08] epoch: 1662 train-loss: 0.00032185710642806953\n",
      "[LOG 20190630-20:51:09] epoch: 1663 train-loss: 0.0003218767099042452\n",
      "[LOG 20190630-20:51:09] epoch: 1664 train-loss: 0.00032189612852562277\n",
      "[LOG 20190630-20:51:10] epoch: 1665 train-loss: 0.0003219153638838179\n",
      "[LOG 20190630-20:51:10] epoch: 1666 train-loss: 0.00032193439187722106\n",
      "[LOG 20190630-20:51:10] epoch: 1667 train-loss: 0.00032195318340200174\n",
      "[LOG 20190630-20:51:11] epoch: 1668 train-loss: 0.000321971713219682\n",
      "[LOG 20190630-20:51:11] epoch: 1669 train-loss: 0.00032198999952015583\n",
      "[LOG 20190630-20:51:12] epoch: 1670 train-loss: 0.000322008095736237\n",
      "[LOG 20190630-20:51:12] epoch: 1671 train-loss: 0.0003220259259251179\n",
      "[LOG 20190630-20:51:13] epoch: 1672 train-loss: 0.00032204350759457157\n",
      "[LOG 20190630-20:51:13] epoch: 1673 train-loss: 0.00032206085279540275\n",
      "[LOG 20190630-20:51:14] epoch: 1674 train-loss: 0.0003220779356070125\n",
      "[LOG 20190630-20:51:14] epoch: 1675 train-loss: 0.0003220947248792072\n",
      "[LOG 20190630-20:51:15] epoch: 1676 train-loss: 0.00032211121970249224\n",
      "[LOG 20190630-20:51:15] epoch: 1677 train-loss: 0.0003221274344014091\n",
      "[LOG 20190630-20:51:16] epoch: 1678 train-loss: 0.00032214328848567675\n",
      "[LOG 20190630-20:51:16] epoch: 1679 train-loss: 0.00032215883743447193\n",
      "[LOG 20190630-20:51:16] epoch: 1680 train-loss: 0.0003221740394110384\n",
      "[LOG 20190630-20:51:17] epoch: 1681 train-loss: 0.0003221888528059935\n",
      "[LOG 20190630-20:51:17] epoch: 1682 train-loss: 0.0003222033499241661\n",
      "[LOG 20190630-20:51:18] epoch: 1683 train-loss: 0.0003222174934762734\n",
      "[LOG 20190630-20:51:18] epoch: 1684 train-loss: 0.00032223125185737445\n",
      "[LOG 20190630-20:51:19] epoch: 1685 train-loss: 0.0003222446639483678\n",
      "[LOG 20190630-20:51:19] epoch: 1686 train-loss: 0.00032225765880866675\n",
      "[LOG 20190630-20:51:20] epoch: 1687 train-loss: 0.0003222702248422138\n",
      "[LOG 20190630-20:51:20] epoch: 1688 train-loss: 0.00032228238228526607\n",
      "[LOG 20190630-20:51:21] epoch: 1689 train-loss: 0.00032229408589046216\n",
      "[LOG 20190630-20:51:21] epoch: 1690 train-loss: 0.0003223053706733481\n",
      "[LOG 20190630-20:51:22] epoch: 1691 train-loss: 0.00032231618365585746\n",
      "[LOG 20190630-20:51:22] epoch: 1692 train-loss: 0.00032232649937213864\n",
      "[LOG 20190630-20:51:23] epoch: 1693 train-loss: 0.0003223363255528966\n",
      "[LOG 20190630-20:51:23] epoch: 1694 train-loss: 0.0003223456144496595\n",
      "[LOG 20190630-20:51:24] epoch: 1695 train-loss: 0.00032235438607131073\n",
      "[LOG 20190630-20:51:24] epoch: 1696 train-loss: 0.0003223625637929217\n",
      "[LOG 20190630-20:51:25] epoch: 1697 train-loss: 0.0003223702294690156\n",
      "[LOG 20190630-20:51:25] epoch: 1698 train-loss: 0.0003223773533136409\n",
      "[LOG 20190630-20:51:26] epoch: 1699 train-loss: 0.00032238394351224997\n",
      "[LOG 20190630-20:51:26] epoch: 1700 train-loss: 0.00032238996595879144\n",
      "[LOG 20190630-20:51:27] epoch: 1701 train-loss: 0.0003223953765427723\n",
      "[LOG 20190630-20:51:27] epoch: 1702 train-loss: 0.0003224001243324892\n",
      "[LOG 20190630-20:51:28] epoch: 1703 train-loss: 0.00032240415725937055\n",
      "[LOG 20190630-20:51:28] epoch: 1704 train-loss: 0.0003224075289836037\n",
      "[LOG 20190630-20:51:29] epoch: 1705 train-loss: 0.0003224102565582143\n",
      "[LOG 20190630-20:51:29] epoch: 1706 train-loss: 0.00032241231156149297\n",
      "[LOG 20190630-20:51:30] epoch: 1707 train-loss: 0.000322413668527588\n",
      "[LOG 20190630-20:51:30] epoch: 1708 train-loss: 0.000322414289939843\n",
      "[LOG 20190630-20:51:31] epoch: 1709 train-loss: 0.000322414195352394\n",
      "[LOG 20190630-20:51:31] epoch: 1710 train-loss: 0.0003224133158710174\n",
      "[LOG 20190630-20:51:32] epoch: 1711 train-loss: 0.00032241161261481466\n",
      "[LOG 20190630-20:51:32] epoch: 1712 train-loss: 0.0003224090769435861\n",
      "[LOG 20190630-20:51:32] epoch: 1713 train-loss: 0.0003224057134048053\n",
      "[LOG 20190630-20:51:33] epoch: 1714 train-loss: 0.00032240146265394287\n",
      "[LOG 20190630-20:51:33] epoch: 1715 train-loss: 0.00032239635720543447\n",
      "[LOG 20190630-20:51:34] epoch: 1716 train-loss: 0.00032239036750070227\n",
      "[LOG 20190630-20:51:34] epoch: 1717 train-loss: 0.00032238350308944064\n",
      "[LOG 20190630-20:51:35] epoch: 1718 train-loss: 0.0003223756821171264\n",
      "[LOG 20190630-20:51:35] epoch: 1719 train-loss: 0.0003223668850296235\n",
      "[LOG 20190630-20:51:36] epoch: 1720 train-loss: 0.0003223571225134947\n",
      "[LOG 20190630-20:51:36] epoch: 1721 train-loss: 0.00032234642230832833\n",
      "[LOG 20190630-20:51:37] epoch: 1722 train-loss: 0.0003223347318908054\n",
      "[LOG 20190630-20:51:37] epoch: 1723 train-loss: 0.00032232192825176753\n",
      "[LOG 20190630-20:51:38] epoch: 1724 train-loss: 0.00032230801707555656\n",
      "[LOG 20190630-20:51:38] epoch: 1725 train-loss: 0.0003222929642561212\n",
      "[LOG 20190630-20:51:38] epoch: 1726 train-loss: 0.00032227675865215133\n",
      "[LOG 20190630-20:51:39] epoch: 1727 train-loss: 0.00032225941413344117\n",
      "[LOG 20190630-20:51:39] epoch: 1728 train-loss: 0.0003222409111458546\n",
      "[LOG 20190630-20:51:40] epoch: 1729 train-loss: 0.00032222117874880496\n",
      "[LOG 20190630-20:51:40] epoch: 1730 train-loss: 0.000322200221262392\n",
      "[LOG 20190630-20:51:41] epoch: 1731 train-loss: 0.00032217798434430733\n",
      "[LOG 20190630-20:51:41] epoch: 1732 train-loss: 0.00032215441160587943\n",
      "[LOG 20190630-20:51:42] epoch: 1733 train-loss: 0.0003221294973627664\n",
      "[LOG 20190630-20:51:42] epoch: 1734 train-loss: 0.000322103147937014\n",
      "[LOG 20190630-20:51:43] epoch: 1735 train-loss: 0.0003220753883397265\n",
      "[LOG 20190630-20:51:43] epoch: 1736 train-loss: 0.0003220462042463623\n",
      "[LOG 20190630-20:51:44] epoch: 1737 train-loss: 0.00032201553881350264\n",
      "[LOG 20190630-20:51:44] epoch: 1738 train-loss: 0.00032198333838096005\n",
      "[LOG 20190630-20:51:44] epoch: 1739 train-loss: 0.00032194958885156666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:51:45] epoch: 1740 train-loss: 0.0003219142670332076\n",
      "[LOG 20190630-20:51:45] epoch: 1741 train-loss: 0.0003218774056676921\n",
      "[LOG 20190630-20:51:46] epoch: 1742 train-loss: 0.0003218389272205968\n",
      "[LOG 20190630-20:51:46] epoch: 1743 train-loss: 0.00032179872118831554\n",
      "[LOG 20190630-20:51:47] epoch: 1744 train-loss: 0.00032175674118661846\n",
      "[LOG 20190630-20:51:47] epoch: 1745 train-loss: 0.00032171298130379\n",
      "[LOG 20190630-20:51:48] epoch: 1746 train-loss: 0.00032166740015782125\n",
      "[LOG 20190630-20:51:48] epoch: 1747 train-loss: 0.0003216199995677016\n",
      "[LOG 20190630-20:51:49] epoch: 1748 train-loss: 0.00032157071291294415\n",
      "[LOG 20190630-20:51:49] epoch: 1749 train-loss: 0.0003215194976746716\n",
      "[LOG 20190630-20:51:50] epoch: 1750 train-loss: 0.0003214662797290657\n",
      "[LOG 20190630-20:51:50] epoch: 1751 train-loss: 0.00032141100996341265\n",
      "[LOG 20190630-20:51:50] epoch: 1752 train-loss: 0.0003213536570001452\n",
      "[LOG 20190630-20:51:51] epoch: 1753 train-loss: 0.0003212941819583648\n",
      "[LOG 20190630-20:51:51] epoch: 1754 train-loss: 0.0003212325659660564\n",
      "[LOG 20190630-20:51:52] epoch: 1755 train-loss: 0.00032116875217980123\n",
      "[LOG 20190630-20:51:52] epoch: 1756 train-loss: 0.00032110272832142073\n",
      "[LOG 20190630-20:51:53] epoch: 1757 train-loss: 0.0003210344696071843\n",
      "[LOG 20190630-20:51:53] epoch: 1758 train-loss: 0.00032096389077196363\n",
      "[LOG 20190630-20:51:54] epoch: 1759 train-loss: 0.0003208910106877738\n",
      "[LOG 20190630-20:51:54] epoch: 1760 train-loss: 0.00032081574954645475\n",
      "[LOG 20190630-20:51:55] epoch: 1761 train-loss: 0.00032073809234134387\n",
      "[LOG 20190630-20:51:55] epoch: 1762 train-loss: 0.0003206579694960965\n",
      "[LOG 20190630-20:51:56] epoch: 1763 train-loss: 0.00032057537396212865\n",
      "[LOG 20190630-20:51:56] epoch: 1764 train-loss: 0.0003204902175184543\n",
      "[LOG 20190630-20:51:57] epoch: 1765 train-loss: 0.00032040248902376334\n",
      "[LOG 20190630-20:51:57] epoch: 1766 train-loss: 0.0003203121436854417\n",
      "[LOG 20190630-20:51:57] epoch: 1767 train-loss: 0.0003202191128366394\n",
      "[LOG 20190630-20:51:58] epoch: 1768 train-loss: 0.00032012343513088126\n",
      "[LOG 20190630-20:51:58] epoch: 1769 train-loss: 0.0003200250328063703\n",
      "[LOG 20190630-20:51:59] epoch: 1770 train-loss: 0.00031992388380785997\n",
      "[LOG 20190630-20:51:59] epoch: 1771 train-loss: 0.0003198199121925427\n",
      "[LOG 20190630-20:52:00] epoch: 1772 train-loss: 0.000319713114777187\n",
      "[LOG 20190630-20:52:00] epoch: 1773 train-loss: 0.0003196034101620171\n",
      "[LOG 20190630-20:52:01] epoch: 1774 train-loss: 0.0003194908038040012\n",
      "[LOG 20190630-20:52:01] epoch: 1775 train-loss: 0.00031937521066538466\n",
      "[LOG 20190630-20:52:02] epoch: 1776 train-loss: 0.00031925658868203755\n",
      "[LOG 20190630-20:52:02] epoch: 1777 train-loss: 0.00031913491125123983\n",
      "[LOG 20190630-20:52:03] epoch: 1778 train-loss: 0.0003190100821939268\n",
      "[LOG 20190630-20:52:03] epoch: 1779 train-loss: 0.0003188820230661804\n",
      "[LOG 20190630-20:52:03] epoch: 1780 train-loss: 0.00031875071385911724\n",
      "[LOG 20190630-20:52:04] epoch: 1781 train-loss: 0.0003186160981840658\n",
      "[LOG 20190630-20:52:04] epoch: 1782 train-loss: 0.00031847806667428813\n",
      "[LOG 20190630-20:52:05] epoch: 1783 train-loss: 0.00031833660227675864\n",
      "[LOG 20190630-20:52:05] epoch: 1784 train-loss: 0.000318191679298252\n",
      "[LOG 20190630-20:52:06] epoch: 1785 train-loss: 0.0003180432631779695\n",
      "[LOG 20190630-20:52:06] epoch: 1786 train-loss: 0.00031789126796866185\n",
      "[LOG 20190630-20:52:07] epoch: 1787 train-loss: 0.00031773559749126434\n",
      "[LOG 20190630-20:52:07] epoch: 1788 train-loss: 0.00031757615465721756\n",
      "[LOG 20190630-20:52:08] epoch: 1789 train-loss: 0.0003174128910359286\n",
      "[LOG 20190630-20:52:08] epoch: 1790 train-loss: 0.0003172457966229558\n",
      "[LOG 20190630-20:52:09] epoch: 1791 train-loss: 0.0003170748091179121\n",
      "[LOG 20190630-20:52:09] epoch: 1792 train-loss: 0.00031689986508354195\n",
      "[LOG 20190630-20:52:09] epoch: 1793 train-loss: 0.0003167208683407807\n",
      "[LOG 20190630-20:52:10] epoch: 1794 train-loss: 0.0003165377847835771\n",
      "[LOG 20190630-20:52:10] epoch: 1795 train-loss: 0.00031635053346690256\n",
      "[LOG 20190630-20:52:11] epoch: 1796 train-loss: 0.00031615905618309625\n",
      "[LOG 20190630-20:52:11] epoch: 1797 train-loss: 0.00031596331700711744\n",
      "[LOG 20190630-20:52:12] epoch: 1798 train-loss: 0.0003157633273076499\n",
      "[LOG 20190630-20:52:12] epoch: 1799 train-loss: 0.00031555897658108734\n",
      "[LOG 20190630-20:52:13] epoch: 1800 train-loss: 0.0003153502250370366\n",
      "[LOG 20190630-20:52:13] epoch: 1801 train-loss: 0.00031513710223407543\n",
      "[LOG 20190630-20:52:14] epoch: 1802 train-loss: 0.00031491960567109345\n",
      "[LOG 20190630-20:52:14] epoch: 1803 train-loss: 0.000314697735802838\n",
      "[LOG 20190630-20:52:15] epoch: 1804 train-loss: 0.0003144714319205377\n",
      "[LOG 20190630-20:52:15] epoch: 1805 train-loss: 0.00031424065036844695\n",
      "[LOG 20190630-20:52:16] epoch: 1806 train-loss: 0.00031400537000081385\n",
      "[LOG 20190630-20:52:16] epoch: 1807 train-loss: 0.00031376561855722684\n",
      "[LOG 20190630-20:52:16] epoch: 1808 train-loss: 0.0003135214888061455\n",
      "[LOG 20190630-20:52:17] epoch: 1809 train-loss: 0.00031327304418482527\n",
      "[LOG 20190630-20:52:17] epoch: 1810 train-loss: 0.0003130203176624491\n",
      "[LOG 20190630-20:52:18] epoch: 1811 train-loss: 0.0003127633633539517\n",
      "[LOG 20190630-20:52:18] epoch: 1812 train-loss: 0.0003125022797121346\n",
      "[LOG 20190630-20:52:19] epoch: 1813 train-loss: 0.0003122372149846342\n",
      "[LOG 20190630-20:52:19] epoch: 1814 train-loss: 0.00031196830036606116\n",
      "[LOG 20190630-20:52:20] epoch: 1815 train-loss: 0.00031169568819677806\n",
      "[LOG 20190630-20:52:20] epoch: 1816 train-loss: 0.00031141958083935606\n",
      "[LOG 20190630-20:52:21] epoch: 1817 train-loss: 0.0003111402556896792\n",
      "[LOG 20190630-20:52:21] epoch: 1818 train-loss: 0.00031085787827578315\n",
      "[LOG 20190630-20:52:22] epoch: 1819 train-loss: 0.00031057260252964625\n",
      "[LOG 20190630-20:52:22] epoch: 1820 train-loss: 0.0003102847383615881\n",
      "[LOG 20190630-20:52:22] epoch: 1821 train-loss: 0.00030999461205283296\n",
      "[LOG 20190630-20:52:23] epoch: 1822 train-loss: 0.0003097024361977674\n",
      "[LOG 20190630-20:52:23] epoch: 1823 train-loss: 0.00030940848978389113\n",
      "[LOG 20190630-20:52:24] epoch: 1824 train-loss: 0.00030911313046999567\n",
      "[LOG 20190630-20:52:24] epoch: 1825 train-loss: 0.00030881676229910227\n",
      "[LOG 20190630-20:52:25] epoch: 1826 train-loss: 0.0003085196863139572\n",
      "[LOG 20190630-20:52:25] epoch: 1827 train-loss: 0.0003082222497141629\n",
      "[LOG 20190630-20:52:26] epoch: 1828 train-loss: 0.00030792486700192967\n",
      "[LOG 20190630-20:52:26] epoch: 1829 train-loss: 0.00030762790561311704\n",
      "[LOG 20190630-20:52:27] epoch: 1830 train-loss: 0.0003073317329835845\n",
      "[LOG 20190630-20:52:27] epoch: 1831 train-loss: 0.0003070367456530221\n",
      "[LOG 20190630-20:52:28] epoch: 1832 train-loss: 0.0003067433281103149\n",
      "[LOG 20190630-20:52:28] epoch: 1833 train-loss: 0.0003064518023165874\n",
      "[LOG 20190630-20:52:28] epoch: 1834 train-loss: 0.0003061625361624465\n",
      "[LOG 20190630-20:52:29] epoch: 1835 train-loss: 0.0003058759721170645\n",
      "[LOG 20190630-20:52:29] epoch: 1836 train-loss: 0.0003055923777992575\n",
      "[LOG 20190630-20:52:30] epoch: 1837 train-loss: 0.00030531206357409246\n",
      "[LOG 20190630-20:52:30] epoch: 1838 train-loss: 0.00030503536549986165\n",
      "[LOG 20190630-20:52:31] epoch: 1839 train-loss: 0.00030476264032586187\n",
      "[LOG 20190630-20:52:31] epoch: 1840 train-loss: 0.00030449411542576854\n",
      "[LOG 20190630-20:52:32] epoch: 1841 train-loss: 0.000304230084793744\n",
      "[LOG 20190630-20:52:32] epoch: 1842 train-loss: 0.00030397077512134274\n",
      "[LOG 20190630-20:52:33] epoch: 1843 train-loss: 0.0003037164444776863\n",
      "[LOG 20190630-20:52:33] epoch: 1844 train-loss: 0.00030346732410180266\n",
      "[LOG 20190630-20:52:34] epoch: 1845 train-loss: 0.0003032235852060694\n",
      "[LOG 20190630-20:52:34] epoch: 1846 train-loss: 0.0003029853974112484\n",
      "[LOG 20190630-20:52:35] epoch: 1847 train-loss: 0.0003027529553492059\n",
      "[LOG 20190630-20:52:35] epoch: 1848 train-loss: 0.0003025263497420383\n",
      "[LOG 20190630-20:52:35] epoch: 1849 train-loss: 0.00030230571019274066\n",
      "[LOG 20190630-20:52:36] epoch: 1850 train-loss: 0.00030209115539037157\n",
      "[LOG 20190630-20:52:36] epoch: 1851 train-loss: 0.00030188278333298513\n",
      "[LOG 20190630-20:52:37] epoch: 1852 train-loss: 0.0003016806165305752\n",
      "[LOG 20190630-20:52:37] epoch: 1853 train-loss: 0.00030148474252200685\n",
      "[LOG 20190630-20:52:38] epoch: 1854 train-loss: 0.000301295219514941\n",
      "[LOG 20190630-20:52:38] epoch: 1855 train-loss: 0.0003011120713836135\n",
      "[LOG 20190630-20:52:39] epoch: 1856 train-loss: 0.00030093528130237246\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LOG 20190630-20:52:39] epoch: 1857 train-loss: 0.00030076488042141136\n",
      "[LOG 20190630-20:52:40] epoch: 1858 train-loss: 0.0003006008894317347\n",
      "[LOG 20190630-20:52:40] epoch: 1859 train-loss: 0.0003004432567195181\n",
      "[LOG 20190630-20:52:41] epoch: 1860 train-loss: 0.0003002919547725469\n",
      "[LOG 20190630-20:52:41] epoch: 1861 train-loss: 0.0003001469597165851\n",
      "[LOG 20190630-20:52:42] epoch: 1862 train-loss: 0.0003000082278958871\n",
      "[LOG 20190630-20:52:42] epoch: 1863 train-loss: 0.00029987572793288564\n",
      "[LOG 20190630-20:52:43] epoch: 1864 train-loss: 0.00029974938365739945\n",
      "[LOG 20190630-20:52:43] epoch: 1865 train-loss: 0.0002996291389081307\n",
      "[LOG 20190630-20:52:44] epoch: 1866 train-loss: 0.00029951495071145473\n",
      "[LOG 20190630-20:52:44] epoch: 1867 train-loss: 0.0002994067442614323\n",
      "[LOG 20190630-20:52:45] epoch: 1868 train-loss: 0.00029930443201919843\n",
      "[LOG 20190630-20:52:45] epoch: 1869 train-loss: 0.00029920793804194545\n",
      "[LOG 20190630-20:52:46] epoch: 1870 train-loss: 0.00029911716842434544\n",
      "[LOG 20190630-20:52:46] epoch: 1871 train-loss: 0.00029903205290793267\n",
      "[LOG 20190630-20:52:47] epoch: 1872 train-loss: 0.00029895251100242604\n",
      "[LOG 20190630-20:52:47] epoch: 1873 train-loss: 0.00029887844357290305\n",
      "[LOG 20190630-20:52:48] epoch: 1874 train-loss: 0.00029880974830120977\n",
      "[LOG 20190630-20:52:48] epoch: 1875 train-loss: 0.00029874634310544934\n",
      "[LOG 20190630-20:52:49] epoch: 1876 train-loss: 0.000298688138855141\n",
      "[LOG 20190630-20:52:49] epoch: 1877 train-loss: 0.00029863502436455747\n",
      "[LOG 20190630-20:52:49] epoch: 1878 train-loss: 0.0002985869082294812\n",
      "[LOG 20190630-20:52:50] epoch: 1879 train-loss: 0.0002985436794915586\n",
      "[LOG 20190630-20:52:50] epoch: 1880 train-loss: 0.00029850524970242986\n",
      "[LOG 20190630-20:52:51] epoch: 1881 train-loss: 0.0002984715092679835\n",
      "[LOG 20190630-20:52:51] epoch: 1882 train-loss: 0.00029844237019460707\n",
      "[LOG 20190630-20:52:52] epoch: 1883 train-loss: 0.0002984177278904099\n",
      "[LOG 20190630-20:52:52] epoch: 1884 train-loss: 0.0002983974734434014\n",
      "[LOG 20190630-20:52:53] epoch: 1885 train-loss: 0.0002983815090829012\n",
      "[LOG 20190630-20:52:53] epoch: 1886 train-loss: 0.0002983697129366192\n",
      "[LOG 20190630-20:52:54] epoch: 1887 train-loss: 0.00029836197904842265\n",
      "[LOG 20190630-20:52:54] epoch: 1888 train-loss: 0.0002983582241995464\n",
      "[LOG 20190630-20:52:55] epoch: 1889 train-loss: 0.00029835834016012086\n",
      "[LOG 20190630-20:52:55] epoch: 1890 train-loss: 0.00029836219277967757\n",
      "[LOG 20190630-20:52:55] epoch: 1891 train-loss: 0.0002983696813316783\n",
      "[LOG 20190630-20:52:56] epoch: 1892 train-loss: 0.00029838073805876775\n",
      "[LOG 20190630-20:52:56] epoch: 1893 train-loss: 0.0002983952686008706\n",
      "[LOG 20190630-20:52:57] epoch: 1894 train-loss: 0.00029841313744327636\n",
      "[LOG 20190630-20:52:57] epoch: 1895 train-loss: 0.0002984342838772136\n",
      "[LOG 20190630-20:52:58] epoch: 1896 train-loss: 0.0002984585678404983\n",
      "[LOG 20190630-20:52:58] epoch: 1897 train-loss: 0.0002984858642776089\n",
      "[LOG 20190630-20:52:59] epoch: 1898 train-loss: 0.00029851604244868213\n",
      "[LOG 20190630-20:52:59] epoch: 1899 train-loss: 0.0002985490561968618\n",
      "[LOG 20190630-20:53:00] epoch: 1900 train-loss: 0.0002985848377647926\n",
      "[LOG 20190630-20:53:00] epoch: 1901 train-loss: 0.00029862324527130113\n",
      "[LOG 20190630-20:53:01] epoch: 1902 train-loss: 0.0002986641561619763\n",
      "[LOG 20190630-20:53:01] epoch: 1903 train-loss: 0.00029870752405258827\n",
      "[LOG 20190630-20:53:02] epoch: 1904 train-loss: 0.00029875322979933117\n",
      "[LOG 20190630-20:53:02] epoch: 1905 train-loss: 0.0002988011526667833\n",
      "[LOG 20190630-20:53:02] epoch: 1906 train-loss: 0.0002988512248975894\n",
      "[LOG 20190630-20:53:03] epoch: 1907 train-loss: 0.00029890334872106905\n",
      "[LOG 20190630-20:53:03] epoch: 1908 train-loss: 0.00029895745910835103\n",
      "[LOG 20190630-20:53:04] epoch: 1909 train-loss: 0.00029901345874350227\n",
      "[LOG 20190630-20:53:04] epoch: 1910 train-loss: 0.00029907122257100127\n",
      "[LOG 20190630-20:53:05] epoch: 1911 train-loss: 0.00029913063667663664\n",
      "[LOG 20190630-20:53:05] epoch: 1912 train-loss: 0.00029919163944214233\n",
      "[LOG 20190630-20:53:06] epoch: 1913 train-loss: 0.00029925414673925843\n",
      "[LOG 20190630-20:53:06] epoch: 1914 train-loss: 0.0002993180235080217\n",
      "[LOG 20190630-20:53:07] epoch: 1915 train-loss: 0.00029938316833977296\n",
      "[LOG 20190630-20:53:07] epoch: 1916 train-loss: 0.00029944958214400685\n",
      "[LOG 20190630-20:53:08] epoch: 1917 train-loss: 0.0002995171305428812\n",
      "[LOG 20190630-20:53:08] epoch: 1918 train-loss: 0.00029958568825350085\n",
      "[LOG 20190630-20:53:08] epoch: 1919 train-loss: 0.0002996552061631519\n",
      "[LOG 20190630-20:53:09] epoch: 1920 train-loss: 0.0002997255685386335\n",
      "[LOG 20190630-20:53:09] epoch: 1921 train-loss: 0.0002997967076225905\n",
      "[LOG 20190630-20:53:10] epoch: 1922 train-loss: 0.0002998685431521153\n",
      "[LOG 20190630-20:53:10] epoch: 1923 train-loss: 0.00029994096303198603\n",
      "[LOG 20190630-20:53:11] epoch: 1924 train-loss: 0.00030001384789102303\n",
      "[LOG 20190630-20:53:11] epoch: 1925 train-loss: 0.0003000870976848091\n",
      "[LOG 20190630-20:53:12] epoch: 1926 train-loss: 0.00030016068240001914\n",
      "[LOG 20190630-20:53:12] epoch: 1927 train-loss: 0.00030023449858163076\n",
      "[LOG 20190630-20:53:13] epoch: 1928 train-loss: 0.00030030844914108457\n",
      "[LOG 20190630-20:53:13] epoch: 1929 train-loss: 0.0003003823746894341\n",
      "[LOG 20190630-20:53:14] epoch: 1930 train-loss: 0.000300456172453778\n",
      "[LOG 20190630-20:53:14] epoch: 1931 train-loss: 0.0003005297598974721\n",
      "[LOG 20190630-20:53:14] epoch: 1932 train-loss: 0.00030060305834922474\n",
      "[LOG 20190630-20:53:15] epoch: 1933 train-loss: 0.0003006759807249182\n",
      "[LOG 20190630-20:53:15] epoch: 1934 train-loss: 0.0003007484231147828\n",
      "[LOG 20190630-20:53:16] epoch: 1935 train-loss: 0.000300820252050471\n",
      "[LOG 20190630-20:53:16] epoch: 1936 train-loss: 0.00030089141364442185\n",
      "[LOG 20190630-20:53:17] epoch: 1937 train-loss: 0.00030096173827587336\n",
      "[LOG 20190630-20:53:17] epoch: 1938 train-loss: 0.00030103115568635985\n",
      "[LOG 20190630-20:53:18] epoch: 1939 train-loss: 0.0003010996329066984\n",
      "[LOG 20190630-20:53:18] epoch: 1940 train-loss: 0.0003011670389696519\n",
      "[LOG 20190630-20:53:19] epoch: 1941 train-loss: 0.0003012331972058746\n",
      "[LOG 20190630-20:53:19] epoch: 1942 train-loss: 0.000301298078511536\n",
      "[LOG 20190630-20:53:20] epoch: 1943 train-loss: 0.0003013615553300042\n"
     ]
    }
   ],
   "source": [
    "# init collection of training epoch losses\n",
    "train_epoch_losses = []\n",
    "\n",
    "# set the model in training mode\n",
    "lstm_model.train()\n",
    "\n",
    "# init the best loss\n",
    "best_loss = 100.00\n",
    "\n",
    "# iterate over epochs\n",
    "for epoch in range(0, num_epochs):\n",
    "\n",
    "    # init collection of mini-batch losses\n",
    "    train_mini_batch_losses = []\n",
    "            \n",
    "    # iterate over mini-batches\n",
    "    for sequence_batch, target_batch in dataloader:\n",
    "\n",
    "        # predict sequence output\n",
    "        prediction_batch = lstm_model(sequence_batch)\n",
    "\n",
    "        # calculate batch loss\n",
    "        batch_loss = loss_function(prediction_batch, target_batch)\n",
    "\n",
    "        # run backward gradient calculation\n",
    "        batch_loss.backward()\n",
    "\n",
    "        # update network parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        # collect mini-batch loss\n",
    "        train_mini_batch_losses.append(batch_loss.data.item())\n",
    "            \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_loss = np.mean(train_mini_batch_losses)\n",
    "    \n",
    "    # print epoch loss\n",
    "    now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "    print('[LOG {}] epoch: {} train-loss: {}'.format(str(now), str(epoch), str(train_epoch_loss)))\n",
    "    \n",
    "    # determine mean min-batch loss of epoch\n",
    "    train_epoch_losses.append(train_epoch_loss)\n",
    "        \n",
    "    # print epoch and save models\n",
    "    if epoch % 10 == 0 and epoch > 0:\n",
    "        \n",
    "        # case: new best model trained\n",
    "        if train_epoch_loss < best_loss:\n",
    "                        \n",
    "            # store new best model\n",
    "            model_name = 'awsome_lstm_model_{}.pth'.format(str(epoch))\n",
    "            torch.save(lstm_model.state_dict(), os.path.join(\"./models\", model_name))\n",
    "            \n",
    "            # update best loss\n",
    "            best_loss = train_epoch_loss\n",
    "            \n",
    "            # print epoch loss\n",
    "            now = dt.datetime.utcnow().strftime(\"%Y%m%d-%H:%M:%S\")\n",
    "            print('[LOG {}] epoch: {} new best train-loss: {} found'.format(str(now), str(epoch), str(train_epoch_loss)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon successful training let's visualize and inspect the training loss per epoch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot the training epochs vs. the epochs' prediction error\n",
    "ax.plot(np.array(range(1, len(train_epoch_losses)+1)), train_epoch_losses, label='epoch loss (blue)')\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[training epoch $e_i$]\", fontsize=10)\n",
    "ax.set_ylabel(\"[Prediction Error $\\mathcal{L}^{MSE}$]\", fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Training Epochs $e_i$ vs. Prediction Error $L^{MSE}$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, fantastic. The training error is nicely going down. We could definitely train the network a couple more epochs until the error converges. But let's stay with the 2'000 training epochs for now and continue with evaluating our trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5.0. Evaluation of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will conduct a visual comparison of the predicted daily returns to the actual ('true') daily returns. The comparison will encompass the daily returns of the in-sample time period as well as the returns of the out-of-sample time period."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.1. In-Sample Evaluation of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior to evaluating our model let's load the best performing model or an already pre-trained model (as done below). Remember, that we stored a snapshot of the model after each training epoch to our local model directory. We will now load one of the (hopefully well performing) snapshots saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'awsome_lstm_model_12250.pth'\n",
    "lstm_model.load_state_dict(torch.load(os.path.join(\"./models\", model_name)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect if the model was loaded successfully: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set model in evaluation mode\n",
    "lstm_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine predictions of the in-sample population of sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't calculate gradients\n",
    "with torch.no_grad():\n",
    "\n",
    "    # predict sequence output\n",
    "    predictions = lstm_model(train_sequences_input)\n",
    "\n",
    "    # collect prediction batch results\n",
    "    predictions_list = predictions.detach().numpy()[:, -1].tolist()\n",
    "\n",
    "    # collect target batch results\n",
    "    targets_list = train_sequences_target.numpy()[:, -1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the lstm neural network in-sample predictions vs. the actual \"ground-truth\" adjusted daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction results\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(pd.to_datetime(stock_data.index).values[0:train_sequences.shape[0],], targets_list, color='C1', label='groundtruth (green)')\n",
    "ax.plot(pd.to_datetime(stock_data.index).values[0:train_sequences.shape[0],], predictions_list, color='C0', label='predictions (blue)')\n",
    "\n",
    "# set y-axis limits\n",
    "ax.set_xlim(pd.to_datetime(stock_data.index).values[0], pd.to_datetime(stock_data.index).values[train_sequences.shape[0]])\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "plt.title('LSTM NN In-Sample Prediction vs. Ground-Truth Market Prices', fontsize=10)\n",
    "plt.xlabel('[time]', fontsize=8)\n",
    "plt.ylabel('[market price]', fontsize=8)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5.2. Out-of-Sample Evaluation of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine predictions of the out-of-sample population of sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't calculate gradients\n",
    "with torch.no_grad():\n",
    "\n",
    "    # predict sequence output\n",
    "    predictions = lstm_model(valid_sequences_input)\n",
    "\n",
    "    # collect prediction batch results\n",
    "    predictions_list = predictions.detach().numpy()[:, -1].tolist()\n",
    "\n",
    "    # collect target batch results\n",
    "    targets_list = valid_sequences_target.numpy()[:, -1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the lstm neural network out-of-sample predictions vs. the actual \"ground-truth\" adjusted daily returns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the prediction results\n",
    "plt.style.use('seaborn')\n",
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],], targets_list, color='C1', label='groundtruth (green)')\n",
    "ax.plot(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],], predictions_list, color='C0', label='predictions (blue)')\n",
    "\n",
    "# set y-axis limits\n",
    "ax.set_xlim(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]], pd.to_datetime(stock_data.index).values[train_sequences.shape[0]+valid_sequences.shape[0]])\n",
    "#ax.set_ylim(0.0, 1.0)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"lower right\", numpoints=1, fancybox=True)\n",
    "\n",
    "plt.title('LSTM NN Out-of-Sample Prediction vs. Ground-Truth Market Prices', fontsize=10)\n",
    "plt.xlabel('[time]', fontsize=8)\n",
    "plt.ylabel('[market price]', fontsize=8)\n",
    "plt.xticks(fontsize=8)\n",
    "plt.yticks(fontsize=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6.0. Backtest of the Trained Neural Network Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will backtest the python `bt` library. Python `bt` is a flexible backtest framework that can be used to test quantitative trading strategies. In general, backtesting is the process of testing a strategy over a given data set (more details about the `bt` library can be found via: https://pmorissette.github.io/bt/). \n",
    "\n",
    "In order to test the predictions derived from the LSTM model we will view its predictions as trade signals. Thereby, we will interpret any positive future return prediction $r_{t+1} > 0.0$ of a sequence $s$ as a \"long\" (buy) signal. Likewise, we will interpret any negative future return prediction $r_{t+1} < 0.0$ of a sequence $s$ as a \"short\" (sell) signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.1: LSTM Trading Signal Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by converting the out-of-sample model predictions into a trading signal as described above. Therefore, we first convert the obtained predictions into a Pandas data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_daily_predictions = pd.DataFrame(predictions_list, columns=['PREDICTIONS'])\n",
    "stock_daily_predictions = stock_daily_predictions.set_index(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's briefly ensure the successful conversion by inspecting the top 5 rows of the predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_daily_predictions.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's derive a trading signal from the converted predictions. As already described, we will generate the trading signal $\\phi$ according to the following function:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "$\n",
    "\\\\\n",
    "\\phi(\\hat{r}_{t+1})=\n",
    "\\begin{cases}\n",
    "1.0, & for & \\hat{r}_{t+1} > 0.0\\\\\n",
    "-1.0, & for & \\hat{r}_{t+1} < 0.0\\\\\n",
    "\\end{cases}\n",
    "$\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\hat{r}_{t+1}$ denotes a by the model predicted future return at time $t+1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data = pd.DataFrame(np.where(stock_daily_predictions['PREDICTIONS'] > 0.0, 1.0, -1.0), columns=['SIGNAL'])\n",
    "signal_data = signal_data.set_index(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]:train_sequences.shape[0]+valid_sequences.shape[0],])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top 5 rows of the prepared trading signals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.2: Stock Market Data Preperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's prepare the daily adjusted closing prices so that they can be utilized in the backtest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market_data = pd.DataFrame(stock_data['Adj. Close'])\n",
    "stock_market_data = stock_market_data.rename(columns={'Adj. Close': 'PRICE'})\n",
    "stock_market_data = stock_market_data.set_index(pd.to_datetime(stock_data.index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's inspect the top 5 rows of the prepared adjusted closing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market_data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-sample the prepared daily adjusted closing prices to the out-of-sample time period:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_market_data = stock_market_data[stock_market_data.index >= stock_daily_predictions.index[0]]\n",
    "stock_market_data = stock_market_data[stock_market_data.index <= stock_daily_predictions.index[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the out-of-sample daily adjusted closing prices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(stock_market_data['PRICE'], color='#9b59b6')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    \n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "#ax.set_xlim(pd.to_datetime(stock_market_data.index).values[train_sequences.shape[0]], pd.to_datetime(stock_data.index).values[train_sequences.shape[0]+valid_sequences.shape[0]])\n",
    "ax.set_ylabel('[equity %]', fontsize=10)\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim([stock_market_data.index[0], stock_market_data.index[-1]])\n",
    "ax.set_ylabel('[adj. closing price]', fontsize=10)\n",
    "\n",
    "# set plot title\n",
    "plt.title('International Business Machines Corporation (IBM) - Daily Historical Stock Closing Prices', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's calculate our potential gained return by the application of a simple \"buy and hold\" strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.abs(stock_market_data.iloc[0]['PRICE'] - stock_market_data.iloc[-1]['PRICE']) / stock_market_data.iloc[0]['PRICE']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.3. Backtest Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the \"Moving Average\" trading strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMStrategy(bt.Algo):\n",
    "    \n",
    "    def __init__(self, signals):\n",
    "        \n",
    "        # set class signals\n",
    "        self.signals = signals\n",
    "        \n",
    "    def __call__(self, target):\n",
    "        \n",
    "        if target.now in self.signals.index[1:]:\n",
    "            \n",
    "            # get actual signal\n",
    "            signal = self.signals.ix[target.now]\n",
    "            \n",
    "            # set target weights according to signal\n",
    "            target.temp['weights'] = dict(PRICE=signal)\n",
    "            \n",
    "        # return True since we want to move on to the next timestep\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize our LSTM based trading strategy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_strategy = bt.Strategy('lstm', [bt.algos.SelectAll(), LSTMStrategy(signal_data['SIGNAL']), bt.algos.Rebalance()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the backtest of our LSTM based trading strategy using the strategy and prepared market data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm = bt.Backtest(strategy=lstm_strategy, data=stock_market_data, name='stock_lstm_backtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's also prepare a backtest of a \"baseline\" buy-and-hold trading strategy for comparison purposes. Our buy-and-hold strategy sends a \"long\" (+1.0) signal at each time step of the out-of-sample time frame: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "signal_data_base = signal_data.copy(deep=True) \n",
    "signal_data_base['SIGNAL'] = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Init the buy-and-hold (\"base\") strategy as well as the corresponding backtest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_strategy = bt.Strategy('base', [bt.algos.SelectAll(), LSTMStrategy(signal_data_base['SIGNAL']), bt.algos.Rebalance()])\n",
    "backtest_base = bt.Backtest(strategy=base_strategy, data=stock_market_data, name='stock_base_backtest')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 6.4. Running the Backtest and Evaluate Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the backtest for both trading strategies: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_results = bt.run(backtest_lstm, backtest_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the individual backtest results and performance measures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_results.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the LSTM trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm_details = backtest_lstm.strategy.data\n",
    "backtest_lstm_details.columns = ['% EQUITY', 'EQUITY', 'CASH', 'FEES']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the LSTM trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_lstm_details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect the \"buy-and-hold\" trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_base_details = backtest_base.strategy.data\n",
    "backtest_base_details.columns = ['% EQUITY', 'EQUITY', 'CASH', 'FEES']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the \"buy-and-hold\" trading strategy backtest details:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "backtest_base_details.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the equity progression of both strategies over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [15, 5]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.plot(backtest_lstm_details['% EQUITY'], color='C1',lw=1.0, label='lstm strategy (green)')\n",
    "ax.plot(backtest_base_details['% EQUITY'], color='C2',lw=1.0, label='base strategy (red)')\n",
    "\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "    \n",
    "# set axis labels\n",
    "ax.set_xlabel('[time]', fontsize=10)\n",
    "ax.set_xlim(pd.to_datetime(stock_data.index).values[train_sequences.shape[0]], pd.to_datetime(stock_data.index).values[train_sequences.shape[0]+valid_sequences.shape[0]])\n",
    "ax.set_ylabel('[equity %]', fontsize=10)\n",
    "\n",
    "# set plot legend\n",
    "plt.legend(loc=\"upper right\", numpoints=1, fancybox=True)\n",
    "\n",
    "# set plot title\n",
    "plt.title('International Business Machines Corporation (IBM) - Backtest % Equity Progression', fontsize=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following exercises as part of the lab:\n",
    "\n",
    "**1. Evaluation of Shallow vs. Deep RNN Models.**\n",
    "\n",
    "> Download the daily adjusted closing prices of the IBM stock within the time frame starting from 01/01/1990 until 05/31/2019. In addition to the architecture of the lab notebook, evaluate further (more shallow as well as more deep) RNN architectures by (1) either re- moving/adding layers of LSTM cells and/or (2) increasing/decreasing the dimensionality of the LSTM cells hidden state. Train your model (using architectures you selected) for at least 20'000 training epochs but keep the following parameters unchanged (a) sequence length: 5 time-steps (days) and (b) train vs. test fraction: 0.9.\n",
    "\n",
    "> Analyze the prediction performance of the trained models in terms of training time and prediction accuracy. Furthermore, backtest the out-of-sample signals predicted by each of your models and evaluate them in terms of total return and equity progression. Which of your architecture results in the best performing model and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Training and Evaluation of Models Learned from Additional Stocks.**\n",
    "\n",
    "> Download the daily adjusted closing prices of at least two additional stocks (e.g., Alphabet, Deutsche Bank) within the time frame starting from 01/01/1990 until 05/31/2017. Pls. select two stocks that you are interested in to investigate ( e.g. stocks that you may occasionally trade yourself). Learn an optimal RNN model of both stocks and backtest their corresponding trade signals by following the approach outlined in the lab notebook regarding the IBM stock. Pls. keep the train vs. test dataset fraction fixed to 0.9, all other parameters of the data preparation and model training can be changed.\n",
    "\n",
    "> Analyse the performance of the learned models in terms of their prediction accuracy as well as their out-of-sample backtest performance (e. g. the total return and equity progression). What architectures and corresponding training parameters result in the best performing models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Training and Evaluation of Models Learned from Augmented Data.**\n",
    "\n",
    "> In the prior exercises we used the historical daily adjusted returns of a single stock to learn a model that can predict the stocks future adjusted closing price (log- return) movement. However, one of the advantages of NNs lies in their capability to learn a model from multiple sources of input data.\n",
    "For each of the two stocks (target stocks) that you selected in exercise 2. learn an optimal RNN model using the daily returns as a target label. However, prior to training your models augment the training data of each stock by the return sequences of at least three additional stocks. The additional stocks, used for data augmentation, should exhibit a high correlation to the historical adjusted closing prices of the target stock price movement you aim to model.\n",
    "\n",
    "> Analyse the performance of the learned models in terms of their prediction accuracy as well as their out-of-sample backtest performance (e. g. the total return and equity progression). Do you observe an improvement of the trained model in terms of out-of-sample backtest performance comparison to exercise 1.?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this seventh lab, a step by step introduction into **design, implementation, training and evaluation** of a deep learning-based LSTM neural network based trading strategy is presented. \n",
    "\n",
    "The strategy trades a specific financial instrument based on its historical adjusted daily market prices. The degree of success of the implemented strategy is evaluated based in its backtest performance with particular focus on (1) the strategy's **total return** as well as (2) its **equity progression** over time. \n",
    "\n",
    "The code provided in this lab provides a blueprint for the development and testing of more complex trading strategies. It furthermore can be tailored to be applied for momentum trading of other financial instruments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to execute the content of your lab outside of the jupyter notebook environment e.g. on compute node or server. The cell below converts the lab notebook into a standalone and executable python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script cfds_lab_07.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In order to execute the statement above and convert your lab notebook to a regular Python script you first need to install the nbconvert Python package e.g. using the pip package installer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
