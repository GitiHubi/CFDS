{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"right\" style=\"max-width: 200px; height: auto\" src=\"images/cfds_logo.png\">\n",
    "\n",
    "###  Lab 04 - \"Unsupervised Machine Learning\"\n",
    "\n",
    "Chartered Financial Data Scientist (CFDS), Spring Term 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the third lab you learned about how to utilize two **supervised** learning classification techniques namely (1) the Gaussian Naive-Bayes (Gaussian NB) classifier and (2) the k Nearest-Neighbor (kNN) classifier. \n",
    "\n",
    "In this fourth lab we will learn about an **unsupervised** machine learning technique referred to as **k-Means Clustering**. We will use this technique to classify un-labeled data (i.e., data without defined categories or groups). In general, clustering based techniques are widely used in unsupervised machine learning.\n",
    "\n",
    "<img align=\"center\" style=\"max-width: 500px\" src=\"images/ml.png\">\n",
    "\n",
    "The **k-Means Clustering** algorithm is one of the most popular clustering algorithms used in machine learning. The goal of k-Means Clustering is to find groups (clusters) in a given dataset. It can be used (1) to **confirm business assumptions** about what types of groups exist or (2) to **identify unknown groups** in complex data sets. Some examples of business related use cases are:\n",
    "\n",
    ">- Segment customers by purchase history;\n",
    ">- Segment users by activities on an application or a website;\n",
    ">- Group inventory by sales activity; or,\n",
    ">- Group inventory by manufacturing metrics.\n",
    "\n",
    "(Source: https://www.datascience.com/blog/k-means-clustering)\n",
    "\n",
    "Once the algorithm has been run and the groups are defined, any new data can be easily assigned to the correct group."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you experiencing any difficulties with the lab content or have any questions pls. don't hesitate to Marco Schreyer (marco.schreyer@unisg.ch)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After today's lab you should be able to:\n",
    "\n",
    "> 1. Understand how a **k-Means Clustering** algorithm can be trained and evaluated.\n",
    "> 2. Know how to Python's **sklearn library** to train and evaluate arbitrary classifiers.\n",
    "> 3. Understand how to evaluate the **classification results** of the k-Means algorithm.\n",
    "> 4. Know how to select an **optimal number of clusters** or cluster means."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup of the Jupyter Notebook Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous labs, we need to import a couple of Python libraries that allow for data analysis and data visualization. We will be using pandas, numpy, sklearn, matplotlib and seaborn library to conduct this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the pandas data science library\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import sklearn data and data pre-processing libraries\n",
    "from sklearn import datasets\n",
    "\n",
    "# import sklearn k-means classifier library\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create nice looking plots using the **seaborn** plotting theme:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable inline Jupyter notebook plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppress potential warnings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsupervised Machine Learning: k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.0: Dataset Download and Data Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The iris dataset is a classic and very easy multi-class classification dataset. This data sets consists of 3 different types of irisesâ€™ (classes),  namely Setosa, Versicolour, and Virginica) as well as their respective petal and sepal length (features). In total, the dataset consists of **150 samples** (50 samples per class) as well as their corresponding **4 different measurements** taken for each sample. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 700px; height: auto\" src=\"images/iris_dataset.png\">\n",
    "\n",
    "(Source: http://www.lac.inpe.br/~rafael.santos/Docs/R/CAP394/WholeStory-Iris.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please, find below the list of the individual measurements (features):\n",
    "\n",
    ">- `Sepal length (cm)`\n",
    ">- `Sepal width (cm)`\n",
    ">- `Petal length (cm)`\n",
    ">- `Petal width (cm)`\n",
    "\n",
    "Further details on the dataset can be obtained from the following publication: *Fisher, R.A. \"The use of multiple measurements in taxonomic problems\" Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to Mathematical Statistics\" (John Wiley, NY, 1950).\"*\n",
    "\n",
    "Let's load the dataset and conduct a preliminary data assessment: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect feature names of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the class names of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the top 5 feature rows of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.data).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print and inspect the top 5 labels of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(iris.target).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and print the feature dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine and print the label dimensionality of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris.target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's briefly envision how the feature data is collected and recorded in the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 900px; height: auto\" src=\"images/featurecollection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the data distributions of the distinct features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# supervised scenario\n",
    "#sns.pairplot(iris_plot, diag_kind='hist', hue='species');\n",
    "\n",
    "# unsupervised scenario\n",
    "sns.pairplot(iris_plot, diag_kind='hist');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1. Introduction into k-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the introduction **k-Means Clustering** is one of the most popular \"first choice\" clustering algorithms to find groups (clusters) in a given dataset $X$. Let's now briefly revisit the distinct step of the algorithm before applying it to the iris dataset. \n",
    "\n",
    "<img align=\"center\" style=\"max-width: 400px; height: auto\" src=\"images/kmeans.png\">\n",
    "\n",
    "But before that let's assume:\n",
    "\n",
    "- We have dataset $X$ consisting records $x_1, x_2, x_3, ..., x_n \\in \\mathcal{R}^d$; \n",
    "- That samples are clustered around $k$ centers (the \"$k$ means\") denoted by $\\mu_1, \\mu_{2}, ..., \\mu_{k} \\in \\mathcal{R}^d$; and,\n",
    "- Each sample $x_{i}$ belongs to its closest mean $\\mu_{i}$.\n",
    "\n",
    "We can then iteratively perform the following steps that comprise the **k-Means Clustering** algorithm:\n",
    "\n",
    ">- **Step 1** - Pick $k$ random points $\\mu_{i}$ as cluster centers called 'means'.\n",
    ">- **Step 2** - Assign each $x_i$ to its to nearest cluster mean by calculating its distance to each mean.\n",
    ">- **Step 3** - Determine the new cluster centers by calculating the average of the assigned points in each cluster.\n",
    ">- **Step 4** - Repeat Step 2 and 3 until none of the cluster assignments change.\n",
    "\n",
    "Note, that a single execution of all the four steps outlined above is usually referred to as 'iteration'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1.1. k-Means clustering in a 2-Dimensional Feature Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's see how we can apply it to the iris dataset. We will start with an introductory example of detecting the classes of the iris dataset based on two of its features namely the (1) `Petal length (cm)` and  `Petal width (cm)`. Let's first gain an intuition of those two features as well as their distribution by visualizing them accordingly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length (3rd feature in the dataset) vs. petal width (4th feature in the dataset)\n",
    "ax.scatter(iris.data[:,2], iris.data[:,3])\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[petal_length]\", fontsize=10)\n",
    "ax.set_ylabel(\"[petal_width]\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Petal Length vs. Petal Width', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now define the parameters of the k-Means Clustering. We will start be specifying the number of clusters we aim to detect in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_clusters = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define a corresponding number (in our case 3) random cluster centers or 'means' that will be used in the first iteration of the algoritm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_means = np.array([[1.0, 3.0], [2.0, 6.0], [1.0, 7.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will define a maximum number of iterations that we want to run the **k-Means Clustering** algorithm. Please, note that the clustering terminates once there will be no further changes in the cluster assignments. However, it's good practice to define an upper bound:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we ready to init the **k-Means Clustering**  using Python's `sklearn` libary of data science algorithms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=no_clusters, init=random_means, max_iter=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the k-Mains classifier and learn a model of the `Petal length (cm)` and  `Petal width (cm)` features in the iris dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(iris.data[:,2:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have learned a model of the data, let's inspect the distinct cluster labels that have been assigned to the records of the iris dataset: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, we want to inspect the coordinates of the cluster controids assigned by the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the iris dataset records now using the two features `Petal length (cm)`, `Petal width (cm)` and `Petal length (cm)`, their corresponding learned labels as well as the controids determined by the **k-Means Clustering** algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length vs. petal width and corresponding classes\n",
    "ax.scatter(iris.data[:,2], iris.data[:,3], c=labels.astype(np.float), cmap=plt.cm.Set1)\n",
    "\n",
    "# plot cluster centers\n",
    "ax.scatter(centers[:,0], centers[:,1], marker='x', c='black', s=100)\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[petal_length]\", fontsize=10)\n",
    "ax.set_ylabel(\"[petal_width]\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Petal Length vs. Petal Width', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, let's inspect the distance of all dataset records $X$ to their nearest means $\\mu_{i}$. Let's recall that k-Means corresponds to a local optimization of the sum of \"squared erros\", as expressed by:\n",
    "\n",
    "\n",
    "$$E(\\mu_{1}, \\mu_{2}, ..., \\mu_{k}) = \\sum_{i=1}^{n}(x_{i}-\\mu_{k(i)})^{2},$$\n",
    "\n",
    "were $x_{i}$ denots a single observation in the dataset and $\\mu_{k(i)}$ its closest mean in the feature space $\\mathcal{R}^{d}$.\n",
    "\n",
    "We can obtain the sum of those squared distances $E(\\mu_{1}, \\mu_{2}, ..., \\mu_{k})$ by calling the `inertia_` attribute of the kmeans clustering object. It will return the sum of squared distances of each sample to its closest cluster center:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = kmeans.inertia_\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following exercises as part of the lab:\n",
    "\n",
    "**1. Train and evaluate the k-Means Clustering results and squared distances $E(\\mu_{1}, \\mu_{2}, ..., \\mu_{k})$ for distinct max iterations.**\n",
    "\n",
    "> Continuously increase the number of training iterations $i$ of the k-Means Clustering starting with 1 and up to 5 iterations ($i=1,...,5$) and repeat the clustering accordingly. What can be observed in terms of the cluster means as well as the sum of squared cluster distances with increasing $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Determine if the k-Mean algorithm always converges to the same results.**\n",
    "\n",
    "> Carefully review the k-Means algorithm and answer the question and answer to following question: Does the k-Means algorithm always converge to the same result? Please, explain your reasoning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Applicability k-Means clustering to distinct data distributions.**\n",
    "\n",
    "> Consider the following data distributions. Determine which are suitable for a k-Means clustering and what $k$ value should be applied in the clustering. Please, explain your reasoning.\n",
    "\n",
    "<img align=\"center\" style=\"max-width: 600px; height: auto\" src=\"images/clustering.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.1.2. k-Means Clustering in a 3-Dimensional Feature Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable interactive 3D plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "from importlib import reload\n",
    "reload(plt)\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now extend the k-Means Clustering that you learned about to a 3-dimensional features space by clustering the first three features, namely `Sepal length (cm)`, `Sepal width (cm)` and `Petal length (cm)`, of the Iris dataset. Let's start by visually inspecting the 3-dimensional feature space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# init 3D plotting\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=30, azim=120)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length (3rd feature in the dataset) vs. petal width (4th feature in the dataset)\n",
    "ax.scatter(iris.data[:,0], iris.data[:,1], iris.data[:,2], s=40)\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[sepal_length]\", fontsize=10)\n",
    "ax.set_ylabel(\"[sepal_width]\", fontsize=10)\n",
    "ax.set_zlabel(\"[petal_length]\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Sepal Length vs. Sepal Width vs. Petal Length', fontsize=10)\n",
    "\n",
    "# show the three dimensional plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start again by defining the max. number of iterations we aim to run the k-Means algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also initialize our initial means randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_means = np.array([[1.0, 3.0, 3.0], [2.0, 6.0, 5.0], [1.0, 7.0, 2.0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As well as the k-Means clustering algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=no_clusters, init=random_means, max_iter=max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now fit the 3-dimensional data (instead of the two-dimensional data that we fitted above):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.fit(iris.data[:,0:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further inspect the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the learned cluster centers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "print(centers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the iris dataset records now using the three features `Sepal length (cm)`, `Sepal width (cm)` and `Petal length (cm)`, their corresponding learned labels as well as the controids determined by the **k-Means Clustering** algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "plt.rcParams['figure.figsize'] = [8, 6]\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# init 3D plotting\n",
    "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=30, azim=120)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length vs. petal width and corresponding classes\n",
    "ax.scatter(iris.data[:,0], iris.data[:,1], iris.data[:,2], c=labels.astype(np.float), cmap=plt.cm.Set1, s=40)\n",
    "\n",
    "# plot cluster centers\n",
    "ax.scatter(centers[:,0], centers[:,1], centers[:,2], marker='x', c='black', s=100)\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"[sepal_length]\", fontsize=10)\n",
    "ax.set_ylabel(\"[sepal_width]\", fontsize=10)\n",
    "ax.set_zlabel(\"[petal_length]\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Sepal Length vs. Sepal Width vs. Petal Length', fontsize=10);\n",
    "\n",
    "# show the three dimensional plot\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's again inspect the distance of each dataset record to its nearest means calling the `inertia_` attribute of the kmeans clustering object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = kmeans.inertia_\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We recommend you to try the following exercises as part of the lab:\n",
    "\n",
    "**1. Apply k-Means clustering to all features of the Iris dataset.**\n",
    "\n",
    "> Use the k-Means classifier to learn a model of all four features contained in the Iris dataset, namely `Sepal length (cm)`, `Sepal width (cm)`, `Petal length (cm)` and `Petal width (cm)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1.2. Detecting the Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enable inline Jupyter notebook plotting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, one of the basic ideas behind unsupervised machine learning methods, such as **k-Means clustering**, is to define clusters such that the total intra-cluster variation (usually measured by the total sum of squared distances) is minimized:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$k^{*} =\\underset{k}{\\arg \\min} \\sum_{i=1}^{n}(x_{i}-\\mu_{k(i)})^{2},$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "were $x_{i}$ denots a single observation in the dataset and $\\mu_{k(i)}$ its closest mean in the feature space $\\mathcal{R}^{d}$. Challange: What is the optimal number of clusters $k$ for a given dataset? Selection of the right $k$ may result in the following issues:\n",
    "\n",
    "- if $k$ too small (undersegmentation), then the clusters are too diverse; and;\n",
    "- if $k$ too high (oversegmentation), then the clusters are too fine-grain.\n",
    "\n",
    "Examples: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"center\" style=\"max-width: 800px; height: auto\" src=\"images/kselection.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solution: We can then use the sum of \"squared erros\" $E(\\mu_{1}, \\mu_{2}, ..., \\mu_{k})$ metric to find an optimal number of clusters $k$! This can be achieved by the execution of the so-called **'elbow'** technique defined by the following algorithm:\n",
    "\n",
    ">- **Step 1** - Compute the k-Means clustering algorithm for different number of clusters $k$.\n",
    ">- **Step 2** - For each $k$ calculate the sum of the within-cluster sum of squared distances $E(\\mu_{1}, \\mu_{2}, ..., \\mu_{k})$.\n",
    ">- **Step 3** - For each $k$ plot the $k$ value vs. its corresponding sum of within-cluster sum of squared distances $E$. \n",
    ">- **Step 4** - Inspect the plot and determine the location of a bend (appropriate number of clusters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's utilize the **'elbow'** technique by first defining the max. number of iterations that we aim to apply to each k-Means clustering run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can implement the and run the 'elbow' technique:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the list of squared distances\n",
    "sum_of_squared_distances = []\n",
    "\n",
    "# define the range of k-values to investigate\n",
    "K = range(1,30)\n",
    "\n",
    "# iterate over all k-values\n",
    "for k in K:\n",
    "    \n",
    "    # init the k-Means clustering algorithm of the current k-value\n",
    "    kmeans = KMeans(n_clusters=k, init='random', max_iter=max_iterations)\n",
    "    \n",
    "    # run the k-Means clustering of sepal-length and sepal-width features\n",
    "    kmeans = kmeans.fit(iris.data[:,0:2])\n",
    "    \n",
    "    # collect the sum of within squared distances of the current k-value\n",
    "    sum_of_squared_distances.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon completion of the loop above let's inspect the distinct within-cluster sum of squared distances $E$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the collected sum of squared distances of each k\n",
    "sum_of_squared_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, let's plot the cluster number $k$ vs. the within-cluster sum of squared distances $E$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare plot\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# add grid\n",
    "ax.grid(linestyle='dotted')\n",
    "\n",
    "# plot petal length (3rd feature in the dataset) vs. petal width (4th feature in the dataset)\n",
    "ax.plot(K, sum_of_squared_distances)\n",
    "\n",
    "# add axis legends\n",
    "ax.set_xlabel(\"$k$\", fontsize=10)\n",
    "ax.set_ylabel(\"Sum Within-Cluster Distance $E$\", fontsize=10)\n",
    "\n",
    "# add plot title\n",
    "plt.title('Cluster number $k$ vs. Within-Cluster $E$', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Determine the optimal number of cluster values $k$ of all four features contained in the iris dataset.**\n",
    "\n",
    "> Determinethe optimal number of clusters $k$ needed to cluster the observations of all four features contained in the iris dataset using the **'elbow'** technique outlined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Summary:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this fourth lab, a step by step introduction into the unsupervised **k-Means clustering** machine learning algorithm was presented. The code and exercises presented in this lab may serves as a starting point for more complex and tailored programs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may want to execute the content of your lab outside of the Jupyter notebook environment e.g. on compute node or server. The cell below converts the lab notebook into a standalone and executable python script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter nbconvert --to script cfds_lab_04.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In order to execute the statement above and convert your lab notebook to a regular Python script you first need to install the nbconvert Python package e.g. using the pip package installer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
